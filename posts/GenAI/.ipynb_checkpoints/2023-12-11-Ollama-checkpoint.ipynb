{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3ba97c8e-6b29-4c33-8aa4-3a8578b7cc4e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ollama Using\"\n",
    "description: \"This post is about Ollama installing and setting for private chatbots.\"\n",
    "author: \"Me\"\n",
    "date: \"2023-12-11\"\n",
    "categories: [GenAI, Ollama, Docker, Streamlit, LLMs, CUDA, CPU]\n",
    "image: images/ollama.png\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5db27-105c-490d-b121-bb7526d822ee",
   "metadata": {},
   "source": [
    "## Install Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cccc3e-1632-4505-8cf8-5107a02d6cad",
   "metadata": {},
   "source": [
    "___src: https://docs.docker.com/engine/install/ubuntu/___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc857c4c-59df-44bd-8005-2499cdee028d",
   "metadata": {},
   "source": [
    "1- Uninstall old versions:\n",
    "\n",
    "The unofficial packages to uninstall are:\n",
    "\n",
    "docker.io</br>\n",
    "docker-compose</br>\n",
    "docker-compose-v2</br>\n",
    "docker-doc</br>\n",
    "podman-docker</br>\n",
    "\n",
    "    for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n",
    "\n",
    "2- Install from a package:\n",
    "\n",
    "If you can't use Docker's apt repository to install Docker Engine, you can download the deb file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.\n",
    "\n",
    "Go to `https://download.docker.com/linux/ubuntu/dists/`.\n",
    "\n",
    "Select your Ubuntu version in the list.\n",
    "\n",
    "Go to pool/stable/ and select the applicable architecture (amd64, armhf, arm64, or s390x).\n",
    "\n",
    "Download the following deb files for the Docker Engine, CLI, containerd, and Docker Compose packages:\n",
    "\n",
    "    containerd.io_<version>_<arch>.deb\n",
    "    docker-ce_<version>_<arch>.deb\n",
    "    docker-ce-cli_<version>_<arch>.deb\n",
    "    docker-buildx-plugin_<version>_<arch>.deb\n",
    "    docker-compose-plugin_<version>_<arch>.deb\n",
    "    \n",
    "Install the .deb packages. Update the paths in the following example to where you downloaded the Docker packages.\n",
    "\n",
    " sudo dpkg -i ./containerd.io_<version>_<arch>.deb \\\n",
    "  ./docker-ce_<version>_<arch>.deb \\\n",
    "  ./docker-ce-cli_<version>_<arch>.deb \\\n",
    "  ./docker-buildx-plugin_<version>_<arch>.deb \\\n",
    "  ./docker-compose-plugin_<version>_<arch>.deb\n",
    "  \n",
    "The Docker daemon starts automatically.\n",
    "\n",
    "Verify that the Docker Engine installation is successful by running the hello-world image.\n",
    "\n",
    "     sudo service docker start\n",
    "     sudo docker run hello-world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de16df-a6d2-4b5e-a9df-86499a9479e5",
   "metadata": {},
   "source": [
    "## Install CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7637d2-857d-45e8-a130-fa5febf8f1fc",
   "metadata": {},
   "source": [
    "Install CUDA for using GPU\n",
    "\n",
    "1- sudo apt update</br>\n",
    "2- sudo apt upgrade</br>\n",
    "3- sudo apt install ubuntu-drivers-common</br>\n",
    "4- sudo ubuntu-drivers devices</br>\n",
    "5- recommends the NVIDIA driver 535</br>\n",
    "\n",
    "\tdriver   : nvidia-driver-535 - distro non-free recommended\n",
    "\t\n",
    "6- sudo apt install nvidia-driver-535</br>\n",
    "7- Reboot</br>\n",
    "8- Using NVIDIA icon in top page change to \"Switch to: NVIDIA (Performance Mode) and then Logout.</br>\n",
    "9- nvidia-smi</br>\n",
    "\n",
    "you must see a table. At the top of the table, we will see the driver version and CUDA driver API compatibility:\n",
    "\n",
    "\tNVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2\n",
    "\n",
    "10- sudo apt install gcc</br>\n",
    "11- Install CUDA toolkit Ubuntu</br>\n",
    "\n",
    "src:https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network\n",
    "\n",
    "\twget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "\tsudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "\tsudo apt-get update  ((for this step you must use VPN such as windscribe:( ))\n",
    "\tsudo apt-get -y install cuda-toolkit-12-3 ((for this step you must use VPN such as windscribe:( ))\n",
    "\t\n",
    "If you encounter dependency errors during the installation, try running `sudo apt --fix-broken install` to fix them. Apt will suggest running it if needed.\n",
    "\n",
    "12- Reboot</br>\n",
    "13- Environment setup</br>\n",
    "\n",
    "We will now proceed to update the environment variables as recommended by the NVIDIA documentation.\n",
    "Add the following line to your `.bashrc` file using `nano ~/.bashrc` and paste the following lines at the end of the file.\n",
    "\n",
    "\texport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "\texport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\\\n",
    "                         \t ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n",
    "\n",
    "Save the file.\n",
    "\n",
    "14- Reboot\n",
    "\t\n",
    "15- Test the CUDA toolkit</br>\n",
    "\t\n",
    "    nvcc -V\n",
    "\t\n",
    "You must see:\n",
    "\n",
    "\tnvcc: NVIDIA (R) Cuda compiler driver\n",
    "\tCopyright (c) 2005-2023 NVIDIA Corporation\n",
    "\tBuilt on Fri_Nov__3_17:16:49_PDT_2023\n",
    "\tCuda compilation tools, release 12.3, V12.3.103\n",
    "\tBuild cuda_12.3.r12.3/compiler.33492891_0\n",
    "\n",
    "src: https://www.cherryservers.com/blog/install-cuda-ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e9005-a2f0-4137-ace7-e89b697f35c6",
   "metadata": {},
   "source": [
    "## Install Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6b019-f0a9-4c76-97f7-7121bbc8d6b0",
   "metadata": {},
   "source": [
    "src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md\n",
    "\n",
    "1- Create a user for Ollama:\n",
    "\n",
    "    sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "2- Create a service file in `/etc/systemd/system/ollama.service`:\n",
    "\n",
    "    [Unit]\n",
    "    Description=Ollama Service\n",
    "    After=network-online.target\n",
    "    \n",
    "    [Service]\n",
    "    ExecStart=/usr/bin/ollama serve\n",
    "    User=ollama\n",
    "    Group=ollama\n",
    "    Restart=always\n",
    "    RestartSec=3\n",
    "    \n",
    "    [Install]\n",
    "    WantedBy=default.target\n",
    "\n",
    "3- Then start the service:\n",
    "\n",
    "    sudo systemctl daemon-reload\n",
    "    sudo systemctl enable ollama\n",
    "\n",
    "4- Start Ollama using systemd:\n",
    "\n",
    "    sudo systemctl start ollama\n",
    "\n",
    "5- Update ollama by downloading the ollama binary:\n",
    "\n",
    "    sudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "    sudo chmod +x /usr/bin/ollama\n",
    "\n",
    "6- To view logs of Ollama running as a startup service, run:\n",
    "\n",
    "    journalctl -u ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618606d-f966-4557-8283-09786184f0f8",
   "metadata": {},
   "source": [
    "## Ollama Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49635e65-e6ad-4040-97ef-c4f4c6c4ee11",
   "metadata": {},
   "source": [
    "src:https://hub.docker.com/r/ollama/ollama\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only:\n",
    "\n",
    "    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3- Nvidia GPU with Apt\n",
    "\n",
    "3-1- Configure the repository:\n",
    "\n",
    "    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "    sudo apt-get update\n",
    "\n",
    "3-2- Install the NVIDIA Container Toolkit packages\n",
    "\n",
    "    sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "3-3- Configure Docker to use Nvidia driver:\n",
    "\n",
    "    sudo nvidia-ctk runtime configure --runtime=docker\n",
    "    sudo systemctl restart docker\n",
    "\n",
    "3-4- Start the container:\n",
    "\n",
    "    docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3-5- Run model locally\n",
    "\n",
    "Now you can run a model:\n",
    "\n",
    "    docker exec -it ollama ollama run llama2 \n",
    "\n",
    "for the first time this code will download your desired model [Ollama Models](https://ollama.ai/library).\n",
    "\n",
    "4- If you want run again and again model use below code in terminal and again 2 (for only CPU) and 3-4(GPU):\n",
    "\n",
    "    sudo docker ps -a\n",
    "    sudo docker stop ollama\n",
    "    sudo docker rm ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5a420-a69f-4cda-b00b-74b85acb80ae",
   "metadata": {},
   "source": [
    "# Latest Method For Ollama Installation with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd7474-f1fc-4550-b7aa-b713a4c291a3",
   "metadata": {},
   "source": [
    "[Build your own RAG and run it locally: Langchain + Ollama + Streamlit](https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\n",
    "[Install Ollama](src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md#manual-install)\n",
    "\n",
    "1- Install Ollama in linux:\n",
    "\n",
    "\tsudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "\tsudo chmod +x /usr/bin/ollama\n",
    "\n",
    "2- Create a user for Ollama:\n",
    "\n",
    "\tsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "3- Create a service file in /etc/systemd/system/ollama.service:\n",
    "\n",
    "\t[Unit]\n",
    "\tDescription=Ollama Service\n",
    "\tAfter=network-online.target\n",
    "\n",
    "\t[Service]\n",
    "\tExecStart=/usr/bin/ollama serve\n",
    "\tUser=ollama\n",
    "\tGroup=ollama\n",
    "\tRestart=always\n",
    "\tRestartSec=3\n",
    "\n",
    "\t[Install]\n",
    "\tWantedBy=default.target\n",
    "\n",
    "4- Then start the service:\n",
    "\n",
    "\tsudo systemctl daemon-reload\n",
    "\tudo systemctl enable ollama\n",
    "\t\n",
    "5- Start Ollama using systemd:\n",
    "\n",
    "\tsudo systemctl start ollama\n",
    "\n",
    "\n",
    "============================================\n",
    "\n",
    "## Above Steps not Worked in Ubuntu\n",
    "\n",
    "============================================\n",
    "\n",
    "Using Ollama as Docker (src:https://hub.docker.com/r/ollama/ollama)\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only\n",
    "\n",
    "\tdocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\t\n",
    "3- Using Ollama with GPU needs at least 4 GPU 4Gb (total 16 Gb), so I can only use CPU.\n",
    "\n",
    "4- Run model locally\n",
    "\n",
    "\tdocker exec -it ollama ollama run llama2\n",
    "\n",
    "6- Create new env with python < 3.12 \n",
    "\n",
    "\t- micromamba activate base\n",
    "\t- micromamba create -n ollama python=3.11\n",
    "\t- micromamba activate ollama\n",
    "\t\n",
    "7- Build the RAG pipeline (src:https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\t- pip install langchain==0.0.343\n",
    "\t- pip install streamlit==1.29.0\n",
    "\t- pip install streamlit-chat==0.1.1\n",
    "\t- pip install pypdf==3.17.1\n",
    "\t- pip install fastembed==0.1.1\n",
    "\t- pip install openai==1.3.6\n",
    "\t- pip install langchainhub==0.1.14\n",
    "\t- pip install chromadb==0.4.18\n",
    "\t- pip install watchdog==3.0.0\n",
    "\n",
    "8- Some Commands with Ollama\n",
    "\n",
    "    sudo docker ps -a # List of AvailableModels\n",
    "    sudo docker stop ollama # Stop the current model\n",
    "    sudo docker rm ollama # Remove the current model\n",
    "\n",
    "9- How do I clean the memory cache?\n",
    "\n",
    "    sync && echo 3 | sudo tee /proc/sys/vm/drop_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218324-0645-4bf2-be35-068340d096f4",
   "metadata": {},
   "source": [
    "# Using Streamlit with Ollama and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09146a-5235-4e42-9081-e7e91cead32b",
   "metadata": {},
   "source": [
    "Save below code in a file with name myapp.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ee7d2-aaf2-4103-bf3c-c0b18acb3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from streamlit_chat import message\n",
    "from langchain.llms import Ollama #Cohere\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #CohereEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "__import__(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Setting Up Streamlit Page\n",
    "st.set_page_config(page_title=\"Ollama Chatbot\", page_icon= \"💬\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.title('💬 OLLAMA Chatbot')\n",
    "    \n",
    "    #st.divider()\n",
    "    # Select the model\n",
    "    selected_model = st.selectbox('Choose a model', ['Mistral', 'Llama2', 'Code Llama', 'Zephyr'], key='selected_model')\n",
    "    \n",
    "    if selected_model == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\") \n",
    "    elif selected_model == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    else:\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\") \n",
    "    #st.divider()\n",
    "    temp_r = st.slider(\"Temperature\", 0.0, 0.9, 0.0, 0.1)\n",
    "    chunk_size = st.slider(\"Chunk Size for Splitting Document \", 256, 1024, 400, 10)\n",
    "    chunk_overlap = st.slider(\"Chunk Overlap \", 0, 100, 20, 5)\n",
    "    clear_button = st.button(\"Clear Conversation\", key=\"clear\")\n",
    "\n",
    "#-----------------------Functions-------------------------------#\n",
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        #model_kwargs={'device': 'cuda'}, #  you can set model_kwargs={'device': 'cuda:0'} for the first GPU, model_kwargs={'device': 'cuda:1'} for the second GPU, and so on.(src:https://github.com/langchain-ai/langchain/issues/10436)\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Creating the chain for Question Answering\n",
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# tabs\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"💬 Chatbot\", \"🖹 ChatPDFs\", \"📈 ChatPandas\", \"🌍 ChatMaps\"])\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Chatbot Tab\n",
    "# with tab1():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPDFs Tab\n",
    "with tab2:\n",
    "\t# Upload PDF files\n",
    "    uploaded_PDF_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True, type=\"pdf\")\n",
    "\n",
    "if uploaded_PDF_files:\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        for uploaded_file in uploaded_PDF_files:\n",
    "            file_name = uploaded_file.name\n",
    "            file_content = uploaded_file.read()\n",
    "            st.write(\"Filename: \", file_name)\n",
    "\n",
    "            # Write the content of the PDF files to a temporary directory\n",
    "            with open(os.path.join(tmpdir, file_name), \"wb\") as file:\n",
    "                file.write(file_content)\n",
    "\n",
    "        # Load the PDF files from the temporary directory\n",
    "        loader = DirectoryLoader(tmpdir, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split the PDF files into smaller chunks of text\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        documents = text_splitter.split_documents(documents)\n",
    "        embeddings = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "        #vectorstore.persist()\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        prompt_template = \"\"\" \n",
    "        System Prompt:\n",
    "        Your are an AI chatbot that helps users chat with PDF documents. How may I help you today?\n",
    "\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        \"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=llm_model, temperature=temp_r),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        )\n",
    "        # Question-Answer\n",
    "        # Get the user question\n",
    "        query = st.text_input(\"Ask a question:\")\n",
    "\n",
    "        if query:\n",
    "                response = chain({'query': query})\n",
    "                # Wrapping the text for better output in Jupyter Notebook\n",
    "                wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "                # Display the answer\n",
    "                st.markdown(f\"**Q:** {query}\")\n",
    "                st.markdown(f\"**A:** {wrapped_text}\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPandas Tab\n",
    "# with tab3():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatMaps Tab\n",
    "# with tab4():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f9ed9-7a98-4035-86dc-38002633fd7c",
   "metadata": {},
   "source": [
    "After Run Ollama docker run below code:\n",
    "\n",
    "    streamlit run myapp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc75af-8389-4340-b42f-6b0049b863c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
