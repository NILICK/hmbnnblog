{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3ba97c8e-6b29-4c33-8aa4-3a8578b7cc4e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ollama Using\"\n",
    "description: \"This post is about Ollama installing and setting for private chatbots.\"\n",
    "author: \"Me\"\n",
    "date: \"2023-12-11\"\n",
    "categories: [GenAI, Ollama, Docker, Streamlit, LLMs, CUDA, CPU, ChatPDF, Langchain]\n",
    "image: images/ollama.png\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5db27-105c-490d-b121-bb7526d822ee",
   "metadata": {},
   "source": [
    "## Install Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cccc3e-1632-4505-8cf8-5107a02d6cad",
   "metadata": {},
   "source": [
    "___src: https://docs.docker.com/engine/install/ubuntu/___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc857c4c-59df-44bd-8005-2499cdee028d",
   "metadata": {},
   "source": [
    "1- Uninstall old versions:\n",
    "\n",
    "The unofficial packages to uninstall are:\n",
    "\n",
    "docker.io</br>\n",
    "docker-compose</br>\n",
    "docker-compose-v2</br>\n",
    "docker-doc</br>\n",
    "podman-docker</br>\n",
    "\n",
    "    for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n",
    "\n",
    "2- Install from a package:\n",
    "\n",
    "If you can't use Docker's apt repository to install Docker Engine, you can download the deb file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.\n",
    "\n",
    "Go to `https://download.docker.com/linux/ubuntu/dists/`.\n",
    "\n",
    "Select your Ubuntu version in the list.\n",
    "\n",
    "Go to pool/stable/ and select the applicable architecture (amd64, armhf, arm64, or s390x).\n",
    "\n",
    "Download the following deb files for the Docker Engine, CLI, containerd, and Docker Compose packages:\n",
    "\n",
    "    containerd.io_<version>_<arch>.deb\n",
    "    docker-ce_<version>_<arch>.deb\n",
    "    docker-ce-cli_<version>_<arch>.deb\n",
    "    docker-buildx-plugin_<version>_<arch>.deb\n",
    "    docker-compose-plugin_<version>_<arch>.deb\n",
    "    \n",
    "Install the .deb packages. Update the paths in the following example to where you downloaded the Docker packages.\n",
    "\n",
    " sudo dpkg -i ./containerd.io_<version>_<arch>.deb \\\n",
    "  ./docker-ce_<version>_<arch>.deb \\\n",
    "  ./docker-ce-cli_<version>_<arch>.deb \\\n",
    "  ./docker-buildx-plugin_<version>_<arch>.deb \\\n",
    "  ./docker-compose-plugin_<version>_<arch>.deb\n",
    "  \n",
    "The Docker daemon starts automatically.\n",
    "\n",
    "Verify that the Docker Engine installation is successful by running the hello-world image.\n",
    "\n",
    "     sudo service docker start\n",
    "     sudo docker run hello-world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de16df-a6d2-4b5e-a9df-86499a9479e5",
   "metadata": {},
   "source": [
    "## Install CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7637d2-857d-45e8-a130-fa5febf8f1fc",
   "metadata": {},
   "source": [
    "Install CUDA for using GPU\n",
    "\n",
    "1- sudo apt update</br>\n",
    "2- sudo apt upgrade</br>\n",
    "3- sudo apt install ubuntu-drivers-common</br>\n",
    "4- sudo ubuntu-drivers devices</br>\n",
    "5- recommends the NVIDIA driver 535</br>\n",
    "\n",
    "\tdriver   : nvidia-driver-535 - distro non-free recommended\n",
    "\t\n",
    "6- sudo apt install nvidia-driver-535</br>\n",
    "7- Reboot</br>\n",
    "8- Using NVIDIA icon in top page change to \"Switch to: NVIDIA (On-Demand) and then Logout.</br>\n",
    "9- nvidia-smi</br>\n",
    "\n",
    "you must see a table. At the top of the table, we will see the driver version and CUDA driver API compatibility:\n",
    "\n",
    "\tNVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2\n",
    "\n",
    "10- sudo apt install gcc</br>\n",
    "11- Install CUDA toolkit Ubuntu</br>\n",
    "\n",
    "src:https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network\n",
    "\n",
    "\twget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "\tsudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "\tsudo apt-get update  ((for this step you must use VPN such as windscribe:( ))\n",
    "\tsudo apt-get -y install cuda-toolkit-12-3 ((for this step you must use VPN such as windscribe:( ))\n",
    "\t\n",
    "If you encounter dependency errors during the installation, try running `sudo apt --fix-broken install` to fix them. Apt will suggest running it if needed.\n",
    "\n",
    "12- Reboot</br>\n",
    "13- Environment setup</br>\n",
    "\n",
    "We will now proceed to update the environment variables as recommended by the NVIDIA documentation.\n",
    "Add the following line to your `.bashrc` file using `nano ~/.bashrc` and paste the following lines at the end of the file.\n",
    "\n",
    "\texport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "\texport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\\\n",
    "                         \t ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n",
    "\n",
    "Save the file.\n",
    "\n",
    "14- Reboot\n",
    "\t\n",
    "15- Test the CUDA toolkit</br>\n",
    "\t\n",
    "    nvcc -V\n",
    "\t\n",
    "You must see:\n",
    "\n",
    "\tnvcc: NVIDIA (R) Cuda compiler driver\n",
    "\tCopyright (c) 2005-2023 NVIDIA Corporation\n",
    "\tBuilt on Fri_Nov__3_17:16:49_PDT_2023\n",
    "\tCuda compilation tools, release 12.3, V12.3.103\n",
    "\tBuild cuda_12.3.r12.3/compiler.33492891_0\n",
    "\n",
    "src: https://www.cherryservers.com/blog/install-cuda-ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e9005-a2f0-4137-ace7-e89b697f35c6",
   "metadata": {},
   "source": [
    "## Install Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6b019-f0a9-4c76-97f7-7121bbc8d6b0",
   "metadata": {},
   "source": [
    "src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md\n",
    "\n",
    "1- Create a user for Ollama:\n",
    "\n",
    "    sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "2- Create a service file in `/etc/systemd/system/ollama.service`:\n",
    "\n",
    "    [Unit]\n",
    "    Description=Ollama Service\n",
    "    After=network-online.target\n",
    "    \n",
    "    [Service]\n",
    "    ExecStart=/usr/bin/ollama serve\n",
    "    User=ollama\n",
    "    Group=ollama\n",
    "    Restart=always\n",
    "    RestartSec=3\n",
    "    \n",
    "    [Install]\n",
    "    WantedBy=default.target\n",
    "\n",
    "3- Then start the service:\n",
    "\n",
    "    sudo systemctl daemon-reload\n",
    "    sudo systemctl enable ollama\n",
    "\n",
    "4- Start Ollama using systemd:\n",
    "\n",
    "    sudo systemctl start ollama\n",
    "\n",
    "5- Update ollama by downloading the ollama binary:\n",
    "\n",
    "    sudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "    sudo chmod +x /usr/bin/ollama\n",
    "\n",
    "6- To view logs of Ollama running as a startup service, run:\n",
    "\n",
    "    journalctl -u ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618606d-f966-4557-8283-09786184f0f8",
   "metadata": {},
   "source": [
    "## Ollama Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49635e65-e6ad-4040-97ef-c4f4c6c4ee11",
   "metadata": {},
   "source": [
    "src:https://hub.docker.com/r/ollama/ollama\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only:\n",
    "\n",
    "    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3- Nvidia GPU with Apt\n",
    "\n",
    "3-1- Configure the repository:\n",
    "\n",
    "    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "    sudo apt-get update\n",
    "\n",
    "3-2- Install the NVIDIA Container Toolkit packages\n",
    "\n",
    "    sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "3-3- Configure Docker to use Nvidia driver:\n",
    "\n",
    "    sudo nvidia-ctk runtime configure --runtime=docker\n",
    "    sudo systemctl restart docker\n",
    "\n",
    "3-4- Start the container:\n",
    "\n",
    "    docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3-5- Run model locally\n",
    "\n",
    "Now you can run a model:\n",
    "\n",
    "    docker exec -it ollama ollama run llama2 \n",
    "\n",
    "for the first time this code will download your desired model [Ollama Models](https://ollama.ai/library).\n",
    "\n",
    "4- If you want run again and again model use below code in terminal and again 2 (for only CPU) and 3-4(GPU):\n",
    "\n",
    "    sudo docker ps -a\n",
    "    sudo docker stop ollama\n",
    "    sudo docker rm ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5a420-a69f-4cda-b00b-74b85acb80ae",
   "metadata": {},
   "source": [
    "# Latest Method For Ollama Installation with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd7474-f1fc-4550-b7aa-b713a4c291a3",
   "metadata": {},
   "source": [
    "[Build your own RAG and run it locally: Langchain + Ollama + Streamlit](https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\n",
    "[Install Ollama](src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md#manual-install)\n",
    "\n",
    "1- Install Ollama in linux:\n",
    "\n",
    "\tsudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "\tsudo chmod +x /usr/bin/ollama\n",
    "\n",
    "2- Create a user for Ollama:\n",
    "\n",
    "\tsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "3- Create a service file in /etc/systemd/system/ollama.service:\n",
    "\n",
    "\t[Unit]\n",
    "\tDescription=Ollama Service\n",
    "\tAfter=network-online.target\n",
    "\n",
    "\t[Service]\n",
    "\tExecStart=/usr/bin/ollama serve\n",
    "\tUser=ollama\n",
    "\tGroup=ollama\n",
    "\tRestart=always\n",
    "\tRestartSec=3\n",
    "\n",
    "\t[Install]\n",
    "\tWantedBy=default.target\n",
    "\n",
    "4- Then start the service:\n",
    "\n",
    "\tsudo systemctl daemon-reload\n",
    "\tudo systemctl enable ollama\n",
    "\t\n",
    "5- Start Ollama using systemd:\n",
    "\n",
    "\tsudo systemctl start ollama\n",
    "\n",
    "\n",
    "============================================\n",
    "\n",
    "## Above Steps not Worked in Ubuntu\n",
    "\n",
    "============================================\n",
    "\n",
    "Using Ollama as Docker (src:https://hub.docker.com/r/ollama/ollama)\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only\n",
    "\n",
    "\tdocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\t\n",
    "3- Using Ollama with GPU needs at least 4 GPU 4Gb (total 16 Gb), so I can only use CPU.\n",
    "\n",
    "4- Run model locally\n",
    "\n",
    "\tdocker exec -it ollama ollama run llama2\n",
    "\n",
    "6- Create new env with python < 3.12 \n",
    "\n",
    "\t- micromamba activate base\n",
    "\t- micromamba create -n ollama python=3.11\n",
    "\t- micromamba activate ollama\n",
    "\t\n",
    "7- Build the RAG pipeline (src:https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\t- pip install langchain==0.0.343\n",
    "\t- pip install streamlit==1.29.0\n",
    "\t- pip install streamlit-chat==0.1.1\n",
    "\t- pip install pypdf==3.17.1\n",
    "\t- pip install fastembed==0.1.1\n",
    "\t- pip install openai==1.3.6\n",
    "\t- pip install langchainhub==0.1.14\n",
    "\t- pip install chromadb==0.4.18\n",
    "\t- pip install watchdog==3.0.0\n",
    "\n",
    "8- Some Commands with Ollama\n",
    "\n",
    "    sudo docker ps -a # List of AvailableModels\n",
    "    sudo docker stop ollama # Stop the current model\n",
    "    sudo docker rm ollama # Remove the current model\n",
    "\n",
    "9- How do I clean the memory cache?\n",
    "\n",
    "    sync && echo 3 | sudo tee /proc/sys/vm/drop_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218324-0645-4bf2-be35-068340d096f4",
   "metadata": {},
   "source": [
    "# Using Streamlit with Ollama and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09146a-5235-4e42-9081-e7e91cead32b",
   "metadata": {},
   "source": [
    "Save below code in a file with name myapp.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ee7d2-aaf2-4103-bf3c-c0b18acb3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from streamlit_chat import message\n",
    "from langchain.llms import Ollama #Cohere\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #CohereEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "__import__(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Setting Up Streamlit Page\n",
    "st.set_page_config(page_title=\"Ollama Chatbot\", page_icon= \"ðŸ’¬\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.title('ðŸ’¬ OLLAMA Chatbot')\n",
    "    \n",
    "    #st.divider()\n",
    "    # Select the model\n",
    "    selected_model = st.selectbox('Choose a model', ['Phi-2', 'Llama2', \"Orca-mini\",\n",
    "                                                     'Zephyr', 'Code Llama', 'Mistral'],\n",
    "                                   key='selected_model')\n",
    "\n",
    "    if selected_model == \"Phi-2\":\n",
    "        llm_model = \"phi\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Phi-2............................\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Orca-mini\":\n",
    "        llm_model = \"orca-mini\"\n",
    "        st.caption(\"\"\"\n",
    "                   Orca-mini..................................\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Code llama\":\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\")\n",
    "    #st.divider()\n",
    "    temp_r = st.slider(\"Temperature\", 0.0, 0.9, 0.0, 0.1)\n",
    "    chunk_size = st.slider(\"Chunk Size for Splitting Document \", 256, 1024, 400, 10)\n",
    "    chunk_overlap = st.slider(\"Chunk Overlap \", 0, 100, 20, 5)\n",
    "    clear_button = st.button(\"Clear Conversation\", key=\"clear\")\n",
    "\n",
    "#-----------------------Functions-------------------------------#\n",
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device': 'cuda'}, #  you can set model_kwargs={'device': 'cuda:0'} for the first GPU, model_kwargs={'device': 'cuda:1'} for the second GPU, and so on.(src:https://github.com/langchain-ai/langchain/issues/10436)\n",
    "        #model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Creating the chain for Question Answering\n",
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# tabs\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ’¬ Chatbot\", \"ðŸ–¹ ChatPDFs\", \"ðŸ“ˆ ChatPandas\", \"ðŸŒ ChatMaps\"])\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Chatbot Tab\n",
    "# with tab1():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPDFs Tab\n",
    "with tab2:\n",
    "\t# Upload PDF files\n",
    "    uploaded_PDF_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True, type=\"pdf\")\n",
    "\n",
    "if uploaded_PDF_files:\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        for uploaded_file in uploaded_PDF_files:\n",
    "            file_name = uploaded_file.name\n",
    "            file_content = uploaded_file.read()\n",
    "            st.write(\"Filename: \", file_name)\n",
    "\n",
    "            # Write the content of the PDF files to a temporary directory\n",
    "            with open(os.path.join(tmpdir, file_name), \"wb\") as file:\n",
    "                file.write(file_content)\n",
    "\n",
    "        # Load the PDF files from the temporary directory\n",
    "        loader = DirectoryLoader(tmpdir, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split the PDF files into smaller chunks of text\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        documents = text_splitter.split_documents(documents)\n",
    "        embeddings = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "        #vectorstore.persist()\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        prompt_template = \"\"\" \n",
    "        System Prompt:\n",
    "        Your are an AI chatbot that helps users chat with PDF documents. How may I help you today?\n",
    "\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        \"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=llm_model, temperature=temp_r),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        )\n",
    "        # Question-Answer\n",
    "        # Get the user question\n",
    "        query = st.text_input(\"Ask a question:\")\n",
    "\n",
    "        if query:\n",
    "                response = chain({'query': query})\n",
    "                # Wrapping the text for better output in Jupyter Notebook\n",
    "                wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "                # Display the answer\n",
    "                st.markdown(f\"**Q:** {query}\")\n",
    "                st.markdown(f\"**A:** {wrapped_text}\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPandas Tab\n",
    "# with tab3():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatMaps Tab\n",
    "# with tab4():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f9ed9-7a98-4035-86dc-38002633fd7c",
   "metadata": {},
   "source": [
    "After Run Ollama docker run below code:\n",
    "\n",
    "    streamlit run myapp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa32432-c89b-43f3-9781-a517326fa7f8",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f680d4-d4d8-4a53-892e-8846bc33af0a",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "\n",
    "In 2023-Dec-20, Ollama pulled `Phi-2` model. This `Phi-2 Model` is a Small Language Model (SLM) type and released by `Microsoft` In 2023-Dec-15. This small model is very incredible model that with 1.6GB and 2.7B parameter is very `????????????????`  .\n",
    "\n",
    "For use this model. I first update `Ollama Docker` using `????????????????`.\n",
    "\n",
    "After that:\n",
    "\n",
    "    sudo docker stop ollama\n",
    "    sudo docker rm ollama\n",
    "\n",
    "Because `Phi-2 model` is small, so I can use `CUDA` with this model. So I can active `GPU` using:\n",
    "\n",
    "    docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "** I remeber that for other models such as `llama2, Orca-mini, Zephyr, Code Llama, Mistral and other 7B models` I only use `CPU` using:\n",
    "\n",
    "    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "After activing the `GPU` or `CPU` I can run `Streamlit` for using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55eba4b-703a-4724-9995-47b1962dbc8c",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923066e9-172b-4ae9-900f-e886bc50521e",
   "metadata": {},
   "source": [
    "## Building Own LLM model using GGUF from Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78256169-2390-4437-9edc-aff7de57137e",
   "metadata": {},
   "source": [
    "Refer Ollama Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc01b1-9423-4142-94bf-494df64f29cc",
   "metadata": {},
   "source": [
    "huggingface-cli download TheBloke/MistralLite-7B-GGUF mistrallite.Q4_K_M.gguf --local-dir downloads --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "huggingface-cli download kroonen/phi-2-GGUF phi-2_Q4_K_M.gguf --local-dir downloads --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "huggingface-cli download TheBloke/Mistral-7B-OpenOrca-GGUF mistral-7b-openorca.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de81ee-608c-445b-bb0a-fd5556857553",
   "metadata": {},
   "source": [
    "# New ChatPDF with Streamlit-Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5bf9b-5337-41b5-b7aa-becd6bb50b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "st.set_page_config(page_title=\"Chat with Documents\", page_icon=\"ðŸ“–\")\n",
    "st.title(\"ðŸ“š: Chat with Documents\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "MODES = (\"CPU\", \"GPU\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.title('ðŸ’¬ OLLAMA Chatbot')\n",
    "\n",
    "    # Select Processing Mode\n",
    "    mode = st.radio(\"Choose a mode\", MODES)\n",
    "    if mode == \"CPU\":\n",
    "        device = \"cpu\"\n",
    "    elif mode == \"GPU\":\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Select the model\n",
    "    model_name = st.selectbox('Choose a model: ', ['Phi-2', 'Llama2', \"Orca-mini\",\n",
    "                                                 'Zephyr', 'Code Llama', 'Mistral'],\n",
    "                              key='model_name')\n",
    "\n",
    "    if model_name == \"Phi-2\":\n",
    "        llm_model = \"phi\"\n",
    "        st.caption(\"\"\"\n",
    "                   Phi-2: a 2.7B language model by Microsoft Research\n",
    "\t\t\t\t   that demonstrates outstanding reasoning and language understanding capabilities.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Orca-mini\":\n",
    "        llm_model = \"orca-mini\"\n",
    "        st.caption(\"\"\"\n",
    "                   A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Code llama\":\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\")\n",
    "    #st.divider()\n",
    "    temp_r = st.slider(\"Temperature\", 0.0, 0.9, 0.0, 0.1)\n",
    "    chunk_size = st.slider(\"Chunk Size for Splitting Document \", 200, 3000, 1500, 20)\n",
    "    chunk_overlap = st.slider(\"Chunk Overlap \", 0, 500, 200, 10)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "@st.cache_resource(ttl=\"1h\")\n",
    "def configure_retriever(uploaded_files):\n",
    "    # Read documents\n",
    "    docs = []\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    for file in uploaded_files:\n",
    "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "        with open(temp_filepath, \"wb\") as f:\n",
    "            f.write(file.getvalue())\n",
    "        loader = PyPDFLoader(temp_filepath)\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create embeddings and store in vectordb\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': device},\n",
    "                                       encode_kwargs = {'normalize_embeddings': True # keep True to compute cosine similarity\n",
    "                                                        })\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "\n",
    "    # Define retriever\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "        self.run_id_ignore_token = None\n",
    "\n",
    "    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):\n",
    "        # Workaround to prevent showing the rephrased question as output\n",
    "        if prompts[0].startswith(\"Human\"):\n",
    "            self.run_id_ignore_token = kwargs.get(\"run_id\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        if self.run_id_ignore_token == kwargs.get(\"run_id\", False):\n",
    "            return\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "class PrintRetrievalHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container):\n",
    "        self.status = container.status(\"**Context Retrieval**\")\n",
    "\n",
    "    def on_retriever_start(self, serialized: dict, query: str, **kwargs):\n",
    "        self.status.write(f\"**Question:** {query}\")\n",
    "        self.status.update(label=f\"**Context Retrieval:** {query}\")\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        for idx, doc in enumerate(documents):\n",
    "            source = os.path.basename(doc.metadata[\"source\"])\n",
    "            self.status.write(f\"**Document {idx} from {source}**\")\n",
    "            self.status.markdown(doc.page_content)\n",
    "        self.status.update(state=\"complete\")\n",
    "#--------------------------------------------------------------#\n",
    "# Upload PDF Files\n",
    "uploaded_files = st.file_uploader(\n",
    "    label=\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True\n",
    ")\n",
    "if not uploaded_files:\n",
    "    st.info(\"Please upload PDF documents to continue.\")\n",
    "    st.stop()\n",
    "\n",
    "retriever = configure_retriever(uploaded_files)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Setup memory for contextual conversation\n",
    "msgs = StreamlitChatMessageHistory()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
    "\n",
    "# Setup LLM and QA chain\n",
    "llm = Ollama(model=\"phi\", temperature=0)#, streaming=True)\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm, retriever=retriever, memory=memory, verbose=True\n",
    ")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "if len(msgs.messages) == 0 or st.sidebar.button(\"Clear message history\"):\n",
    "    msgs.clear()\n",
    "    msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "avatars = {\"human\": \"user\", \"ai\": \"assistant\"}\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "for msg in msgs.messages:\n",
    "    st.chat_message(avatars[msg.type]).write(msg.content)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "if user_query := st.chat_input(placeholder=\"Ask me anything!\"):\n",
    "    st.chat_message(\"user\").write(user_query)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        retrieval_handler = PrintRetrievalHandler(st.container())\n",
    "        stream_handler = StreamHandler(st.empty())\n",
    "        response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad327b-0772-49a3-aa2e-c285af849537",
   "metadata": {},
   "source": [
    "# Free Up GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76743f90-8f26-49b9-8602-def48ebd3b26",
   "metadata": {},
   "source": [
    "    nvidia-smi\n",
    "    \n",
    "    sudo fuser -v /dev/nvidia*\n",
    "\n",
    "    sudo kill -9 PID\n",
    "\n",
    "    nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca934f4a-da00-453a-b712-0dccdc7f8044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
