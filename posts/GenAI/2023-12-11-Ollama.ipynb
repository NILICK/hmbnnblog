{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3ba97c8e-6b29-4c33-8aa4-3a8578b7cc4e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ollama Using\"\n",
    "description: \"This post is about Ollama installing and setting for private chatbots.\"\n",
    "author: \"Me\"\n",
    "date: \"2023-12-11\"\n",
    "categories: [GenAI, Ollama, Docker, Streamlit, LLMs, CUDA, CPU, ChatPDF, Langchain]\n",
    "image: images/ollama.png\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5db27-105c-490d-b121-bb7526d822ee",
   "metadata": {},
   "source": [
    "## Install Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cccc3e-1632-4505-8cf8-5107a02d6cad",
   "metadata": {},
   "source": [
    "___src: https://docs.docker.com/engine/install/ubuntu/___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc857c4c-59df-44bd-8005-2499cdee028d",
   "metadata": {},
   "source": [
    "1- Uninstall old versions:\n",
    "\n",
    "The unofficial packages to uninstall are:\n",
    "\n",
    "docker.io</br>\n",
    "docker-compose</br>\n",
    "docker-compose-v2</br>\n",
    "docker-doc</br>\n",
    "podman-docker</br>\n",
    "\n",
    "    for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n",
    "\n",
    "2- Install from a package:\n",
    "\n",
    "If you can't use Docker's apt repository to install Docker Engine, you can download the deb file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.\n",
    "\n",
    "Go to `https://download.docker.com/linux/ubuntu/dists/`.\n",
    "\n",
    "Select your Ubuntu version in the list.\n",
    "\n",
    "Go to pool/stable/ and select the applicable architecture (amd64, armhf, arm64, or s390x).\n",
    "\n",
    "Download the following deb files for the Docker Engine, CLI, containerd, and Docker Compose packages:\n",
    "\n",
    "    containerd.io_<version>_<arch>.deb\n",
    "    docker-ce_<version>_<arch>.deb\n",
    "    docker-ce-cli_<version>_<arch>.deb\n",
    "    docker-buildx-plugin_<version>_<arch>.deb\n",
    "    docker-compose-plugin_<version>_<arch>.deb\n",
    "    \n",
    "Install the .deb packages. Update the paths in the following example to where you downloaded the Docker packages.\n",
    "\n",
    " sudo dpkg -i ./containerd.io_<version>_<arch>.deb \\\n",
    "  ./docker-ce_<version>_<arch>.deb \\\n",
    "  ./docker-ce-cli_<version>_<arch>.deb \\\n",
    "  ./docker-buildx-plugin_<version>_<arch>.deb \\\n",
    "  ./docker-compose-plugin_<version>_<arch>.deb\n",
    "  \n",
    "The Docker daemon starts automatically.\n",
    "\n",
    "Verify that the Docker Engine installation is successful by running the hello-world image.\n",
    "\n",
    "     sudo service docker start\n",
    "     sudo docker run hello-world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de16df-a6d2-4b5e-a9df-86499a9479e5",
   "metadata": {},
   "source": [
    "## Install CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7637d2-857d-45e8-a130-fa5febf8f1fc",
   "metadata": {},
   "source": [
    "Install CUDA for using GPU\n",
    "\n",
    "1- sudo apt update</br>\n",
    "2- sudo apt upgrade</br>\n",
    "3- sudo apt install ubuntu-drivers-common</br>\n",
    "4- sudo ubuntu-drivers devices</br>\n",
    "5- recommends the NVIDIA driver 535</br>\n",
    "\n",
    "\tdriver   : nvidia-driver-535 - distro non-free recommended\n",
    "\t\n",
    "6- sudo apt install nvidia-driver-535</br>\n",
    "7- Reboot</br>\n",
    "8- Using NVIDIA icon in top page change to \"Switch to: NVIDIA (On-Demand) and then Logout.</br>\n",
    "9- nvidia-smi</br>\n",
    "\n",
    "you must see a table. At the top of the table, we will see the driver version and CUDA driver API compatibility:\n",
    "\n",
    "\tNVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2\n",
    "\n",
    "10- sudo apt install gcc</br>\n",
    "11- Install CUDA toolkit Ubuntu</br>\n",
    "\n",
    "src:https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network\n",
    "\n",
    "\twget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "\tsudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "\tsudo apt-get update  ((for this step you must use VPN such as windscribe:( ))\n",
    "\tsudo apt-get -y install cuda-toolkit-12-3 ((for this step you must use VPN such as windscribe:( ))\n",
    "\t\n",
    "If you encounter dependency errors during the installation, try running `sudo apt --fix-broken install` to fix them. Apt will suggest running it if needed.\n",
    "\n",
    "12- Reboot</br>\n",
    "13- Environment setup</br>\n",
    "\n",
    "We will now proceed to update the environment variables as recommended by the NVIDIA documentation.\n",
    "Add the following line to your `.bashrc` file using `nano ~/.bashrc` and paste the following lines at the end of the file.\n",
    "\n",
    "\texport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "\texport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\\\n",
    "                         \t ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n",
    "\n",
    "Save the file.\n",
    "\n",
    "14- Reboot\n",
    "\t\n",
    "15- Test the CUDA toolkit</br>\n",
    "\t\n",
    "    nvcc -V\n",
    "\t\n",
    "You must see:\n",
    "\n",
    "\tnvcc: NVIDIA (R) Cuda compiler driver\n",
    "\tCopyright (c) 2005-2023 NVIDIA Corporation\n",
    "\tBuilt on Fri_Nov__3_17:16:49_PDT_2023\n",
    "\tCuda compilation tools, release 12.3, V12.3.103\n",
    "\tBuild cuda_12.3.r12.3/compiler.33492891_0\n",
    "\n",
    "src: https://www.cherryservers.com/blog/install-cuda-ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e9005-a2f0-4137-ace7-e89b697f35c6",
   "metadata": {},
   "source": [
    "## Install Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6b019-f0a9-4c76-97f7-7121bbc8d6b0",
   "metadata": {},
   "source": [
    "src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md\n",
    "\n",
    "1- Create a user for Ollama:\n",
    "\n",
    "    sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "2- Create a service file in `/etc/systemd/system/ollama.service`:\n",
    "\n",
    "    [Unit]\n",
    "    Description=Ollama Service\n",
    "    After=network-online.target\n",
    "    \n",
    "    [Service]\n",
    "    ExecStart=/usr/bin/ollama serve\n",
    "    User=ollama\n",
    "    Group=ollama\n",
    "    Restart=always\n",
    "    RestartSec=3\n",
    "    \n",
    "    [Install]\n",
    "    WantedBy=default.target\n",
    "\n",
    "3- Then start the service:\n",
    "\n",
    "    sudo systemctl daemon-reload\n",
    "    sudo systemctl enable ollama\n",
    "\n",
    "4- Start Ollama using systemd:\n",
    "\n",
    "    sudo systemctl start ollama\n",
    "\n",
    "5- Update ollama by downloading the ollama binary:\n",
    "\n",
    "    sudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "    sudo chmod +x /usr/bin/ollama\n",
    "\n",
    "6- To view logs of Ollama running as a startup service, run:\n",
    "\n",
    "    journalctl -u ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618606d-f966-4557-8283-09786184f0f8",
   "metadata": {},
   "source": [
    "## Ollama Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49635e65-e6ad-4040-97ef-c4f4c6c4ee11",
   "metadata": {},
   "source": [
    "src:https://hub.docker.com/r/ollama/ollama\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only:\n",
    "\n",
    "    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3- Nvidia GPU with Apt\n",
    "\n",
    "3-1- Configure the repository:\n",
    "\n",
    "    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "    sudo apt-get update\n",
    "\n",
    "3-2- Install the NVIDIA Container Toolkit packages\n",
    "\n",
    "    sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "3-3- Configure Docker to use Nvidia driver:\n",
    "\n",
    "    sudo nvidia-ctk runtime configure --runtime=docker\n",
    "    sudo systemctl restart docker\n",
    "\n",
    "3-4- Start the container:\n",
    "\n",
    "    docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "3-5- Run model locally\n",
    "\n",
    "Now you can run a model:\n",
    "\n",
    "    docker exec -it ollama ollama run llama2 \n",
    "\n",
    "for the first time this code will download your desired model [Ollama Models](https://ollama.ai/library).\n",
    "\n",
    "4- If you want run again and again model use below code in terminal and again 2 (for only CPU) and 3-4(GPU):\n",
    "\n",
    "    sudo docker ps -a\n",
    "    sudo docker stop ollama\n",
    "    sudo docker rm ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5a420-a69f-4cda-b00b-74b85acb80ae",
   "metadata": {},
   "source": [
    "# Latest Method For Ollama Installation with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd7474-f1fc-4550-b7aa-b713a4c291a3",
   "metadata": {},
   "source": [
    "[Build your own RAG and run it locally: Langchain + Ollama + Streamlit](https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\n",
    "[Install Ollama](src: https://github.com/jmorganca/ollama/blob/main/docs/linux.md#manual-install)\n",
    "\n",
    "1- Install Ollama in linux:\n",
    "\n",
    "\tsudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "\tsudo chmod +x /usr/bin/ollama\n",
    "\n",
    "2- Create a user for Ollama:\n",
    "\n",
    "\tsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n",
    "\n",
    "3- Create a service file in /etc/systemd/system/ollama.service:\n",
    "\n",
    "\t[Unit]\n",
    "\tDescription=Ollama Service\n",
    "\tAfter=network-online.target\n",
    "\n",
    "\t[Service]\n",
    "\tExecStart=/usr/bin/ollama serve\n",
    "\tUser=ollama\n",
    "\tGroup=ollama\n",
    "\tRestart=always\n",
    "\tRestartSec=3\n",
    "\n",
    "\t[Install]\n",
    "\tWantedBy=default.target\n",
    "\n",
    "4- Then start the service:\n",
    "\n",
    "\tsudo systemctl daemon-reload\n",
    "\tudo systemctl enable ollama\n",
    "\t\n",
    "5- Start Ollama using systemd:\n",
    "\n",
    "\tsudo systemctl start ollama\n",
    "\n",
    "\n",
    "============================================\n",
    "\n",
    "## Above Steps not Worked in Ubuntu\n",
    "\n",
    "============================================\n",
    "\n",
    "Using Ollama as Docker (src:https://hub.docker.com/r/ollama/ollama)\n",
    "\n",
    "1- docker pull ollama/ollama\n",
    "\n",
    "2- CPU only\n",
    "\n",
    "\tdocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\t\n",
    "3- Using Ollama with GPU needs at least 4 GPU 4Gb (total 16 Gb), so I can only use CPU.\n",
    "\n",
    "4- Run model locally\n",
    "\n",
    "\tdocker exec -it ollama ollama run llama2\n",
    "\n",
    "6- Create new env with python < 3.12 \n",
    "\n",
    "\t- micromamba activate base\n",
    "\t- micromamba create -n ollama python=3.11\n",
    "\t- micromamba activate ollama\n",
    "\t\n",
    "7- Build the RAG pipeline (src:https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/)\n",
    "\n",
    "\t- pip install langchain==0.0.343\n",
    "\t- pip install streamlit==1.29.0\n",
    "\t- pip install streamlit-chat==0.1.1\n",
    "\t- pip install pypdf==3.17.1\n",
    "\t- pip install fastembed==0.1.1\n",
    "\t- pip install openai==1.3.6\n",
    "\t- pip install langchainhub==0.1.14\n",
    "\t- pip install chromadb==0.4.18\n",
    "\t- pip install watchdog==3.0.0\n",
    "\n",
    "8- Some Commands with Ollama\n",
    "\n",
    "    sudo docker ps -a # List of AvailableModels\n",
    "    sudo docker stop ollama # Stop the current model\n",
    "    sudo docker rm ollama # Remove the current model\n",
    "\n",
    "9- How do I clean the memory cache?\n",
    "\n",
    "    sync && echo 3 | sudo tee /proc/sys/vm/drop_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218324-0645-4bf2-be35-068340d096f4",
   "metadata": {},
   "source": [
    "# Using Streamlit with Ollama and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09146a-5235-4e42-9081-e7e91cead32b",
   "metadata": {},
   "source": [
    "Save below code in a file with name myapp.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ee7d2-aaf2-4103-bf3c-c0b18acb3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from streamlit_chat import message\n",
    "from langchain.llms import Ollama #Cohere\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #CohereEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "__import__(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Setting Up Streamlit Page\n",
    "st.set_page_config(page_title=\"Ollama Chatbot\", page_icon= \"ðŸ’¬\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.title('ðŸ’¬ OLLAMA Chatbot')\n",
    "    \n",
    "    #st.divider()\n",
    "    # Select the model\n",
    "    selected_model = st.selectbox('Choose a model', ['Phi-2', 'Llama2', \"Orca-mini\",\n",
    "                                                     'Zephyr', 'Code Llama', 'Mistral'],\n",
    "                                   key='selected_model')\n",
    "\n",
    "    if selected_model == \"Phi-2\":\n",
    "        llm_model = \"phi\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Phi-2............................\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Orca-mini\":\n",
    "        llm_model = \"orca-mini\"\n",
    "        st.caption(\"\"\"\n",
    "                   Orca-mini..................................\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Code llama\":\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\")\n",
    "    elif selected_model == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\")\n",
    "    #st.divider()\n",
    "    temp_r = st.slider(\"Temperature\", 0.0, 0.9, 0.0, 0.1)\n",
    "    chunk_size = st.slider(\"Chunk Size for Splitting Document \", 256, 1024, 400, 10)\n",
    "    chunk_overlap = st.slider(\"Chunk Overlap \", 0, 100, 20, 5)\n",
    "    clear_button = st.button(\"Clear Conversation\", key=\"clear\")\n",
    "\n",
    "#-----------------------Functions-------------------------------#\n",
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device': 'cuda'}, #  you can set model_kwargs={'device': 'cuda:0'} for the first GPU, model_kwargs={'device': 'cuda:1'} for the second GPU, and so on.(src:https://github.com/langchain-ai/langchain/issues/10436)\n",
    "        #model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Creating the chain for Question Answering\n",
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# tabs\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ’¬ Chatbot\", \"ðŸ–¹ ChatPDFs\", \"ðŸ“ˆ ChatPandas\", \"ðŸŒ ChatMaps\"])\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Chatbot Tab\n",
    "# with tab1():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPDFs Tab\n",
    "with tab2:\n",
    "\t# Upload PDF files\n",
    "    uploaded_PDF_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True, type=\"pdf\")\n",
    "\n",
    "if uploaded_PDF_files:\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        for uploaded_file in uploaded_PDF_files:\n",
    "            file_name = uploaded_file.name\n",
    "            file_content = uploaded_file.read()\n",
    "            st.write(\"Filename: \", file_name)\n",
    "\n",
    "            # Write the content of the PDF files to a temporary directory\n",
    "            with open(os.path.join(tmpdir, file_name), \"wb\") as file:\n",
    "                file.write(file_content)\n",
    "\n",
    "        # Load the PDF files from the temporary directory\n",
    "        loader = DirectoryLoader(tmpdir, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split the PDF files into smaller chunks of text\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        documents = text_splitter.split_documents(documents)\n",
    "        embeddings = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "        #vectorstore.persist()\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        prompt_template = \"\"\" \n",
    "        System Prompt:\n",
    "        Your are an AI chatbot that helps users chat with PDF documents. How may I help you today?\n",
    "\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        \"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=llm_model, temperature=temp_r),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        )\n",
    "        # Question-Answer\n",
    "        # Get the user question\n",
    "        query = st.text_input(\"Ask a question:\")\n",
    "\n",
    "        if query:\n",
    "                response = chain({'query': query})\n",
    "                # Wrapping the text for better output in Jupyter Notebook\n",
    "                wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "                # Display the answer\n",
    "                st.markdown(f\"**Q:** {query}\")\n",
    "                st.markdown(f\"**A:** {wrapped_text}\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatPandas Tab\n",
    "# with tab3():\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# ChatMaps Tab\n",
    "# with tab4():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f9ed9-7a98-4035-86dc-38002633fd7c",
   "metadata": {},
   "source": [
    "After Run Ollama docker run below code:\n",
    "\n",
    "    streamlit run myapp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa32432-c89b-43f3-9781-a517326fa7f8",
   "metadata": {},
   "source": [
    "============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f680d4-d4d8-4a53-892e-8846bc33af0a",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "\n",
    "In 2023-Dec-20, Ollama pulled `Phi-2` model. This `Phi-2 Model` is a Small Language Model (SLM) type and released by `Microsoft` In 2023-Dec-15. This small model is very incredible model that with 1.6GB and 2.7B parameter is very `????????????????`  .\n",
    "\n",
    "For use this model. I first update `Ollama Docker` using `????????????????`.\n",
    "\n",
    "After that:\n",
    "\n",
    "    sudo docker stop ollama\n",
    "    sudo docker rm ollama\n",
    "\n",
    "Because `Phi-2 model` is small, so I can use `CUDA` with this model. So I can active `GPU` using:\n",
    "\n",
    "    docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "** I remeber that for other models such as `llama2, Orca-mini, Zephyr, Code Llama, Mistral and other 7B models` I only use `CPU` using:\n",
    "\n",
    "    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "After activing the `GPU` or `CPU` I can run `Streamlit` for using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55eba4b-703a-4724-9995-47b1962dbc8c",
   "metadata": {},
   "source": [
    "============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923066e9-172b-4ae9-900f-e886bc50521e",
   "metadata": {},
   "source": [
    "## Building Own LLM model using GGUF from Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78256169-2390-4437-9edc-aff7de57137e",
   "metadata": {},
   "source": [
    "Refer Ollama Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc01b1-9423-4142-94bf-494df64f29cc",
   "metadata": {},
   "source": [
    "huggingface-cli download TheBloke/MistralLite-7B-GGUF mistrallite.Q4_K_M.gguf --local-dir downloads --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "huggingface-cli download kroonen/phi-2-GGUF phi-2_Q4_K_M.gguf --local-dir downloads --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "huggingface-cli download TheBloke/Mistral-7B-OpenOrca-GGUF mistral-7b-openorca.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de81ee-608c-445b-bb0a-fd5556857553",
   "metadata": {},
   "source": [
    "# New ChatPDF with Streamlit-Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5bf9b-5337-41b5-b7aa-becd6bb50b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "st.set_page_config(page_title=\"Chat with Documents\", page_icon=\"ðŸ“–\")\n",
    "st.title(\"ðŸ“š: Chat with Documents\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "MODES = (\"CPU\", \"GPU\")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.title('ðŸ’¬ OLLAMA Chatbot')\n",
    "\n",
    "    # Select Processing Mode\n",
    "    mode = st.radio(\"Choose a mode\", MODES)\n",
    "    if mode == \"CPU\":\n",
    "        device = \"cpu\"\n",
    "    elif mode == \"GPU\":\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Select the model\n",
    "    model_name = st.selectbox('Choose a model: ', ['Phi-2', 'Llama2', \"Orca-mini\",\n",
    "                                                 'Zephyr', 'Code Llama', 'Mistral'],\n",
    "                              key='model_name')\n",
    "\n",
    "    if model_name == \"Phi-2\":\n",
    "        llm_model = \"phi\"\n",
    "        st.caption(\"\"\"\n",
    "                   Phi-2: a 2.7B language model by Microsoft Research\n",
    "\t\t\t\t   that demonstrates outstanding reasoning and language understanding capabilities.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Orca-mini\":\n",
    "        llm_model = \"orca-mini\"\n",
    "        st.caption(\"\"\"\n",
    "                   A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Code llama\":\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\")\n",
    "    #st.divider()\n",
    "    temp_r = st.slider(\"Temperature\", 0.0, 0.9, 0.0, 0.1)\n",
    "    chunk_size = st.slider(\"Chunk Size for Splitting Document \", 200, 3000, 1500, 20)\n",
    "    chunk_overlap = st.slider(\"Chunk Overlap \", 0, 500, 200, 10)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "@st.cache_resource(ttl=\"1h\")\n",
    "def configure_retriever(uploaded_files):\n",
    "    # Read documents\n",
    "    docs = []\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    for file in uploaded_files:\n",
    "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "        with open(temp_filepath, \"wb\") as f:\n",
    "            f.write(file.getvalue())\n",
    "        loader = PyPDFLoader(temp_filepath)\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create embeddings and store in vectordb\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': device},\n",
    "                                       encode_kwargs = {'normalize_embeddings': True # keep True to compute cosine similarity\n",
    "                                                        })\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "\n",
    "    # Define retriever\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "        self.run_id_ignore_token = None\n",
    "\n",
    "    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):\n",
    "        # Workaround to prevent showing the rephrased question as output\n",
    "        if prompts[0].startswith(\"Human\"):\n",
    "            self.run_id_ignore_token = kwargs.get(\"run_id\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        if self.run_id_ignore_token == kwargs.get(\"run_id\", False):\n",
    "            return\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "class PrintRetrievalHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container):\n",
    "        self.status = container.status(\"**Context Retrieval**\")\n",
    "\n",
    "    def on_retriever_start(self, serialized: dict, query: str, **kwargs):\n",
    "        self.status.write(f\"**Question:** {query}\")\n",
    "        self.status.update(label=f\"**Context Retrieval:** {query}\")\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        for idx, doc in enumerate(documents):\n",
    "            source = os.path.basename(doc.metadata[\"source\"])\n",
    "            self.status.write(f\"**Document {idx} from {source}**\")\n",
    "            self.status.markdown(doc.page_content)\n",
    "        self.status.update(state=\"complete\")\n",
    "#--------------------------------------------------------------#\n",
    "# Upload PDF Files\n",
    "uploaded_files = st.file_uploader(\n",
    "    label=\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True\n",
    ")\n",
    "if not uploaded_files:\n",
    "    st.info(\"Please upload PDF documents to continue.\")\n",
    "    st.stop()\n",
    "\n",
    "retriever = configure_retriever(uploaded_files)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Setup memory for contextual conversation\n",
    "msgs = StreamlitChatMessageHistory()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
    "\n",
    "# Setup LLM and QA chain\n",
    "llm = Ollama(model=llm_model, temperature=temp_r)#, streaming=True)\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm, retriever=retriever, memory=memory, verbose=True\n",
    ")\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "if len(msgs.messages) == 0 or st.sidebar.button(\"Clear message history\"):\n",
    "    msgs.clear()\n",
    "    msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "avatars = {\"human\": \"user\", \"ai\": \"assistant\"}\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "for msg in msgs.messages:\n",
    "    st.chat_message(avatars[msg.type]).write(msg.content)\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "if user_query := st.chat_input(placeholder=\"Ask me anything!\"):\n",
    "    st.chat_message(\"user\").write(user_query)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        retrieval_handler = PrintRetrievalHandler(st.container())\n",
    "        stream_handler = StreamHandler(st.empty())\n",
    "        response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad327b-0772-49a3-aa2e-c285af849537",
   "metadata": {},
   "source": [
    "# Free Up GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76743f90-8f26-49b9-8602-def48ebd3b26",
   "metadata": {},
   "source": [
    "    nvidia-smi\n",
    "    \n",
    "    sudo fuser -v /dev/nvidia*\n",
    "\n",
    "    sudo kill -9 PID\n",
    "\n",
    "    nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3577bdb-00ad-4fa2-9072-487d5e1f6fcd",
   "metadata": {},
   "source": [
    "# Chat Maps(NetCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbb3de-47df-41ad-a163-bb7e3d4c04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "#import yaml\n",
    "import folium\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import streamlit as st\n",
    "import geopandas as gpd\n",
    "from pyproj import Geod\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from geopy.geocoders import Nominatim\n",
    "from streamlit_folium import st_folium\n",
    "from requests.exceptions import Timeout\n",
    "from streamlit_folium import folium_static ################\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "st.markdown(\n",
    "            f'''\n",
    "            <style>\n",
    "                .reportview-container .sidebar-content {{\n",
    "                    padding-top: {0.1}rem;\n",
    "                }}\n",
    "                .reportview-container .main .block-container {{\n",
    "                    padding-top: {1}rem;\n",
    "                }}\n",
    "            </style>\n",
    "            ''',unsafe_allow_html=True)\n",
    "#-----------------------------------------------------------------#\n",
    "data_path = \"./data/\"\n",
    "coastline_shapefile = \"./data/natural_earth/coastlines/ne_50m_coastline.shp\"\n",
    "clicked_coords = None\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "system_role = \"\"\"\n",
    "You are the system that should help people to evaluate the impact of climate change\n",
    "on decisions they are taking today (e.g. install wind turbines, solar panels, build a building,\n",
    "parking lot, open a shop, buy crop land). You are working with data on a local level,\n",
    "and decisions also should be given for particular locations. You will be given information \n",
    "about changes in environmental variables for particular location, and how they will \n",
    "change in a changing climate. Your task is to provide assessment of potential risks \n",
    "and/or benefits for the planned activity related to change in climate. Use information \n",
    "about the country to retrieve information about policies and regulations in the \n",
    "area related to climate change, environmental use and activity requested by the user.\n",
    "You don't have to use all variables provided to you, if the effect is insignificant,\n",
    "don't use variable in analysis. DON'T just list information about variables, don't \n",
    "just repeat what is given to you as input. I don't want to get the code, \n",
    "I want to receive a narrative, with your assessments and advice. Format \n",
    "your response as MARKDOWN, don't use Heading levels 1 and 2.\n",
    "\"\"\"\n",
    "\n",
    "content_message = \"{user_message} \\n \\\n",
    "      Location: latitude = {lat}, longitude = {lon} \\\n",
    "      Adress: {location_str} \\\n",
    "      Policy: {policy} \\\n",
    "      Distance to the closest coastline: {distance_to_coastline} \\\n",
    "      Elevation above sea level: {elevation} \\\n",
    "      Current landuse: {current_land_use} \\\n",
    "      Current soil type: {soil} \\\n",
    "      Current mean monthly temperature for each month: {hist_temp_str} \\\n",
    "      Future monthly temperatures for each month at the location: {future_temp_str}\\\n",
    "      Curent precipitation flux (mm/month): {hist_pr_str} \\\n",
    "      Future precipitation flux (mm/month): {future_pr_str} \\\n",
    "      Curent u wind component (in m/s): {hist_uas_str} \\\n",
    "      Future u wind component (in m/s): {future_uas_str} \\\n",
    "      Curent v wind component (in m/s): {hist_vas_str} \\\n",
    "      Future v wind component (in m/s): {future_vas_str} \\\n",
    "      \"\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    \"\"\"\n",
    "    Taken from here: https://discuss.streamlit.io/t/langchain-stream/43782\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, container, initial_text=\"\", display_method=\"markdown\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "        self.display_method = display_method\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        display_function = getattr(self.container, self.display_method, None)\n",
    "        if display_function is not None:\n",
    "            display_function(self.text)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid display_method: {self.display_method}\")\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def get_location(lat, lon):\n",
    "    \"\"\"\n",
    "    Returns the address of a given latitude and longitude using the Nominatim geocoding service.\n",
    "\n",
    "    Parameters:\n",
    "    lat (float): The latitude of the location.\n",
    "    lon (float): The longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the address information of the location.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"climsight\")\n",
    "    location = geolocator.reverse((lat, lon), language=\"en\")\n",
    "    return location.raw[\"address\"]\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def get_adress_string(location):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing two strings:\n",
    "    1. A string representation of the location address with all the key-value pairs in the location dictionary.\n",
    "    2. A string representation of the location address with only the country, state, city and road keys in the location dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    location (dict): A dictionary containing the location address information.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two strings.\n",
    "    \"\"\"\n",
    "    location_str = \"Adress: \"\n",
    "    for key in location:\n",
    "        location_str = location_str + f\"{key}:{location[key]}, \"\n",
    "    location_str_for_print = \"**Address:** \"\n",
    "    if \"country\" in location:\n",
    "        location_str_for_print += f\"{location['country']}, \"\n",
    "    if \"state\" in location:\n",
    "        location_str_for_print += f\"{location['state']}, \"\n",
    "    if \"city\" in location:\n",
    "        location_str_for_print += f\"{location['city']}, \"\n",
    "    if \"road\" in location:\n",
    "        location_str_for_print += f\"{location['road']}\"\n",
    "    return location_str, location_str_for_print\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def closest_shore_distance(lat: float, lon: float, coastline_shapefile: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the closest distance between a given point (lat, lon) and the nearest point on the coastline.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude of the point\n",
    "        lon (float): Longitude of the point\n",
    "        coastline_shapefile (str): Path to the shapefile containing the coastline data\n",
    "\n",
    "    Returns:\n",
    "        float: The closest distance between the point and the coastline, in meters.\n",
    "    \"\"\"\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    min_distance = float(\"inf\")\n",
    "\n",
    "    coastlines = gpd.read_file(coastline_shapefile)\n",
    "\n",
    "    for _, row in coastlines.iterrows():\n",
    "        geom = row[\"geometry\"]\n",
    "        if geom.geom_type == \"MultiLineString\":\n",
    "            for line in geom.geoms:\n",
    "                for coastal_point in line.coords:\n",
    "                    _, _, distance = geod.inv(\n",
    "                        lon, lat, coastal_point[0], coastal_point[1]\n",
    "                    )\n",
    "                    min_distance = min(min_distance, distance)\n",
    "        else:  # Assuming LineString\n",
    "            for coastal_point in geom.coords:\n",
    "                _, _, distance = geod.inv(lon, lat, coastal_point[0], coastal_point[1])\n",
    "                min_distance = min(min_distance, distance)\n",
    "\n",
    "    return min_distance\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def get_elevation_from_api(lat, lon):\n",
    "    \"\"\"\n",
    "    Get the elevation of a location using the Open Topo Data API.\n",
    "\n",
    "    Parameters:\n",
    "    lat (float): The latitude of the location.\n",
    "    lon (float): The longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    float: The elevation of the location in meters.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.opentopodata.org/v1/etopo1?locations={lat},{lon}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data[\"results\"][0][\"elevation\"]\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def fetch_land_use(lon, lat):\n",
    "    \"\"\"\n",
    "    Fetches land use data for a given longitude and latitude using the Overpass API.\n",
    "\n",
    "    Args:\n",
    "    - lon (float): The longitude of the location to fetch land use data for.\n",
    "    - lat (float): The latitude of the location to fetch land use data for.\n",
    "\n",
    "    Returns:\n",
    "    - data (dict): A dictionary containing the land use data for the specified location.\n",
    "    \"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    is_in({lat},{lon})->.a;\n",
    "    area.a[\"landuse\"];\n",
    "    out tags;\n",
    "    \"\"\"\n",
    "    response = requests.get(overpass_url, params={\"data\": overpass_query})\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def get_soil_from_api(lat, lon):\n",
    "    \"\"\"\n",
    "    Retrieves the soil type at a given latitude and longitude using the ISRIC SoilGrids API.\n",
    "\n",
    "    Parameters:\n",
    "    lat (float): The latitude of the location.\n",
    "    lon (float): The longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    str: The name of the World Reference Base (WRB) soil class at the given location.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://rest.isric.org/soilgrids/v2.0/classification/query?lon={lon}&lat={lat}&number_classes=5\"\n",
    "        response = requests.get(url, timeout=2)  # Set timeout to 2 seconds\n",
    "        data = response.json()\n",
    "        return data[\"wrb_class_name\"]\n",
    "    except Timeout:\n",
    "        return \"not found\"\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    hist = xr.open_mfdataset(f\"{data_path}/AWI_CM_mm_historical*.nc\", compat=\"override\")\n",
    "    future = xr.open_mfdataset(f\"{data_path}/AWI_CM_mm_ssp585*.nc\", compat=\"override\")\n",
    "    return hist, future\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "def convert_to_mm_per_month(monthly_precip_kg_m2_s1):\n",
    "    days_in_months = np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\n",
    "    return monthly_precip_kg_m2_s1 * 60 * 60 * 24 * days_in_months\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "@st.cache_data\n",
    "def extract_climate_data(lat, lon, _hist, _future):\n",
    "    \"\"\"\n",
    "    Extracts climate data for a given latitude and longitude from historical and future datasets.\n",
    "\n",
    "    Args:\n",
    "    - lat (float): Latitude of the location to extract data for.\n",
    "    - lon (float): Longitude of the location to extract data for.\n",
    "    - _hist (xarray.Dataset): Historical climate dataset.\n",
    "    - _future (xarray.Dataset): Future climate dataset.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing present day and future temperature, precipitation, and wind speed data for each month of the year.\n",
    "    - data_dict (dict): Dictionary containing string representations of the extracted climate data.\n",
    "    \"\"\"\n",
    "    hist_temp = hist.sel(lat=lat, lon=lon, method=\"nearest\")[\"tas\"].values - 273.15\n",
    "    hist_temp_str = np.array2string(hist_temp.ravel(), precision=3, max_line_width=100)[\n",
    "        1:-1\n",
    "    ]\n",
    "\n",
    "    hist_pr = hist.sel(lat=lat, lon=lon, method=\"nearest\")[\"pr\"].values\n",
    "    hist_pr = convert_to_mm_per_month(hist_pr)\n",
    "\n",
    "    hist_pr_str = np.array2string(hist_pr.ravel(), precision=3, max_line_width=100)[\n",
    "        1:-1\n",
    "    ]\n",
    "\n",
    "    hist_uas = hist.sel(lat=lat, lon=lon, method=\"nearest\")[\"uas\"].values\n",
    "    hist_uas_str = np.array2string(hist_uas.ravel(), precision=3, max_line_width=100)[\n",
    "        1:-1\n",
    "    ]\n",
    "\n",
    "    hist_vas = hist.sel(lat=lat, lon=lon, method=\"nearest\")[\"vas\"].values\n",
    "    hist_vas_str = np.array2string(hist_vas.ravel(), precision=3, max_line_width=100)[\n",
    "        1:-1\n",
    "    ]\n",
    "\n",
    "    future_temp = future.sel(lat=lat, lon=lon, method=\"nearest\")[\"tas\"].values - 273.15\n",
    "    future_temp_str = np.array2string(\n",
    "        future_temp.ravel(), precision=3, max_line_width=100\n",
    "    )[1:-1]\n",
    "\n",
    "    future_pr = future.sel(lat=lat, lon=lon, method=\"nearest\")[\"pr\"].values\n",
    "    future_pr = convert_to_mm_per_month(future_pr)\n",
    "    future_pr_str = np.array2string(future_pr.ravel(), precision=3, max_line_width=100)[\n",
    "        1:-1\n",
    "    ]\n",
    "\n",
    "    future_uas = future.sel(lat=lat, lon=lon, method=\"nearest\")[\"uas\"].values\n",
    "    future_uas_str = np.array2string(\n",
    "        future_uas.ravel(), precision=3, max_line_width=100\n",
    "    )[1:-1]\n",
    "\n",
    "    future_vas = future.sel(lat=lat, lon=lon, method=\"nearest\")[\"vas\"].values\n",
    "    future_vas_str = np.array2string(\n",
    "        future_vas.ravel(), precision=3, max_line_width=100\n",
    "    )[1:-1]\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Present day Temperature\": hist_temp[0, 0, :],\n",
    "            \"Future Temeprature\": future_temp[0, 0, :],\n",
    "            \"Present day Precipitation\": hist_pr[0, 0, :],\n",
    "            \"Future Precipitation\": future_pr[0, 0, :],\n",
    "            \"Present day Wind speed\": np.hypot(hist_uas[0, 0, :], hist_vas[0, 0, :]),\n",
    "            \"Future Wind speed\": np.hypot(future_uas[0, 0, :], future_vas[0, 0, :]),\n",
    "            \"Month\": range(1, 13),\n",
    "        }\n",
    "    )\n",
    "    data_dict = {\n",
    "        \"hist_temp\": hist_temp_str,\n",
    "        \"hist_pr\": hist_pr_str,\n",
    "        \"hist_uas\": hist_uas_str,\n",
    "        \"hist_vas\": hist_vas_str,\n",
    "        \"future_temp\": future_temp_str,\n",
    "        \"future_pr\": future_pr_str,\n",
    "        \"future_uas\": future_uas_str,\n",
    "        \"future_vas\": future_vas_str,\n",
    "    }\n",
    "    return df, data_dict\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "hist, future = load_data()\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "st.title(\n",
    "    \" :deciduous_tree:  Environmental Assessment\"\n",
    ") \n",
    "# Emoji list: https://www.fileformat.info/info/emoji/list.htm\n",
    "#\" :cyclone: :ocean: :globe_with_meridians:  Climate Foresight\"\n",
    "# :umbrella_with_rain_drops: :earth_africa:  :tornado:\n",
    "user_message = st.text_input(\n",
    "    \"Describe activity you would like to evaluate for this location:\"\n",
    ")\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "col1, col2 = st.columns(2)\n",
    "lat_default = 33.2583\n",
    "lon_default = 51.3081\n",
    "\n",
    "m = folium.Map(location=[lat_default, lon_default], zoom_start=9)\n",
    "#--------------------------------------------------------------#\n",
    "MODES = (\"CPU\", \"GPU\")\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "with st.sidebar:\n",
    "    st.info(\n",
    "    \"Click on your desired location\")\n",
    "    \n",
    "    map_data = st_folium(m, width = 300, height=300)\n",
    "\n",
    "    if map_data:\n",
    "        clicked_coords = map_data[\"last_clicked\"]\n",
    "        if clicked_coords:\n",
    "            lat_default = clicked_coords[\"lat\"]\n",
    "            lon_default = clicked_coords[\"lng\"]\n",
    "\n",
    "    lat = col1.number_input(\"Latitude\", value=lat_default, format=\"%.4f\")\n",
    "    lon = col2.number_input(\"Longitude\", value=lon_default, format=\"%.4f\")\n",
    "\n",
    "    # Select Processing Mode\n",
    "    mode = st.radio(\"Choose a mode\", MODES)\n",
    "    if mode == \"CPU\":\n",
    "        device = \"cpu\"\n",
    "    elif mode == \"GPU\":\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Select the model\n",
    "    model_name = st.selectbox('Choose a model: ', ['Phi-2', 'Llama2', \"Orca-mini\",\n",
    "                                                 'Zephyr', 'Code Llama', 'Mistral'],\n",
    "                              key='model_name')\n",
    "\n",
    "    if model_name == \"Phi-2\":\n",
    "        llm_model = \"phi\"\n",
    "        st.caption(\"\"\"\n",
    "                   Phi-2: a 2.7B language model by Microsoft Research\n",
    "\t\t\t\t   that demonstrates outstanding reasoning and language understanding capabilities.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Llama2\":\n",
    "        llm_model = \"llama2\"\n",
    "        st.caption(\"\"\"\n",
    "                   Llama 2 is released by Meta Platforms, Inc.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Orca-mini\":\n",
    "        llm_model = \"orca-mini\"\n",
    "        st.caption(\"\"\"\n",
    "                   A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Zephyr\":\n",
    "        llm_model = \"zephyr\"\n",
    "        st.caption(\"\"\"\n",
    "                   Zephyr is a 7 billion parameter model, fine-tuned on Mistral to achieve results similar to \n",
    "                   Llama 2 70B Chat in several benchmarks.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Code llama\":\n",
    "        llm_model = \"codellama\"\n",
    "        st.caption(\"\"\"\n",
    "                   Code Llama is a model for generating and discussing code, built on top of Llama 2.\n",
    "                   \"\"\")\n",
    "    elif model_name == \"Mistral\":\n",
    "        llm_model = \"mistral\"\n",
    "        st.caption(\"\"\"\n",
    "                   The Mistral 7B model released by Mistral AI.\n",
    "                   \"\"\")\n",
    "    folium.Marker(location=[lat, lon]).add_to(m)\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "if st.button(\"Generate\") and user_message:\n",
    "    with st.spinner(\"Getting info on a point...\"):\n",
    "        location = get_location(lat, lon)\n",
    "        location_str, location_str_for_print = get_adress_string(location)\n",
    "        st.markdown(f\"**Coordinates:** {round(lat, 4)}, {round(lon, 4)}\")\n",
    "        st.markdown(location_str_for_print)\n",
    "        elevation = get_elevation_from_api(lat, lon)\n",
    "        st.markdown(f\"**Elevation:** {elevation} m\")\n",
    "\n",
    "        land_use_data = fetch_land_use(lon, lat)\n",
    "        try:\n",
    "            current_land_use = land_use_data[\"elements\"][0][\"tags\"][\"landuse\"]\n",
    "        except:\n",
    "            current_land_use = \"Not known\"\n",
    "        st.markdown(f\"**Current land use:** {current_land_use}\")\n",
    "\n",
    "        soil = get_soil_from_api(lat, lon)\n",
    "        st.markdown(f\"**Soil type:** {soil}\")\n",
    "\n",
    "        distance_to_coastline = closest_shore_distance(lat, lon, coastline_shapefile)\n",
    "        st.markdown(f\"**Distance to the shore:** {round(distance_to_coastline, 2)} m\")\n",
    "\n",
    "        # create pandas dataframe\n",
    "        df, data_dict = extract_climate_data(lat, lon, hist, future)\n",
    "        # Plot the chart\n",
    "        st.text(\n",
    "            \"Near surface temperature [souce: AWI-CM-1-1-MR, historical and SSP5-8.5]\",\n",
    "        )\n",
    "        st.line_chart(\n",
    "            df,\n",
    "            x=\"Month\",\n",
    "            y=[\"Present day Temperature\", \"Future Temeprature\"],\n",
    "            color=[\"#d62728\", \"#2ca02c\"],\n",
    "        )\n",
    "        st.text(\n",
    "            \"Precipitation [souce: AWI-CM-1-1-MR, historical and SSP5-8.5]\",\n",
    "        )\n",
    "        st.line_chart(\n",
    "            df,\n",
    "            x=\"Month\",\n",
    "            y=[\"Present day Precipitation\", \"Future Precipitation\"],\n",
    "            color=[\"#d62728\", \"#2ca02c\"],\n",
    "        )\n",
    "        st.text(\n",
    "            \"Wind speed [souce: AWI-CM-1-1-MR, historical and SSP5-8.5]\",\n",
    "        )\n",
    "        st.line_chart(\n",
    "            df,\n",
    "            x=\"Month\",\n",
    "            y=[\"Present day Wind speed\", \"Future Wind speed\"],\n",
    "            color=[\"#d62728\", \"#2ca02c\"],\n",
    "        )\n",
    "    policy = \"\"\n",
    "    with st.spinner(\"Generating...\"):\n",
    "        chat_box = st.empty()\n",
    "        stream_handler = StreamHandler(chat_box, display_method=\"write\")\n",
    "        llm = Ollama(model=llm_model, temperature=0, callbacks=[stream_handler],)\n",
    "\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_role)\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(content_message)\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            [system_message_prompt, human_message_prompt]\n",
    "        )\n",
    "        chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=chat_prompt,\n",
    "            output_key=\"review\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        output = chain.run(\n",
    "            user_message=user_message,\n",
    "            lat=str(lat),\n",
    "            lon=str(lon),\n",
    "            location_str=location_str,\n",
    "            policy=policy,\n",
    "            distance_to_coastline=str(distance_to_coastline),\n",
    "            elevation=str(elevation),\n",
    "            current_land_use=current_land_use,\n",
    "            soil=soil,\n",
    "            hist_temp_str=data_dict[\"hist_temp\"],\n",
    "            future_temp_str=data_dict[\"future_temp\"],\n",
    "            hist_pr_str=data_dict[\"hist_pr\"],\n",
    "            future_pr_str=data_dict[\"future_pr\"],\n",
    "            hist_uas_str=data_dict[\"hist_uas\"],\n",
    "            future_uas_str=data_dict[\"future_uas\"],\n",
    "            hist_vas_str=data_dict[\"hist_vas\"],\n",
    "            future_vas_str=data_dict[\"future_vas\"],\n",
    "            verbose=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b91147-e114-46b7-921b-da9c80443959",
   "metadata": {},
   "source": [
    "Src: https://github.com/koldunovn/climsight/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef78f26-123f-49fa-b179-a01f7680bb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
