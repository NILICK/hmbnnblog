{
 "cells": [
  {
   "cell_type": "raw",
   "id": "898144f1-fac6-4ba6-8d5f-c28d64fe15e9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TopoPyScale\"\n",
    "description: \"Downscaling climate data based on topography.\"\n",
    "author: \"Me\"\n",
    "date: \"2023-03-23\"\n",
    "categories: [GeoSpatial, Downscale, Climate]\n",
    "image: images/downscale.jpeg\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e1a77-2f19-4815-ac7f-a87b7be4b9c9",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "__TopoPyScale__ is a downscaling toolbox for globmal and regional climate model datasets, particularly relevant to mountain ranges, and hillslopes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85750e04-70df-46d7-96f9-8817fa354d37",
   "metadata": {},
   "source": [
    "src: https://topopyscale.readthedocs.io/en/latest/m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7c7e0-7917-4f8a-8a65-db5748bbd281",
   "metadata": {},
   "source": [
    "`TopoPyScale` uses both climate model data and Digital Elevation Models (DEM) for correcting atmospheric state variables (e.g. temperature, pressure, humidity, etc). TopoPyScale provides tools to interpolate and correct such variables to be relevant locally given a topographical context.\n",
    "\n",
    "The most basic requirements of TopoPyScale is a DEM used to defined the spatial domain of interest as well as compute a number of morphometrics, and configuration file defining the temporal period, the downscaling methods and other parameters. In its current version, `TopoPyScale` includes the `topoclass` class that wraps all functionalities for ease of use. It automatically fetches data from the [ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview) repositories (Pressure and Surface levels). Other climate data sources can be added. Based on the high resolution (30-100m) DEM and the climate data, methods in the `topoclass` will compute, correct and interpolate variables need to force specialized land-surface models.\n",
    "\n",
    "Indeed, with this library you can download hourly ERA5 climate data and the downscale them in desires spatial resolution.\n",
    "\n",
    "Downscaled variable includes:\n",
    "\n",
    "    2m air temperature\n",
    "    2m air humidity\n",
    "    2m air pressure\n",
    "    10m wind speed and direction\n",
    "    Surface incoming shortwave radiation\n",
    "    Surface incoming longwave radiation\n",
    "    Precipitation (possibility to partition snow and rain)\n",
    "\n",
    "In the following, you can done downscaling climate data step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5482640-3956-4559-b8c1-9d10dcb5eb7f",
   "metadata": {},
   "source": [
    "## Very Important Note:\n",
    "### To run TopoPyScale you must in python 3.9 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a4b25-b082-48ca-8a92-934c4257bd60",
   "metadata": {},
   "source": [
    "### 1- Python Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1dcd9-e0db-41df-9c22-b8c0a0304980",
   "metadata": {},
   "source": [
    "TopoPyScale is tested for `Python 3.9`. You may create a new virtual environment using conda prior to installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57558778-3bdf-4de1-b2c5-906b91015e17",
   "metadata": {},
   "source": [
    "    conda create -n downscaling python=3.9 ipython\n",
    "    conda activate downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a4c00-7abd-4d43-a5b9-07b09bb7c059",
   "metadata": {},
   "source": [
    "For install dependencies use `mamba` instead `conda`:\n",
    "\n",
    "    mamba install xarray matplotlib scikit-learn pandas numpy netcdf4 h5netcdf rasterio pyproj dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e860cb-5f1e-4a9b-ad9b-5038a6c20734",
   "metadata": {},
   "source": [
    "### 2- Release Installation\n",
    "\n",
    "    pip install topopyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eff89-7d3a-4b54-9e81-3723fc1e3c8c",
   "metadata": {},
   "source": [
    "### 3- Setting up `cdsapi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f726af6-1719-482b-bc69-beefe014f337",
   "metadata": {},
   "source": [
    "Then you need to setup your `cdsapi` with the Copernicus API key system. After after creating an account with [Copernicus](https://cds.climate.copernicus.eu/), use following step for setting up to download EAR5 data. \n",
    "\n",
    "#### 3-1- Config File\n",
    "\n",
    "On Linux, create a file gedit `~/.cdsapirc` or ` $HOME/.cdsapirc` with inside:\n",
    "\n",
    "    url: https://cds.climate.copernicus.eu/api/v2\n",
    "    key: 2609:a8a3aba4-af46-4f7a-a79b-5799b58def50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4046513-4bc1-4f36-92e7-8fc67ee49cd3",
   "metadata": {},
   "source": [
    "#### 3-2- Install cdsapi\n",
    "\n",
    "    pip install cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13206b1f-d356-487f-9998-7bba94eba76f",
   "metadata": {},
   "source": [
    "#### 3-3- Testing download climate data\n",
    "\n",
    "In jupyterlab cell:\n",
    "\n",
    "    import cdsapi\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\"reanalysis-era5-pressure-levels\",\n",
    "    {\n",
    "    \"variable\": \"temperature\",\n",
    "    \"pressure_level\": \"1000\",\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"year\": \"2008\",\n",
    "    \"month\": \"01\",\n",
    "    \"day\": \"01\",\n",
    "    \"time\": \"12:00\",\n",
    "    \"format\": \"grib\"\n",
    "    }, \"download.grib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56536c-52a6-491b-9241-e886d0ba0f34",
   "metadata": {},
   "source": [
    "### 4- Create your project directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df6b42-6130-42bf-89e4-6ca91082b756",
   "metadata": {},
   "source": [
    "Folders and files in your project directory must be as follow:\n",
    "Note:  `inputs`, `dem` amd `config.yml` files are mandatory\n",
    "\n",
    "    my_project/\n",
    "        ├── inputs/\n",
    "            ├── dem/ \n",
    "                ├── my_dem.tif\n",
    "                └── pts_list.csv  (OPTIONAL: to downscale to specific points)\n",
    "            └── climate/\n",
    "                ├── PLEV*.nc\n",
    "                └── SURF*.nc\n",
    "        ├── outputs/\n",
    "                ├── tmp/\n",
    "        ├── pipeline.py (OPTIONAL: script for the downscaling instructions)\n",
    "        └── config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a12cc-09e7-480f-b868-92157726beb4",
   "metadata": {},
   "source": [
    "### 5- Create Config file\n",
    "\n",
    "Create `config.yml` file in project directory with inside for example:\n",
    "\n",
    "    project:\n",
    "        name: Khaeiz\n",
    "        description: Downscaling for the Khaeiz mountains\n",
    "        authors:\n",
    "            - Madadi H.\n",
    "        date: March 2023\n",
    "        directory: /mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/\n",
    "        start: 2020-01-01\n",
    "        end: 2020-12-31\n",
    "        split:\n",
    "            IO: False\n",
    "            time: 1  # run indivudal batch in time\n",
    "            space: None  # to be implemented\n",
    "        extent: \n",
    "        CPU_cores: 4\n",
    "        climate: era5\n",
    "\n",
    "    climate:\n",
    "        era5:\n",
    "            path: inputs/climate/\n",
    "            product: reanalysis\n",
    "            timestep: 1H\n",
    "            plevels: [700,750,800,850,900,950,1000]\n",
    "            download_threads: 12\n",
    "\n",
    "    dem:\n",
    "        file: srtm_47_06_z39_khaeiz.tif\n",
    "        epsg: 32639\n",
    "        horizon_increments: 10\n",
    "\n",
    "    sampling:\n",
    "        method: toposub\n",
    "\n",
    "\n",
    "        toposub:\n",
    "            clustering_method: minibatchkmean\n",
    "            n_clusters: 50\n",
    "            random_seed: 2\n",
    "            clustering_features: {'x':1, 'y':1, 'elevation':4, 'slope':1, 'aspect_cos':1, 'aspect_sin':1, 'svf':1}\n",
    "\n",
    "\n",
    "    toposcale:\n",
    "        interpolation_method: idw\n",
    "        pt_sampling_method: nearest\n",
    "        LW_terrain_contribution: True\n",
    "\n",
    "    outputs:\n",
    "        variables: all  # list or combination name\n",
    "        file:\n",
    "            clean_outputs: True\n",
    "            clean_FSM: True\n",
    "            df_centroids: df_centroids.pck\n",
    "            ds_param: ds_param.nc\n",
    "            ds_solar: ds_solar.nc\n",
    "            da_horizon: da_horizon.nc\n",
    "            landform: landform.tif\n",
    "            downscaled_pt: down_pt_*.nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcb71b-53e8-4a48-b883-3449f8d00a12",
   "metadata": {},
   "source": [
    "### 6- Run codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeda935-83a1-4ee7-95f5-2ec2f4f3c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Number of Clusters\n",
    "from TopoPyScale import topoclass as tc\n",
    "import numpy as np\n",
    "\n",
    "config_file = '/mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/config.yml'\n",
    "mp = tc.Topoclass(config_file)\n",
    "mp.compute_dem_param()\n",
    "df = mp.search_optimum_number_of_clusters(cluster_range=np.arange(100,1000,50),plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe3e77-7675-45a1-bc9c-9e60459f78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TopoPyScale import topoclass as tc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599c374-ef60-4f5b-baf0-7195a69d1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 1 ==========\n",
    "# Load Configuration\n",
    "config_file = '/mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/config.yml'\n",
    "mp = tc.Topoclass(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789a47a-4bb6-45e2-b466-042ce8abe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== STEP 2 ===========\n",
    "# Compute parameters of the DEM (slope, aspect, sky view factor)\n",
    "mp.compute_dem_param()\n",
    "mp.extract_topo_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4123a-2b23-4b45-8e56-d02c5a380802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Option 1:\n",
    "# Compute clustering of the input DEM and extract cluster centroids\n",
    "#mp.extract_dem_cluster_param()\n",
    "mp.extract_topo_cluster_param()\n",
    "# plot clusters\n",
    "mp.toposub.plot_clusters_map()\n",
    "# plot sky view factor\n",
    "mp.toposub.plot_clusters_map(var='svf', cmap=plt.cm.viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f1eeb-24d0-4565-bbe5-d52089bc640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 3 ==========\n",
    "# compute solar geometry and horizon angles\n",
    "mp.compute_solar_geometry()\n",
    "mp.compute_horizon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c7785-7b51-428a-b825-0c0c9fddf3b2",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "\n",
    "In input folder only `surf` and `PLEV` files must be coresponding to `start date` and `end date` in config file, otherwise below code encounter with `memory error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960266a-1d1e-46a1-9f79-71b5266c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 4 ==========\n",
    "# Perform the downscaling\n",
    "mp.downscale_climate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8791d-1510-4e3e-948a-f7a78c64229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 5 ==========\n",
    "# Export output to desired format\n",
    "mp.to_netcdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692a68c-d022-4fb4-998e-680495fdcff6",
   "metadata": {},
   "source": [
    "### 7- Convert `Hourly` to `Yearly` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4369369-d433-4222-bdcd-dbb1bc9b7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds_down_dsk = xr.open_dataset(\n",
    "    \"./Caracal/Topopyscale_prj_khaeiz/outputs/output.nc\",\n",
    "    chunks={\"point_id\": 10}\n",
    ")\n",
    "ds_down_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b3f6f-b55a-4c0d-9d93-59c01b16466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_year = ds_down_dsk.groupby('time.year').mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e422c15-d0c3-474d-a030-9fa332252b6d",
   "metadata": {},
   "source": [
    "### 8- Adding Coordinate dimention to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee6ece-c933-4eb2-bea3-351af5d4677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_param = xr.open_dataset(\"./Caracal/Topopyscale_prj_khaeiz/outputs/ds_param.nc\")\n",
    "ds_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5f97e-36ab-4350-9f96-d88cb373749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_khaeiz = ds_year.sel(point_id=ds_param.cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77953ce3-aa0d-45e1-a8a5-762f6a856789",
   "metadata": {},
   "source": [
    "### 9- Saving reult to netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793852c-67f7-40ab-a24f-042763724441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scaling_and_offset(da, n=16):\n",
    "    \"\"\"\n",
    "    Compute offset and scale factor for int conversion\n",
    "\n",
    "    Args:\n",
    "        da (dataarray): of a given variable\n",
    "        n (int): number of digits to account for\n",
    "    \"\"\"\n",
    "    vmin = float(da.min().values)\n",
    "    vmax = float(da.max().values)\n",
    "\n",
    "    # stretch/compress data to the available packed range\n",
    "    scale_factor = (vmax - vmin) / (2 ** n - 1)\n",
    "    # translate the range to be symmetric about zero\n",
    "    add_offset = vmin + 2 ** (n - 1) * scale_factor\n",
    "\n",
    "    return scale_factor, add_offset\n",
    "\n",
    "def to_netcdf(ds, fname='output.nc', variables=None):\n",
    "        \"\"\"\n",
    "        Generic function to save a datatset to one single compressed netcdf file\n",
    "\n",
    "        Args:\n",
    "            fname (str): name of export file\n",
    "            variables (list str): list of variable to export. Default exports all variables\n",
    "        \"\"\"\n",
    "\n",
    "        encod_dict = {}\n",
    "        if variables is None:\n",
    "            variables = list(ds.keys())\n",
    "\n",
    "        for var in variables:\n",
    "            scale_factor, add_offset = compute_scaling_and_offset(ds[var], n=10)\n",
    "            if str(ds[var].dtype)[:3] == 'int':\n",
    "                encod_dict.update({var:{\n",
    "                                   'dtype':ds[var].dtype}})\n",
    "            else:\n",
    "                encod_dict.update({var:{\"zlib\": True,\n",
    "                                       \"complevel\": 9,\n",
    "                                       'dtype':'int16',\n",
    "                                       'scale_factor':scale_factor,\n",
    "                                       'add_offset':add_offset}})\n",
    "        ds[variables].to_netcdf(fname, encoding=encod_dict, engine='h5netcdf')\n",
    "\n",
    "        print(f'---> File {fname} saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb30cc-c1aa-4d34-b9d3-0fa74b6e62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_netcdf(ds_khaeiz, fname='downscaled_nc.nc', variables=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594db24-06ed-46cc-a39f-bdf0080aafc2",
   "metadata": {},
   "source": [
    "### 10- Extract `Temperature` from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbcc9a-1501-4292-9014-eef172e460ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t = ds_khaeiz.t\n",
    "ds_t.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f9cab-c24b-4b5c-b2e1-92acba566cf6",
   "metadata": {},
   "source": [
    "### 11- Save a variable to geotif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb933d5-f9fe-474f-affd-523289a30604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import rioxarray\n",
    "from pyproj import CRS\n",
    "\n",
    "xds = xarray.open_dataset(\"downscaled_nc.nc\")\n",
    "\n",
    "bT = xds['t']\n",
    "bT = bT.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "bT.rio.crs\n",
    "\n",
    "# Define the CRS projection\n",
    "bT.rio.write_crs(\"epsg:32639\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d60d91-216e-4570-a478-2e56e235ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the GeoTIFF file: \n",
    "bT.rio.to_raster(r\"temp_raster.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa0fd4-0a81-4ca9-97f1-a91e8afc2377",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "It is possible to improve accuracy by determining "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc6eaa-d19b-4eae-be5d-aecc8f2da00e",
   "metadata": {},
   "source": [
    "# Run TopoPyScale in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d07f1d-e747-42e5-aee9-65cdda086266",
   "metadata": {},
   "source": [
    "Using TopoPyScale for downlading ERA-5 data seems be more difficult without VPN, in other hand connecting to VPN is not simple in my country, so after searching to find suitable method to running ToPoPyScale and downloading ERA-5 data without VPN, I found that the `Colab` is a good choice for this problem. In the following I tried to explain step-by-step in using TopoPyScale in Colab and then download data from google drive to local drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e871e-d7ff-41f8-be68-6a12b1b3ca6a",
   "metadata": {},
   "source": [
    "Open a new notebook in `https://colab.research.google.com/`. I used `nilick.gitcolab@gmail.com` account for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b3d9e-0732-4dfe-8f54-b2274f07cbb5",
   "metadata": {},
   "source": [
    "### Mount the google drive\n",
    "\n",
    "some guides is from: https://netraneupane.medium.com/how-to-install-libraries-permanently-in-google-colab-fb15a585d8a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f656-1243-4b1e-bb6d-eb7b4add00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0fd2f-49fe-461f-8f1f-653fea8a5846",
   "metadata": {},
   "source": [
    "### Create a virtual environment using virtualenv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0c058-b8ca-47d9-9366-9f3df924e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a2e03-2d70-426d-bc84-e1acb1d288d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!virtualenv /content/drive/MyDrive/colab_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111b0de-a7bf-45f0-b8fc-f728b04ee864",
   "metadata": {},
   "source": [
    "### Activate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd7833-c096-48aa-afa3-77377d9e8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /content/drive/MyDrive/colab_env/bin/activate;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd1be3-489a-40ff-b01b-0fb2c401ddc9",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e6bd8-4270-47ac-9189-33944c578265",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray matplotlib scikit-learn pandas numpy netcdf4 h5netcdf rasterio pyproj dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e707d43-755c-4808-b18f-89ae810ea742",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install topopyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb9d8f-cbd7-4c98-a4d2-e9521ae82ba8",
   "metadata": {},
   "source": [
    "### Test Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e85a1-631b-408d-a050-ff1f53c37ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TopoPyScale import topoclass as tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd4443-f6cd-43fe-b179-8f192761f80c",
   "metadata": {},
   "source": [
    "### Setting up cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36838a8a-2835-4d5d-a02e-f4d40c0b655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'url: https://cds.climate.copernicus.eu/api/v2'\n",
    "key = 'key: 2609:a8a3aba4-af46-4f7a-a79b-5799b58def50'\n",
    "\n",
    "with open('/root/.cdsapirc', 'w') as f:\n",
    "    f.write('\\n'.join([url, key]))\n",
    "\n",
    "with open('/root/.cdsapirc') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c800d-bceb-4c06-bab2-65716c2f18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7e3fd-dfd6-401a-bf81-3be06b25d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/colab_env/lib/python3.9/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd64a2-0811-4fab-a33e-67de0cd0c98b",
   "metadata": {},
   "source": [
    "### Running TopoPyScale in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b0c7f-11e6-4693-a4a1-4a4da6480124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Number of Clusters\n",
    "\n",
    "from TopoPyScale import topoclass as tc\n",
    "import numpy as np\n",
    "\n",
    "config_file = '/content/drive/MyDrive/topopyscale_run/config.yml'\n",
    "mp = tc.Topoclass(config_file)\n",
    "mp.compute_dem_param()\n",
    "#df = mp.search_optimum_number_of_clusters(cluster_range=np.arange(100,1000,50),plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775826e1-fa4d-4bea-a0e9-55c42068e98b",
   "metadata": {},
   "source": [
    "### Local Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf5037-cb23-4632-afe4-d8b6dbd73838",
   "metadata": {},
   "source": [
    "After running topopyscale in colab successfully, ERA-5 data are stored in google drive that these files must be transfer to local drive. One easy way for do this is using local google drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d42e6-f5a2-45b9-84ff-80f4499e93b9",
   "metadata": {},
   "source": [
    "#### Access Google Drive Using Graphical-User-Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad0a63-ecaf-40d3-bef1-0f821c96e33a",
   "metadata": {},
   "source": [
    "__Step 1: Open the settings__\n",
    "\n",
    "To open the settings, go to the“Applications” and search the “Settings” in the search bar:\n",
    "\n",
    "__Step 2: Select online accounts__\n",
    "\n",
    "When the settings open, it will show the following interface. Now, select “Online Accounts”\n",
    "\n",
    "__Step 3: Choose Google__\n",
    "\n",
    "From the following window, choose“Google”\n",
    "\n",
    "__Step 4: Allow for Gnome Trust__\n",
    "\n",
    "After Enter the email & password you should allow Gnome trust to continue with google drive. For this purpose, click on the “Allow” button\n",
    "\n",
    "__Step 5: Sync Applications__\n",
    "\n",
    "To sync the application with Google drive, choose the items that google drive will be used for\n",
    "\n",
    "__Step 6: Using local Google drive__\n",
    "\n",
    "open the `Home` directory and in left side open `Network` and then click on your google account to accessing your files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29c33f-06cf-4ea2-b274-2b44a3543011",
   "metadata": {},
   "source": [
    "# Downscaling for large area and big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b3a5c-85ff-4640-9c83-4abc8006d913",
   "metadata": {},
   "source": [
    "Netcdf file as the result of `Topopyscale` downscaling processing does not have `Latitude` and `Longitude` information and only has `point_id` data that is about cluster points id. Georeferencing the downscaled parameters is necessary for analysis and plotting. Fortunately, there is a file that has `Latitude` and `Longitude` data that was created during the downscaling process from `dem`. This file name is `ds_param`. In this file, we have a data variable name as `cluster_labels`. This data variable is the same as `point_id` in the downscaled file. Adding `cluster_labels` data to downscaled file can solve the georeferencing problem. Unfortunately, by adding this data when we want to save the new dataset as a new NetCDF file, it creates a huge file in size. For example for only one year hourly data such as 2000, doenscaled file size is ~35Mb but merged file size is ~3.5Gb! In [this URL](https://stackoverflow.com/a/72116071/7717176) you can find the reason for this huge file creation.\n",
    "\n",
    "I tried to solve this problem by reducing data variables, changing data type, and using different chunk sizes, but this approach did not impact significantly the file size. Finally, I decided to use the downscaled file with `ds_param` simultaneously.\n",
    "\n",
    "__Note:__ In another prompet I tried use `Zarr` to save dataset but it build more larger than file than netcdf file.\n",
    "\n",
    "For this purpose, I wrote below function for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c41481-6407-4c3f-b8fe-c425639bc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def georef_down(downscaled_file, ds_param_file):\n",
    "    # Taking files\n",
    "    ds = xr.open_dataset(downscaled_file, chunks={'time': 1000})\n",
    "    ds_param = xr.open_dataset(ds_param_file)\n",
    "    \n",
    "    # Merge Two Dataframe for adding (x,y)\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        ds_new = ds.sel(point_id=ds_param.cluster_labels)\n",
    "        \n",
    "    # Rename coordinate\n",
    "    ds_final = ds_new.rename({'x': 'Longitude','y': 'Latitude'})\n",
    "    ds_final.Longitude.attrs['units'] = 'degrees_east'\n",
    "    ds_final.Latitude.attrs['units'] = 'degrees_north'\n",
    "    \n",
    "    # Drop useless parameters\n",
    "    ds_droped_coor = ds_final.drop(['reference_time', 'point_id'])\n",
    "    ds_droped_coor = ds_final.drop(['reference_time', 'point_id'])\n",
    "    ds_droped_vars = ds_droped_coor.drop_vars(['u', 'v', 'w', 'cse',\n",
    "                                               'SW_diffuse', 'cos_illumination',\n",
    "                                               'SW_direct'])\n",
    "    # Rechunk\n",
    "    ds_rch = ds_droped_vars.chunk(chunks={'Latitude': 200, 'Longitude': 250,'time': 1024})\n",
    "    \n",
    "    # Change dtypes\n",
    "    ds_rch[\"Latitude\"] = ds_rch[\"Latitude\"].astype(\"float32\")\n",
    "    ds_rch[\"Longitude\"] = ds_rch[\"Longitude\"].astype(\"float32\")\n",
    "\n",
    "    variables = list(ds_rch.keys())\n",
    "    for var in variables:\n",
    "        ds_rch[var] = ds_rch[var].astype(\"float32\")\n",
    "        \n",
    "    return ds_rch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c08942-012c-4aef-87bd-a984a9b61467",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "\n",
    "downscaled_file = \"...........~/downsale_yearly/outputs/zagros_era5_dwns_1km_2000.nc\"\n",
    "ds_param_file = \"............~/downsale_yearly/outputs/ds_param.nc\"\n",
    "ds_2000 = georef_down(downscaled_file, ds_param_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f0b25-14d5-4e58-a864-b6996058e1ca",
   "metadata": {},
   "source": [
    "You can use above code for downscaling hourly data for each year and then combine yearly doenscaled datasets in Xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648e692-8083-4315-977b-8e30f48ef334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
