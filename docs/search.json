[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Envinformatics",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\nPhilosophy: How to take and organize notes effectively\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhat is a Mind Map?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSecond Brain\n\n\n\n\n\n\n\nZettelkasten\n\n\nZotero\n\n\nObsidian\n\n\n\n\nZettelkasten Workflow using Zotero + Obsidian\n\n\n\n\n\n\nJul 9, 2023\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nPrivateGPT + Streamlit\n\n\n\n\n\n\n\nChatGPT\n\n\nStramlit\n\n\nPrivateGPT\n\n\n\n\nBuilding Offline (Local) ChatGPT App Using PrivateGPT and Streamlit\n\n\n\n\n\n\nJun 1, 2023\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT + Python\n\n\n\n\n\n\n\nChatGPT\n\n\n\n\nUsing Tools and Python Codes about ChatGPT.\n\n\n\n\n\n\nMay 23, 2023\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nTopoPyScale\n\n\n\n\n\n\n\nGeoSpatial\n\n\nDownscale\n\n\nClimate\n\n\n\n\nDownscaling climate data based on topography.\n\n\n\n\n\n\nMar 23, 2023\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nGepSpatial ML-DL Sources\n\n\n\n\n\n\n\nGeoSpatial\n\n\nML\n\n\nDL\n\n\n\n\nWeb Resources for GeoSpatial Machine Learning and Deep Learning.\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nScraping AQI\n\n\n\n\n\n\n\nScraping\n\n\nML\n\n\n\n\nExtracting AQI data from web for data sciences analysis.\n\n\n\n\n\n\nNov 11, 2022\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nUbuntu\n\n\n\n\n\n\n\nUbuntu\n\n\nNotes\n\n\n\n\nMy notes in Ubuntu working\n\n\n\n\n\n\nSep 30, 2022\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis process of downscaled MERRA-2 data\n\n\n\n\n\n\n\nDownscaling\n\n\nMERRA-2\n\n\nLDT\n\n\n\n\nThis post is about create own your blog with free resources in Github.\n\n\n\n\n\n\nAug 16, 2021\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nGIS & RS learning and notes\n\n\n\n\n\n\n\nGeospatial\n\n\n\n\nThis post is about GIS and Remote Sensing learning and keynotes of them.\n\n\n\n\n\n\nJul 3, 2021\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nML-DL Resources\n\n\n\n\n\n\n\nML\n\n\nDL\n\n\n\n\nThis post is about resources for machine learning and deep learning from web.\n\n\n\n\n\n\nJul 2, 2021\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial Data Address\n\n\n\n\n\n\n\nGeospatial\n\n\n\n\nURLs for geospatial data, always in custructing!\n\n\n\n\n\n\nMar 25, 2021\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nBlogging with Quarto\n\n\n\n\n\n\n\nWeblog\n\n\nQuarto\n\n\nNbdev\n\n\n\n\nCreating weblog in combination of Jupyterlab and Quarto\n\n\n\n\n\n\nMar 1, 2021\n\n\nMe\n\n\n\n\n\n\n  \n\n\n\n\nJupyterlab\n\n\n\n\n\n\n\nJupyterlab\n\n\n\n\nMy notes in Jupyterlab working\n\n\n\n\n\n\nInvalid Date\n\n\nMe\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "",
    "text": "In this post, I would like to describe full story of my experiences in spatial downscaling of MERRA-2 data. The main spark of using MERRA-2 data refers to the paper of Assessment of drought conditions over Vietnam using standardized precipitation evapotranspiration index, MERRA-2 re-analysis, and dynamic land cover (Manh-Hung Le et al., 2020). Researchers of this paper used spatial rescaled MERRA-2 data from ~50-km to ~1-km and used it for calculation of drought index.\nIn first step, I tried to use Land surface Data Toolkit (LDT). So, based on LDT User Guide, required libraries were installed for setup LDT in personal laptop. Although successfully installation of all necessary libraries in a hard way, but I didn’t success in using LDT to spatial downscaling of MERRA-2 data.\nAfter unsuccessful attempts to use LDT correctly, connection with authors of mentioned paper was seemed a right way. Dr. Hyunglok Kim as one of the paper’s researches warned me that their spatial downscaled of MERRA-2 data was generated from the Land Information System (LIS), not LDT. Specifically, the LDT will just produce some basic information to produce data from LIS. Based on Hyung explanation about producing spatial rescaled data, me seemed to need a supercomputer to do that. With the help of Hyung, temperature and rain monthly data from 2000 to 2020 produced in 0.001 degree (~1 km) of spatial resolution. In the following, the procedure of analysis of this data will describe."
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#overall-view-of-data",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#overall-view-of-data",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Overall view of data",
    "text": "Overall view of data\nSpatial downscaled of MERRA-2 data files was in NetCDF format. So, me preferred to use Xarray python library to check some its properties.\nOpening 252 NetCDF files simultaneously will done with some python libraries.\n\n\nCode\n#collapse\n\n# Import libraries\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8,5)\n\n\nModuleNotFoundError: No module named 'xarray'\n\n\nReading files:\n\n\nCode\n#hide\nds = xr.open_mfdataset(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc\")\n\n\n\n\nCode\n#collapse_output\nprint(ds)\n\n\n<xarray.Dataset>\nDimensions:           (time: 252, north_south: 701, east_west: 1001, ensemble: 1)\nCoordinates:\n  * time              (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01\n  * ensemble          (ensemble) float32 1.0\nDimensions without coordinates: north_south, east_west\nData variables:\n    lat               (time, north_south, east_west) float32 dask.array<chunksize=(1, 701, 1001), meta=np.ndarray>\n    lon               (time, north_south, east_west) float32 dask.array<chunksize=(1, 701, 1001), meta=np.ndarray>\n    Rainf_f_tavg      (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\n    Rainf_f_inst      (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\n    Tair_f_tavg       (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\n    Tair_f_inst       (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\n    TotalPrecip_tavg  (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\n    TotalPrecip_inst  (time, ensemble, north_south, east_west) float32 dask.array<chunksize=(1, 1, 701, 1001), meta=np.ndarray>\nAttributes: (12/15)\n    missing_value:           -9999.0\n    NUM_SOIL_LAYERS:         4\n    SOIL_LAYER_THICKNESSES:  [0.1 0.3 0.6 1. ]\n    title:                   LIS land surface model output\n    institution:             NASA GSFC\n    source:                  +template open water\n    ...                      ...\n    comment:                 website: http://lis.gsfc.nasa.gov/\n    MAP_PROJECTION:          EQUIDISTANT CYLINDRICAL\n    SOUTH_WEST_CORNER_LAT:   27.0\n    SOUTH_WEST_CORNER_LON:   47.0\n    DX:                      0.01\n    DY:                      0.01\n\n\nOutput reading fils show that thera are time, north_south, east_west, ensemble as Dimensions, time and ensemble as Coordinates and lat, lon, Rainf_f_tavg, Rainf_f_inst, Tair_f_tavg, Tair_f_inst, TotalPrecip_tavg, TotalPrecip_inst as Data variables of data.\nYou can see all of these items individually as follow:\n\n\nCode\n#collapse_output\nds.dims\n\n\nFrozen({'time': 252, 'north_south': 701, 'east_west': 1001, 'ensemble': 1})\n\n\n\n\nCode\n#collapse_output\nds.coords\n\n\nCoordinates:\n  * time      (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01\n  * ensemble  (ensemble) float32 1.0\n\n\n\n\nCode\n#collapse_output\nds.variables\n\n\nFrozen({'lat': <xarray.Variable (time: 252, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 701, 1001), dtype=float32, chunksize=(1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          degree_north\n    standard_name:  latitude\n    long_name:      latitude\n    vmin:           0.0\n    vmax:           0.0, 'lon': <xarray.Variable (time: 252, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 701, 1001), dtype=float32, chunksize=(1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          degree_east\n    standard_name:  longitude\n    long_name:      longitude\n    vmin:           0.0\n    vmax:           0.0, 'time': <xarray.IndexVariable 'time' (time: 252)>\narray(['2000-01-01T00:00:00.000000000', '2000-02-01T00:00:00.000000000',\n       '2000-03-01T00:00:00.000000000', ..., '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')\nAttributes:\n    long_name:       time\n    time_increment:  2592000\n    begin_date:      20000101\n    begin_time:      000000, 'ensemble': <xarray.IndexVariable 'ensemble' (ensemble: 1)>\narray([1.], dtype=float32)\nAttributes:\n    units:      ensemble number\n    long_name:  Ensemble numbers, 'Rainf_f_tavg': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          kg m-2 s-1\n    standard_name:  rainfall_flux\n    long_name:      rainfall flux\n    vmin:           0.0\n    vmax:           0.02, 'Rainf_f_inst': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          kg m-2 s-1\n    standard_name:  rainfall_flux\n    long_name:      rainfall flux\n    vmin:           0.0\n    vmax:           0.02, 'Tair_f_tavg': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          K\n    standard_name:  air_temperature\n    long_name:      air temperature\n    vmin:           213.0\n    vmax:           333.0, 'Tair_f_inst': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          K\n    standard_name:  air_temperature\n    long_name:      air temperature\n    vmin:           213.0\n    vmax:           333.0, 'TotalPrecip_tavg': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          kg m-2 s-1\n    standard_name:  total_precipitation_amount\n    long_name:      total precipitation amount\n    vmin:           0.0\n    vmax:           0.02, 'TotalPrecip_inst': <xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)>\ndask.array<concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray>\nAttributes:\n    units:          kg m-2 s-1\n    standard_name:  total_precipitation_amount\n    long_name:      total precipitation amount\n    vmin:           0.0\n    vmax:           0.02})\n\n\n\n\nCode\n#collapse_output\nds.attrs\n\n\n{'missing_value': -9999.0,\n 'NUM_SOIL_LAYERS': 4,\n 'SOIL_LAYER_THICKNESSES': array([0.1, 0.3, 0.6, 1. ], dtype=float32),\n 'title': 'LIS land surface model output',\n 'institution': 'NASA GSFC',\n 'source': '+template open water',\n 'history': 'created on date: 2021-07-10T04:31:27.879',\n 'references': 'Kumar_etal_EMS_2006, Peters-Lidard_etal_ISSE_2007',\n 'conventions': 'CF-1.6',\n 'comment': 'website: http://lis.gsfc.nasa.gov/',\n 'MAP_PROJECTION': 'EQUIDISTANT CYLINDRICAL',\n 'SOUTH_WEST_CORNER_LAT': 27.0,\n 'SOUTH_WEST_CORNER_LON': 47.0,\n 'DX': 0.01,\n 'DY': 0.01}\n\n\nIt is possible to show items for a dimension of our dataset that can be useful in future analysis.\n\n\nCode\n#collapse_output\nprint(ds.time)\n\n\n<xarray.DataArray 'time' (time: 252)>\narray(['2000-01-01T00:00:00.000000000', '2000-02-01T00:00:00.000000000',\n       '2000-03-01T00:00:00.000000000', ..., '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01\nAttributes:\n    long_name:       time\n    time_increment:  2592000\n    begin_date:      20000101\n    begin_time:      000000"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#simple-plotting",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#simple-plotting",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Simple Plotting",
    "text": "Simple Plotting\nXarray library in combination of matplotlib library has powerful capability to draw various plots. For example in section will show some plots with xarray. Dataset variables show there are three climate variables include air temperature, rainfall and total precipitation. We can plot air temperature in first time as follow.\n\n\nCode\n#collapse\ntemp = ds.Tair_f_tavg\ntemp[0].plot();\n\n\n\n\n\nThere is another way to plot based on specific time.\n\n\nCode\n#hide_output\ntemp = ds.Tair_f_tavg\ntemp.sel(time='2000-01-01').plot();\n\n\n\n\n\nDespite there are lat and lon data variables in dataset, in above plot geographical coordinates have been displayed incorrectly. Because lat and lon are not as dimension, we can not display geographical coordinates correctly. For improve the plot in showing geographical coordinates, mentioned data variables should be set as coords. Furthermore, air temperature unit is in kelvin [K], while we need it as celsius [C]. So it is necessary to convert air temperature unit.\n\n\nCode\n#collapse\n\n# set `lat` and `lon` as coords\nds = ds.set_coords(['lon','lat'])\n\n# convert `air temperature` unit from kelin to celsius\ntemp_c = ds.Tair_f_tavg - 273.15\n\n# plotting `air temperature` data variable in celsius and geographical coordinates.\ntemp_c.sel(time='2000-01-01').plot(x='lon', y='lat');"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#simple-statistics",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#simple-statistics",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Simple statistics",
    "text": "Simple statistics\nNow, with new dataarray created for air temperature it is possible to compute some statistics for this climate variable such as mean, std, min, max, etc. For example, some air temperature statistics for August 2015 are shown as follow.\n\n\nCode\n#collapse\ntemp_20150825 = temp_c.sel(time='2015-08-01')\ntemp_avg = temp_20150825.mean().values\ntemp_std = temp_20150825.std().values\ntemp_min = temp_20150825.min().values\ntemp_max = temp_20150825.max().values\n\nprint(\"Mean air temperature for August 2015 is\", temp_avg , \"celsius.\\n\"\n     \"Standard deviation of air temperature for August 2015 is\", temp_std ,\".\\n\"\n     \"Minimum air temperature for August 2015 is\", temp_min , \"celsius.\\n\"\n     \"Maximum air temperature for August 2015 is\", temp_max , \"celsius.\")\n\n\nMean air temperature for August 2015 is 31.591408 celsius.\nStandard deviation of air temperature for August 2015 is 4.980299 .\nMinimum air temperature for August 2015 is 21.36026 celsius.\nMaximum air temperature for August 2015 is 40.377167 celsius.\n\n\n\nTip: Our data is monthly from 2000 to 2020.."
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#certain-point-values",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#certain-point-values",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Certain point values",
    "text": "Certain point values\nThe Merre-2 rescaled data is a reanalysis data with contiguous values, so if we want to compare this values with climatic station data as ground values it is important to extract the climate variables from Merra-2 dataset in a specific values in accordance with climatic station latitude and longitude. For extract this value(s) based on certain latitude and longitude we can finding the index of the grid point nearest a specific lat/lon.\nIn this example we want to find air temperature in latitude = 30.66 and longitude = 51.58 for August 2015.\n\n\nCode\n#collapse\n\nlatitude = 30.66\nlongitude = 51.58\n\n#finding the index of the grid point nearest a specific lat/lon.   \nabslat = np.abs(temp_20150825.lat-latitude)\nabslon = np.abs(temp_20150825.lon-longitude)\nc = np.maximum(abslon, abslat)\n([xloc], [yloc]) = np.where(c == np.min(c))\n\n# Using that index location to get the values at the x/y diminsion\npoint_ds = ds.sel(north_south=xloc, east_west=yloc)\n\n# Value of certain point in celsius\ntemp_pnt = point_ds.sel(time='2015-08-01').Tair_f_tavg.values- 273.15\nprint(\"The monthly air temperature in August 2015 for latitude=\", latitude, \" and longitude=\", longitude, \"is\", format(float(temp_pnt), '.2f'), \"celsius.\")\n\n\nThe monthly air temperature in August 2015 for latitude= 30.66  and longitude= 51.58 is 25.74 celsius.\n\n\nWe can also plot for this point in August 2015.\n\n\nCode\n#collapse\n\n# Plot requested lat/lon point blue\ntemp_20150825.plot(x='lon', y='lat')\nplt.scatter(longitude, latitude, color='b')\nplt.text(longitude, latitude, 'requested')\n\n# Plot nearest point in the array red\nplt.scatter(point_ds.lon[0], point_ds.lat[0], color='r')\nplt.text(point_ds.lon[0], point_ds.lat[0], 'nearest')\nplt.title('Temperature at nearest point: %s celsius' % format(float(temp_pnt), '.2f'));\n\n\n\n\n\nThe mean monthly temperature for all time series can also be calculated.\n\n\nCode\n#collapse\n\n# Mean temperature average for specifiv lat/lon\npnt = point_ds.Tair_f_tavg.values - 273.15\nprint(\"The monthly air temperature from 2000 to 2020 for latitude=\", latitude, \" and longitude=\", longitude, \"is\", format(float(pnt.mean()), '.2f'), \"celsius.\")\n\n\nThe monthly air temperature from 2000 to 2020 for latitude= 30.667  and longitude= 51.583 is 14.65 celsius."
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#swap-north_south-and-east_west-dimensions-with-lat-and-lon-coordinates",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#swap-north_south-and-east_west-dimensions-with-lat-and-lon-coordinates",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Swap “north_south” and “east_west” dimensions with “lat” and “lon” coordinates",
    "text": "Swap “north_south” and “east_west” dimensions with “lat” and “lon” coordinates\nOne of the perbolem in working with this dataset is about latitude/longitude. As you saw before there is not any dimension related latitude and longitude and we used set_coords to define lat and lot data variables to plot correctly in geographical coordinates while dataset did not any change in inherent and do any analysis related to latitude and longitude of dataset will have challenges. So, it’s best suggestion to create new dataset with raw data that have latitude and longitude dimensions. Two dimensions include north_south and east_west are linespaces related to latitude and longitude. In this dimensions, from beginning to ending latitude divide based on 0.001 degree distances that first value for north_south set to zero and last value will be based on dividing distances. For example for this dataset zero value of north_south is equal minimum latitude (27.0 degree) and last value (700) is equal maximum latitude (34.0 degree). The east_wet dimension values is also same as north_south but for longitude. So, if we can swap the north_south and east_west dimensions values with original latitude and longitude, respectively, we will have new dataset with original latitude and longitude dimensions.\nFor swapping “north_south” and “east_west” dimensions with lat and lon coordinates there is a Regridder function in xesmf library. Because of multiple NetCDF files should be regridded, so it’s better to done it in a loop.\nAlthough input netcdf files have small size but output netcdf files exported from xarray dataset will be more than x9-10 larger, so for solve this challenge we use zarr library to save output regridded files. But if drop unuseful data variables such as ‘Rainf_f_inst’, ‘Tair_f_inst’, ‘TotalPrecip_inst’ and set remain data variables data type as float32, we can save output file in netcdf format with smaller size.\n\n\nCode\n#collapse\n\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport xesmf as xe\nimport zarr\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# reading dataset\nds = xr.open_mfdataset(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/MERRA_2/monthly_data/*.nc\")\nds = ds.set_coords(['lon','lat'])\n\n# definign outout path\npath_output = '/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/MERRA_2/monthly_data/monthly_regrid2/'\n\n# regriding loop\nfor i in range (0, len(ds.time)):\n    dsi = ds.sel(time = ds.time[i])\n    \n    # swap dimensions with multi-dimensional coordinates\n    ds_xy_grid = dsi.rename(north_south='lat', east_west='lon')\n    ds_out = xr.Dataset({'lat': (['lat'], np.linspace(27.0, 34.0, 701)),\n                     'lon': (['lon'], np.linspace(47.0, 57.0, 1001))})\n    \n    regridder = xe.Regridder(ds_xy_grid, ds_out, method='bilinear')\n    ds_lonlat_grid = regridder(ds_xy_grid)\n    ds_lonlat_grid = ds_lonlat_grid.rename(lat='latitude', lon='longitude')\n    \n    # Removing some zero values for plotting correctly\n    ds_lonlat_grid = ds_lonlat_grid.where(ds_lonlat_grid['Tair_f_tavg'] > 0.01, drop=True)\n    ds_lonlat_grid.attrs = dsi.attrs\n    droped = ds_lonlat_grid.drop_vars(['Rainf_f_inst', 'Tair_f_inst', 'TotalPrecip_inst'])\n    \n    # Take date time for name of output file\n    time = str(ds_lonlat_grid.time.values)\n    time = time.removesuffix('T00:00:00.000000000')\n    \n    # save to netcdf\n    encoding = {'Rainf_f_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'},\n                'Tair_f_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'},\n                'TotalPrecip_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'}}\n    \n    droped.to_netcdf((path_output + \"regrid_merra_\" + time + \".nc\"),encoding=encoding)\n    \n    del ds_xy_grid, ds_out, regridder, ds_lonlat_grid, time, encoding, droped\n\n\n\nImproved Above Code\nNOTE: The below code in Jupyterlab app desktop with python 8 did not work correctly but it worked in jupyterlab in browser with python 3.9.5 correctly.\n\n\nCode\n# reading dataset\nds = xr.open_mfdataset(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/MERRA_2/monthly_data/*.nc\")\nds = ds.set_coords(['lon','lat'])\n\n# definign outout path\npath_output = '/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/MERRA_2/monthly_data/monthly_regrid3/'\n\n# regriding loop\nfor i in range (0, len(ds.time)):\n    dsi = ds.sel(time = ds.time[i])\n    \n    # swap dimensions with multi-dimensional coordinates\n    ds_xy_grid = dsi.rename(north_south='lat', east_west='lon')    \n    ds_out = xr.Dataset({'lat': (['lat'], np.linspace(27.0, 34.0, 701)),\n                     'lon': (['lon'], np.linspace(47.0, 57.0, 1001))})\n    \n    regridder = xe.Regridder(ds_xy_grid, ds_out, method='bilinear')\n    ds_lonlat_grid = regridder(ds_xy_grid)\n    ds_lonlat_grid = ds_lonlat_grid.rename(lat='latitude', lon='longitude')\n    \n    # Removing some zero values for plotting correctly\n    ds_lonlat_grid = ds_lonlat_grid.where(ds_lonlat_grid['Tair_f_tavg'] > 0.01, drop=True)\n    #dropnan1 = ds_lonlat_grid.dropna('longitude', how = 'any')\n    #dropnan2 = dropnan1.dropna('latitude', how = 'any')\n    ds_lonlat_grid.attrs = dsi.attrs\n    droped = ds_lonlat_grid.drop_vars(['Rainf_f_inst', 'Tair_f_inst', 'TotalPrecip_inst'])\n    expanded_da = xr.concat([droped], 'time') # add 'time' dimension\n    #resample_monthly = expanded_da.resample(time='1M').max()\n    final_ds = expanded_da.squeeze(dim='ensemble', drop=False) # drop 'ensemble' dimension\n    \n    # Take date time for name of output file\n    time = str(ds_lonlat_grid.time.values)\n    time = time.removesuffix('T00:00:00.000000000')\n    \n    # save to netcdf\n    encoding = {'Rainf_f_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'},\n                'Tair_f_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'},\n                'TotalPrecip_tavg':{'zlib': True, 'complevel': 5, 'dtype': 'float32'}}\n    \n    final_ds.to_netcdf((path_output + \"regrid_merra_\" + time + \".nc\"),encoding=encoding)\n    \n    del ds_xy_grid, ds_out, regridder, ds_lonlat_grid, time, encoding, droped, expanded_da, final_ds\n\n\nComparison between raw and regreded MERRA-2 data are plotted in below. The July 2009 selected for comparison.\n\n\nCode\n# collapse\n\nds_raw = xr.open_dataset('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/LIS_HIST_200906010000.d01.nc')\nds_reg = xr.open_dataset('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/monthly_regrid/regrid_merra_2009-06-01.nc')\n\n# Compare two plots\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n\nds_raw.Tair_f_tavg.plot(ax=axes[0])\naxes[0].set_title(\"Raw data\")\n\nds_reg.Tair_f_tavg.plot(ax=axes[1])\naxes[1].set_title(\"Regrided data\")\n\nfig.tight_layout()\n\n\n\n\n\nNow we can select a random point in above plots and extract its air temperature and rain values for more exactly comparison.\n\n\nCode\n# collapse\n\n# defining a random point for extract climatic variables\nlatitude = 30.48\nlongitude = 50.58\n\n# raw data\nabslat = np.abs(ds_raw.lat-latitude)\nabslon = np.abs(ds_raw.lon-longitude)\nc = np.maximum(abslon, abslat)\n([xloc], [yloc]) = np.where(c == np.min(c))\npnt_raw = ds_raw.sel(north_south=xloc, east_west=yloc)\ntemp_raw = pnt_raw.Tair_f_tavg.values\nrain_raw = pnt_raw.Rainf_f_tavg.values\n\n# regrided data\npnt_reg = ds_reg.where((ds_reg.longitude==longitude) & (ds_reg.latitude==latitude), drop=True)\ntemp_reg = pnt_reg.Tair_f_tavg.values\nrain_reg = pnt_reg.Rainf_f_tavg.values\n\nprint(\"The air temperature in raw Merra-2 is \", format(float(temp_raw), '.2f'),\"\\n\"\n     \"The air temperature in regrided Merra-2 is \", format(float(temp_reg), '.2f'),\"\\n\"\n     \"The rain in raw Merra-2 is \", format(float(rain_raw), '.2f'),\"\\n\"\n     \"The rain in regrided Merra-2 is \", format(float(rain_reg), '.2f'))\n\n\nThe air temperature in raw Merra-2 is  303.64 \nThe air temperature in regrided Merra-2 is  303.64 \nThe rain in raw Merra-2 is  0.00 \nThe rain in regrided Merra-2 is  0.00"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#integrated-surface-dataset-global",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#integrated-surface-dataset-global",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Integrated Surface Dataset (Global)",
    "text": "Integrated Surface Dataset (Global)\nOur data preparation till here was about MERRA-2 dataset. As mentioned earlier, MERRA-2 data type is reanalysis data, since we downscaled this data spatially, verifying this data with real data is necessary. The best database for real hourly climate variables is The Integrated Surface Dataset (ISD). According to its website, ISD is composed of worldwide surface weather observations from over 35,000 stations, though the best spatial coverage is evident in North America, Europe, Australia, and parts of Asia. Parameters included are: air quality, atmospheric pressure, atmospheric temperature/dew point, atmospheric winds, clouds, precipitation, ocean waves, tides and more. ISD refers to the data contained within the digital database as well as the format in which the hourly, synoptic (3-hourly), and daily weather observations are stored. ISD provides hourly data that can be used in a wide range of climatological applications. For some stations, data may go as far back as 1901, though most data show a substantial increase in volume in the 1940s and again in the early 1970s. Currently (2021-08-17), there are over 14,000 “active” stations updated daily in the database.\nIn our study area boundary, there is 20 stations for surface weather observations. The air temperature and rainfall was downloaded for this stations in target time period. This data need preparation to use in analysis. The vaex library is a useful python package for work with large data with low memory usage. We used this library for preparation of surface weather data.\n\n\nCode\n#collapse_output\n\nimport vaex\ndfv = vaex.from_csv('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/2654018.csv')\ndfv\n\n\n/home/nilik/MYPROGRAMS/miniconda3/envs/envinfo/lib/python3.9/site-packages/vaex/__init__.py:524: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n  return _from_csv_read(filename_or_buffer=filename_or_buffer, copy_index=copy_index,\n\n\n\n\n\n#                                    STATION    NAME                           LATITUDE  LONGITUDE  ELEVATION  DATE               SOURCE  REPORT_TYPE  CALL_SIGN  QUALITY_CONTROL  TMP    \n\n\n0        40875099999BANDAR ABBASS INTERNATIONAL, IR27.218169 56.377875  6.7        2000-01-01T00:00:004       FM-12        OIKB       V020             +0142,1\n1        40875099999BANDAR ABBASS INTERNATIONAL, IR27.218169 56.377875  6.7        2000-01-01T07:00:004       FM-15        OIKB       V020             +0260,1\n2        40875099999BANDAR ABBASS INTERNATIONAL, IR27.218169 56.377875  6.7        2000-01-01T08:00:004       FM-15        OIKB       V020             +0280,1\n3        40875099999BANDAR ABBASS INTERNATIONAL, IR27.218169 56.377875  6.7        2000-01-01T09:00:004       FM-15        OIKB       V020             +0290,1\n4        40875099999BANDAR ABBASS INTERNATIONAL, IR27.218169 56.377875  6.7        2000-01-01T10:00:004       FM-15        OIKB       V020             +0300,1\n...                                  ...        ...                            ...       ...        ...        ...                ...     ...          ...        ...              ...    \n2,480,20940836199999PERSIAN GULF AIRPORT, IR       27.366666652.7333333 8.0        2020-12-31T14:00:004       FM-15        99999      V020             +0220,1\n2,480,21040836199999PERSIAN GULF AIRPORT, IR       27.366666652.7333333 8.0        2020-12-31T15:00:004       FM-15        99999      V020             +0200,1\n2,480,21140836199999PERSIAN GULF AIRPORT, IR       27.366666652.7333333 8.0        2020-12-31T16:00:004       FM-15        99999      V020             +0180,1\n2,480,21240836199999PERSIAN GULF AIRPORT, IR       27.366666652.7333333 8.0        2020-12-31T17:00:004       FM-15        99999      V020             +0160,1\n2,480,21340836199999PERSIAN GULF AIRPORT, IR       27.366666652.7333333 8.0        2020-12-31T18:00:004       FM-15        99999      V020             +0150,1\n\n\n\n\nThe name of surface weather stations are:\n\n\nCode\n# collapse\ndfv.NAME.unique()\n\n\n['BANDAR ABBASS INTERNATIONAL, IR',\n 'KERMAN, IR',\n 'RAFSANJAN, IR',\n 'LAMERD, IR',\n 'SIRJAN, IR',\n 'BUSHEHR, IR',\n 'SHIRAZ SHAHID DASTGHAIB INTERNATIONAL, IR',\n 'JAM, IR',\n 'FASA, IR',\n 'YASOGE, IR',\n 'ABADEH, IR',\n 'BANDAR E DAYYER, IR',\n 'BAFT, IR',\n 'GACHSARAN, IR',\n 'LAR, IR',\n 'FARSI ISLAND, IR',\n 'YASOUJ, IR',\n 'KHARG, IR',\n 'ASALOYEH, IR',\n 'PERSIAN GULF AIRPORT, IR']\n\n\nSome columns in this data should be changed, you can see preparation commands in follow.\nCounting values for each station:\n\n\nCode\n# collapse_output\nstation_count = dfv.groupby(by='NAME').agg({'count':'count'})\nfor x in range(len(station_count)):\n    print (station_count[x])\n\n\n['BUSHEHR, IR', 267359]\n['YASOGE, IR', 9650]\n['JAM, IR', 54130]\n['BANDAR E DAYYER, IR', 33972]\n['FASA, IR', 190922]\n['YASOUJ, IR', 166293]\n['BAFT, IR', 53720]\n['GACHSARAN, IR', 149147]\n['ASALOYEH, IR', 2]\n['SHIRAZ SHAHID DASTGHAIB INTERNATIONAL, IR', 407263]\n['KHARG, IR', 74317]\n['PERSIAN GULF AIRPORT, IR', 97346]\n['LAR, IR', 152458]\n['FARSI ISLAND, IR', 15]\n['BANDAR ABBASS INTERNATIONAL, IR', 335261]\n['ABADEH, IR', 110148]\n['LAMERD, IR', 32682]\n['KERMAN, IR', 273070]\n['RAFSANJAN, IR', 18811]\n['SIRJAN, IR', 53648]\n\n\nChecking temperature column data type:\n\n\nCode\n# collapse_output\ndfv['TMP'].data_type\n\n\n<bound method Expression.data_type of Expression = TMP\nLength: 2,480,214 dtype: string (column)\n----------------------------------------\n      0  +0142,1\n      1  +0260,1\n      2  +0280,1\n      3  +0290,1\n      4  +0300,1\n      ...       \n2480209  +0220,1\n2480210  +0200,1\n2480211  +0180,1\n2480212  +0160,1\n2480213  +0150,1\n>\n\n\nIt’s need to be changed the temperature column data type from string to float. So, the ‘Temp’ column values was splitted and then was built new temperature column with correct values.\n\n\nCode\n# collapse_output\ndfv['Temp_C'] = dfv[\"TMP\"].str.split(\",\").apply(lambda x: x[0])\ndfv['Temp_C'].astype('float32')\n\n\nExpression = astype(Temp_C, 'float32')\nLength: 2,480,214 dtype: float32 (expression)\n---------------------------------------------\n      0  142\n      1  260\n      2  280\n      3  290\n      4  300\n    ...     \n2480209  220\n2480210  200\n2480211  180\n2480212  160\n2480213  150\n\n\nThen, we convert temperature values to celsius degree.\n\n\nCode\n#collapse_output\ndfv['Temp_C_2'] = (dfv['Temp_C'].astype('float32'))/10\n\n\nChecking DATA column data type:\n\n\nCode\n# collapse_output\ndfv.DATE.data_type\n\n\n<bound method Expression.data_type of Expression = DATE\nLength: 2,480,214 dtype: string (column)\n----------------------------------------\n      0  2000-01-01T00:00:00\n      1  2000-01-01T07:00:00\n      2  2000-01-01T08:00:00\n      3  2000-01-01T09:00:00\n      4  2000-01-01T10:00:00\n            ...             \n2480209  2020-12-31T14:00:00\n2480210  2020-12-31T15:00:00\n2480211  2020-12-31T16:00:00\n2480212  2020-12-31T17:00:00\n2480213  2020-12-31T18:00:00\n>\n\n\nThe data type of DATA column should be in datetime64 format, so it was changed.\n\n\nCode\n# collapse\ndfv['DATE']=dfv.DATE.astype('datetime64[ns]')\n\n\nAfter finalizing preparation of surface data observations, it was saved as a new CSV file.\n\n\nCode\n# collpase\ndfv.export_csv(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv\")"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#comparison-of-merra-2-data-with-surface-data-observation-data",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#comparison-of-merra-2-data-with-surface-data-observation-data",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Comparison of MERRA-2 data with surface data observation data",
    "text": "Comparison of MERRA-2 data with surface data observation data\nAfter regriding MERRA-2 data and cleaning surface observation data, we can compare these data based on station latitude and longitude. For example we want to compare air temperature in Shiraz station for November 2018.\n\n\nCode\n# collapse\n\n# Import libraries\nimport vaex\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport xesmf as xe\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Opening surface data observations\ndfv = vaex.from_csv('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv')\n\n# Opening Merra-2 data\nds = xr.open_mfdataset(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc\")\nds = ds.set_coords(['lon','lat'])\n\n########## Extracting data from surface weather observation data ##########\n\n# Changing 'DATE' column data type from string to datetime64 \ndfv['DATE']=dfv.DATE.astype('datetime64[ns]')\n\n# Filtering dataframe based on 'NAME' column\nstation_name = 'LAMERD, IR'\nstation = dfv[dfv.NAME == str(station_name)]\n\n# Select a certain month data\nstart_date = np.datetime64('2018-11-01 00:00:00')\nend_date = np.datetime64('2018-11-30 23:00:00')\n\nstartDATE = station[(station.DATE > start_date)]\nrangeDATE = startDATE[(startDATE.DATE < end_date)]\n\n# Drop nan values (999.9)\nedit = rangeDATE[rangeDATE.Temp_C < np.float64(100)]\n\n# Calculate monthly mean temperature for 2018-11\ntrmp_2018_11 = edit.mean(edit['Temp_C'])\n\n# Print surface monthly air temperature\nprint('Surface monthly air temperature for [', station_name, '] is: ', format(float(trmp_2018_11), '.2f'))\n\n########## Extracting data from rrgrided MERRA-2 data ##########\n\n# Sekecting target month\nds201811 = ds.sel(time='2018-11-01')\n\n# Extracting minimum and maximum of lat/lon\nmin_lon = ds201811.lon.min().values\nmin_lat = ds201811.lat.min().values\nmax_lon = ds201811.lon.max().values\nmax_lat = ds201811.lat.max().values\n\n# swap dimensions with multi-dimensional coordinates\nds_xy_grid = ds201811.rename(north_south='lat', east_west='lon')\nds_out = xr.Dataset({'lat': (['lat'], np.linspace(float(min_lat), float(max_lat), int(ds201811.north_south.count()))),\n                     'lon': (['lon'], np.linspace(float(min_lon), float(max_lon), int(ds201811.east_west.count())))})\n                     \nregridder = xe.Regridder(ds_xy_grid, ds_out, method='bilinear')\nds_lonlat_grid = regridder(ds_xy_grid)\nds_lonlat_grid = ds_lonlat_grid.rename(lat='latitude', lon='longitude')\n\n# Extracting latitude and longitude values based on the dataframe of target station\nlatitude = station.LATITUDE.unique()[0]\nlongitude = station.LONGITUDE.unique()[0]\nlatitude = round(latitude, 2)\nlongitude = round(longitude, 2)\n\n# Extracting the air temperature from merra-2 data\npnts = ds_lonlat_grid.where((ds_lonlat_grid.longitude==longitude) & (ds_lonlat_grid.latitude==latitude), drop=True)\ncelsius = (pnts.Tair_f_tavg.values) - 273.15\n\n# Print MERRA-2 monthly air temperature\nprint('MERRA-2 monthly air temperature for [', station_name, '] is: ', format(float(celsius), '.2f'))\n\n\nSurface monthly air temperature for [ LAMERD, IR ] is:  23.44\nMERRA-2 monthly air temperature for [ LAMERD, IR ] is:  27.52\n\n\nIn above example we compared real and reanalused air temperature for one location in a specific time, but it’s ideal to compare all locations in all date time period. So, first we will try to extract the real air temperature for all locations in all time and then extract these values from MERRA-2 data. Finally we’ll try to compare all of data in one plot."
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#extract-surface-temperature-values-for-each-station-in-all-252-months",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#extract-surface-temperature-values-for-each-station-in-all-252-months",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Extract surface temperature values for each station in all 252 months",
    "text": "Extract surface temperature values for each station in all 252 months\nWith this code we can plot monthly air temperature of surface weather stations (real data) year by year.\n\n\nCode\n#collapse\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Opening surface data observations\ndf_station = pd.read_csv('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv')\n\n# Changing 'DATE' column data type from string to datetime64 \ndf_station['DATE']=df_station.DATE.astype('datetime64[ns]')\n\n# there is one station with two name, improve it!\ndf_station.NAME[df_station.NAME=='YASOUJ, IR'] = 'YASOGE, IR'\n\n# Drop nan values (999.9)\ndf_station = df_station[df_station.Temp_C < np.float64(100)]\n\n# Calculate monthly mean temperature in each year\nmean_month_staion = df_station.groupby([df_station.DATE.dt.month, df_station.DATE.dt.year,\n                                 df_station.NAME, round(df_station.LATITUDE, 2), round(df_station.LONGITUDE, 2)])[\"Temp_C\"].mean()\nmean_month_staion = mean_month_staion.rename_axis(['MONTH', 'YEAR', 'NAME', 'LATITUDE', 'LONGITUDE'])\n#mean_month_staion.hvplot.line(x = 'MONTH', y= 'Temp_C', groupby=['NAME', 'YEAR'])"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#extract-merra-2-temperature-values-for-each-station-in-all-252-months",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#extract-merra-2-temperature-values-for-each-station-in-all-252-months",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Extract MERRA-2 temperature values for each station in all 252 months",
    "text": "Extract MERRA-2 temperature values for each station in all 252 months\nWith these two below codes we can extract monthly air temperature from MERRA-2 dataset based on surface weather stations latitude and longitude and then plot them year by year.\n\n\nCode\n# collapse\n\n# import required libraries\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport xesmf as xe\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\n\n# reading MERRA-2 dataset\nds = xr.open_mfdataset(\"/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc\")\nds = ds.set_coords(['lon','lat'])\n\n# Extracting latitude and longitude of surface weather stations\ndf = pd.read_csv('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv')\ndf.NAME[df.NAME=='YASOUJ, IR'] = 'YASOGE, IR'\n\ndf_merra = {'NAME': [], 'time': [],\n            'mean_monthly_temp':[], 'lot':[], 'lon':[]}\n# regriding loop\nfor i in tqdm(range(len(ds.time))):\n    dsi = ds.sel(time = ds.time[i])\n    \n    # Extracting minimum and maximum of lat/lon\n    min_lon = dsi.lon.min().values\n    min_lat = dsi.lat.min().values\n    max_lon = dsi.lon.max().values\n    max_lat = dsi.lat.max().values\n    \n    # swap dimensions with multi-dimensional coordinates\n    ds_xy_grid = dsi.rename(north_south='lat', east_west='lon')\n    ds_out = xr.Dataset({'lat': (['lat'], np.linspace(float(min_lat),\n                                                      float(max_lat), int(dsi.north_south.count()))),\n                         'lon': (['lon'], np.linspace(float(min_lon),\n                                                      float(max_lon), int(dsi.east_west.count())))})\n\n    \n    regridder = xe.Regridder(ds_xy_grid, ds_out, method='bilinear')\n    ds_lonlat_grid = regridder(ds_xy_grid)\n    ds_lonlat_grid = ds_lonlat_grid.rename(lat='latitude', lon='longitude')\n    \n    # Extracting latitude and longitude of surface weather stations\n    for x in range (len(df.NAME.unique())):\n        # Filtering dataframe based on 'NAME' column\n        station_name = df.NAME.unique()[x]\n        station = df[df.NAME == str(station_name)]\n        latitude = station.LATITUDE.unique()[0]\n        longitude = station.LONGITUDE.unique()[0]\n        latitude = round(latitude, 2)\n        longitude = round(longitude, 2)\n\n        # Extracting the air temperature from merra-2 data\n        pnts = ds_lonlat_grid.where((ds_lonlat_grid.longitude==longitude) &\n                                    (ds_lonlat_grid.latitude==latitude), drop=True)\n        temp_value = pnts.Tair_f_tavg.values\n        if temp_value.size == 0:\n            celsius = -9999.99\n        else:\n            celsius = float((pnts.Tair_f_tavg.values) - 273.15)\n        \n        \n        #print(station_name, dsi.time.values, str(pnts.Tair_f_tavg.values), latitude, longitude)\n        #data = {\"NAME\": station_name, 'time': str(dsi.time.values),\n        #                     'mean_monthly_temp':celsius, 'lot':latitude, 'lon':longitude}\n        df_merra['NAME'].append(station_name)\n        df_merra['time'].append(str(dsi.time.values))\n        df_merra['mean_monthly_temp'].append(celsius)\n        df_merra['lat'].append(latitude)\n        df_merra['lon'].append(longitude)\n\n        del station_name, station, latitude, longitude, pnts, celsius\n        \n    del dsi, min_lon, min_lat, max_lon, max_lat, ds_xy_grid, ds_out, regridder, ds_lonlat_grid\n\n# Save output as a CSV file\ndf_merra2 = pd.DataFrame.from_dict(df_merra)\ndf_merra2.to_csv('./data/monthly_merra2_stations.csv')\n\n\n\n\nCode\n# collapse\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Opening surface data observations\ndf_merra = pd.read_csv('./data/monthly_merra2_stations.csv')\n\n# Changing 'DATE' column data type from string to datetime64 \ndf_merra['time']=df_merra.time.astype('datetime64[ns]')\n\n# Drop nan values (-9999.99)\ndf_merra = df_merra[df_merra.mean_monthly_temp > np.float64(-9999.99)]\ndf_merra.drop(['Unnamed: 0'], axis=1)\n# Calculate monthly mean temperature in each year\nmean_month_merra = df_merra.groupby([df_merra.time.dt.month, df_merra.time.dt.year,\n                               df_merra.NAME, df_merra.lot, df_merra.lon])[\"mean_monthly_temp\"].mean()\nmean_month_merra = mean_month_merra.rename_axis(['MONTH', 'YEAR', 'NAME', 'LATITUDE', 'LONGITUDE'])\n#mean_month_merra.hvplot.line(x = 'MONTH', y= 'mean_monthly_temp', groupby=['NAME', 'YEAR'])\n\n\nNow, we can plot monthly air temperature of MERRA-2 data and surface weather stations simultaneously.\n\n\nCode\n# collapse\nimport panel as pn\n\nmean_month_all = pd.concat([mean_month_staion, mean_month_merra], axis=1)\nplt_station = mean_month_all.hvplot.line(x = 'MONTH', y= 'Temp_C' ,\n                                 groupby=['NAME', 'YEAR'], label='Surface Stations')\nplt_merra = mean_month_all.hvplot.line(x = 'MONTH', y= 'mean_monthly_temp' ,\n                                 groupby=['NAME', 'YEAR'], label='MERRA-2 Reanalysis')\ncompare_plots = (plt_station * plt_merra).opts(legend_position='top_left')\n\n# Using panel\nhv_panel = pn.panel(compare_plots, widget_location='top_left')\nhv_panel.save('../_pages/htmls_plots/compare_plots.html', embed=True)"
  },
  {
    "objectID": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#plotting-monthly-air-temperature-of-real-data-vs-reanalysis-data",
    "href": "posts/analysis_methods/2021-08-26-Anaysis_Process_Merra2.html#plotting-monthly-air-temperature-of-real-data-vs-reanalysis-data",
    "title": "Analysis process of downscaled MERRA-2 data",
    "section": "Plotting Monthly Air Temperature of Real Data VS Reanalysis DATA",
    "text": "Plotting Monthly Air Temperature of Real Data VS Reanalysis DATA\n\n\nCode\n#hide_input\nfrom IPython.display import IFrame\nIFrame(src='./htmls/compare_plots.html', width=1000, height=500)\n\n\n\n        \n        \n\n\nIt seems that we have a problem with these data! Both patterns of variation in monthly data are similar but with one month lag in each other. Because of this problem, I checked data. In surface weather station we have hourly data that mean monthly data was aggregated from these hourly data. In downscaled merra-2 data we time dimension array of first day of each month such as 2000-01-01T00:00:00.000000000, I assumed that each mean temperature for each item of time in merra-2 data is for its past month, so I changed the time of merra-2 data with below code and then compared the monthly mean air temperature from surface weather stations with new merra-2 data.\n\n\nCode\n# collapse\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Opening surface data observations\ndf_merra_new = pd.read_csv('./data/monthly_merra2_stations.csv')\n\n# Changing 'DATE' column data type from string to datetime64 \ndf_merra_new['time']=df_merra_new.time.astype('datetime64[ns]')\n\n# Drop nan values (-9999.99)\ndf_merra_new = df_merra_new[df_merra_new.mean_monthly_temp > np.float64(-9999.99)]\ndf_merra_new.drop(['Unnamed: 0'], axis=1)\n\n\ndf_merra_new['time'] = df_merra_new.time + pd.DateOffset(months=-1)\ndf_merra_new = df_merra_new[df_merra_new['time'] > '1999-12-01']\n\n# Calculate monthly mean temperature in each year\nmean_month_merra_new = df_merra_new.groupby([df_merra_new.time.dt.month, df_merra_new.time.dt.year,\n                               df_merra_new.NAME, df_merra_new.lot, df_merra_new.lon])[\"mean_monthly_temp\"].mean()\nmean_month_merra_new = mean_month_merra_new.rename_axis(['MONTH', 'YEAR', 'NAME', 'LATITUDE', 'LONGITUDE'])\n#mean_month_merra.hvplot.line(x = 'MONTH', y= 'mean_monthly_temp', groupby=['NAME', 'YEAR'])\n\n# compare plots\nmean_month_all_new = pd.concat([mean_month_staion, mean_month_merra_new], axis=1)\nab1 = mean_month_all_new.hvplot.line(x = 'MONTH', y= 'Temp_C' ,\n                                 groupby=['NAME', 'YEAR'], label='Station')\nab2 = mean_month_all_new.hvplot.line(x = 'MONTH', y= 'mean_monthly_temp' ,\n                                 groupby=['NAME', 'YEAR'], label='MERRA-2')\ncompare_plots_new = (ab1 * ab2).opts(legend_position='top_left')\n\n# Using panel\nhv_panel_new = pn.panel(compare_plots_new, widget_location='top_left')\nhv_panel_new.save('../_pages/htmls_plots/compare_plots_changed_merra_time.html', embed=True)\n\n\n\n\nCode\n#hide_input\nfrom IPython.display import IFrame\nIFrame(src='./htmls/compare_plots_changed_merra_time.html', width=1000, height=500)\n\n\n\n        \n        \n\n\nIn follow you can see the time series of hourly surface weather station data availability identified by station name.\n\n\nCode\n# collapse\n\nimport pandas as pd\nimport hvplot.pandas\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv')\n\n# remove max value of temperature as NAN value\ndf = df[df.Temp_C != 999.9]\n\n# there is one station with two name, improve it!\ndf.NAME[df.NAME=='YASOUJ, IR'] = 'YASOGE, IR'\n\ndf['time_hour'] = pd.to_datetime(df['DATE']).dt.hour\ndf['time_day'] = pd.to_datetime(df['DATE']).dt.day\ndf['time_month'] = pd.to_datetime(df['DATE']).dt.month\ndf['time_year'] = pd.to_datetime(df['DATE']).dt.year\n\n# Plot time series of data\nscaterplot = df.hvplot.scatter(x='time_hour', y='time_day',groupby=['NAME','time_year', 'time_month'])\n\n# Using panel\nscatter_panel = pn.panel(scaterplot, widget_location='top_left')\nscatter_panel.save('../_pages/htmls_plots/scatterplot.html', embed=True)\n\n\n\n\nCode\n#hide_input\nfrom IPython.display import IFrame\nIFrame(src='./htmls/scatterplot.html', width=1000, height=550)\n\n\n\n        \n        \n\n\n\n\nCode\n# hide_input\n\n################## Convert CSV file to NetCDF file ##################\n# import libraries\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Opening surface data observations\npath_file = '/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/'\ndf = pd.read_csv(path_file + 'improved_stations.csv')\n\n# convert DATE column to datetime\ndf['DATE'] = pd.to_datetime(df['DATE'])\n\n# set Date as index\ndf = df.set_index('DATE')\n\n# remove max value of temperature as NAN value\ndf = df[df.Temp_C != 999.9]\n\n# there is one station with two name, improve it!\ndf.NAME[df.NAME=='YASOUJ, IR'] = 'YASOGE, IR'\n\n# Stations name as a list\nstations = df.NAME.unique()\n\n# output path\noutput_path = '/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/Final_Imporoved_CSV/'\n\nfor i in stations:\n    dfi = df[df['NAME']==i]\n    \n    # remove duplicate\n    dfi_clean = dfi[~dfi.index.duplicated(keep='first')]\n    \n    # convert latitude and lonfitude column to eastin and northing respectively, these new columns will used for plotting points with leaflet library\n    dfi_clean[\"easting\"], dfi_clean[\"northing\"] = hv.Tiles.lon_lat_to_easting_northing(dfi_clean[\"LONGITUDE\"], dfi_clean[\"LATITUDE\"])\n    \n    # drop 'year', 'month', 'day', 'hour', 'SOURCE', 'REPORT_TYPE', 'CALL_SIGN', 'QUALITY_CONTROL' and 'TMP' columns\n    df_export = dfi_clean.drop(['SOURCE', 'REPORT_TYPE', 'CALL_SIGN', 'QUALITY_CONTROL', 'TMP'], axis=1)\n    \n    # add a column with station name\n    df_export['station'] = str(i)\n\n    # Save final modified (remove NAN & duplicated values)\n    df_export.to_csv(output_path + i + '.csv', sep='\\t')\n    \n    del dfi, dfi_clean, df_export\n\n##### save CSV file stations as NetCDF file\n\nall_files = glob.glob(os.path.join(output_path, \"*.csv\"))\n\nfor filename in all_files:\n    namecsv = os.path.splitext(os.path.basename(filename))[0]\n    df = pd.read_csv(filename, header=0, error_bad_lines=False, sep='\\t')\n    df[\"DATE\"]= pd.to_datetime(df[\"DATE\"]) \n\n    xr = df.set_index(['LATITUDE', 'LONGITUDE', 'DATE', 'easting', 'northing', 'station']).to_xarray()\n\n    # add variable attribute metadata\n    xr['Temp_C'].attrs={'units':'c', 'long_name':'Air Temperature'}\n\n    # Save NetCDF file\n    xr.to_netcdf(output_path + namecsv + '.nc')\n    del namecsv, df, xr"
  },
  {
    "objectID": "posts/data_science/2022-11-11-Scraping_AQI.html",
    "href": "posts/data_science/2022-11-11-Scraping_AQI.html",
    "title": "Scraping AQI",
    "section": "",
    "text": "Data available in https://aqicn.org is good for downloading all data, but this data in compartion of data available in https://aqms.doe.ir/ site is incomplete and incorrect. So, because the latest site has DOE reference it is better to obtain data from this site. Downloading data from https://aqms.doe.ir/ can be done in multi-steps as in following:\n1- Install chrome extension Print Friendly & PDF. 2- Open site in English language. 3- Define Date. 4- Export data as PDF. 5- Convert pdf files to python list with tabula-py python library. 6- Convert list to Pandas Dataframe. 7- This data is AQI and must be converted to Concenteartion, for doing it we can use AQI Calculator."
  },
  {
    "objectID": "posts/data_science/2023-02-08-Geospatial_ML_DL.html",
    "href": "posts/data_science/2023-02-08-Geospatial_ML_DL.html",
    "title": "GepSpatial ML-DL Sources",
    "section": "",
    "text": "Datasets for deep learning with satellite & aerial imagery\nlists software for working with satellite & aerial imagery data & datasets.\ncourse\nTraining and deployment of deep learning models for satellite & aerial imagery\nAnnotation of datasets for deep learning applied to satellite and aerial imagery\nMineral Prediction Mapping with Machine Learning\nPredicting Spatial Data with Machine Learning\n\ngeospatial-machine-learning\n\nPython module for geospatial prediction using scikit-learn and rasterio\ngeospatial-learn\nDarts: Time Series Made Easy in Python\nCourse materials for: Geospatial Data Science\nDeep learning (DL) on satellite imagery\nA Training of Trainers Bootcamp on Machine Learning for Earth Observations\nPyspatialml: Machine learning classification and regression modelling for spatial raster data\nTorchGeo: datasets, samplers, transforms, and pre-trained models for geospatial data\nLand Cover Mapping\nLand cover mapping of the Orinoquía region in Colombia, in collaboration with Wildlife Conservation Society Colombia\nSpatial Data Science: Spatial Data Storage\nSpatial classification and regression using Scikit-learn and Rasterio\nAlgorithms and utilities for Synthetic Aperture Radar (SAR) sensors\nAwesome-EarthObservation-Code\nMachine Learning Notebooks\nSharing machine learning course / lecture notes\nExplanation to key concepts in ML\n10 Weeks, 20 Lessons, Data Science for All!"
  },
  {
    "objectID": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#tutorials-articles",
    "href": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#tutorials-articles",
    "title": "GepSpatial ML-DL Sources",
    "section": "Tutorials & Articles",
    "text": "Tutorials & Articles\n\nIntroduction to Machine Learning with scikit-learn\nPython examples of popular machine learning algorithms with interactive Jupyter demos and math being explained\nAn Introduction to Earth and Environmental Data Science\nThe Illustrated Machine Learning website\nMachine Learning on Earth Observation: ML4EO Bootcamp\nPredicting Rain from Satellite Images\nExciting Machine Learning Project On Satellite Images\nApplying Deep Learning on Satellite Imagery Classification\nMineral Prediction Mapping with Machine Learning\nGeospatial Machine Learning\nGeospatial Deep Learning\nIntroducing Machine Learning for Spatial Data Analysis\nSpatial Prediction using ML in Python\nTree-Boosting for Spatial Data\nWorking with Geospatial Data in Machine Learning\nHow to Extract Locations from Text with Natural Language Processing\nGeoparsing with Python and Natural Language Processing\nGeographic Data Science 8 video tutorials on spatial and spatiotemporal data in Python\nGeographic Data Science with PySAL and the pydata stack\nThe Rise of Machine Learning (ML): How to Use Artificial Intelligence in GIS\nA Guide: Turning OpenStreetMap Location Data into ML Features\nLatest in Geospatial Data Science Developments\nTrain & Deploy Geospatial Deep Learning Application in Python\nUse Google Colab Like A Pro\nUnderstanding Satellite Image For Geo-spatial Deep Learning\nLand Cover Classification\nHow to Perform Exploratory Data Analysis in QGis\nDeep learning for Geospatial data applications — Multi-label Classification\nIndexing and querying spatial data\nMachine Learning of Spatial Data\nSpatial Data Analysis With Hexagonal Grids\nRICE MAPPING USING DEEP NEURAL NETWORKS\nLand Cover and Land Use Classification\nAll Machine Learning Algorithms You Should Know for 2023\nExploring Time and Space\nArtificial Intelligence for Geospatial Analysis with Pytorch’s TorchGeo (Part 1)\nTorchGeo: datasets, samplers, transforms, and pre-trained models for geospatial data.\nGeospatial Machine Learning\nPEARL: Land Cover Mapping\nLand Cover Classification\nDetecting deforestation from satellite images\nSpatial Autocorrelation: Neighbors Affecting Neighbors\nAn Artificial Intelligence Dataset for Solar Energy Locations in India -Techniques for deep learning with satellite & aerial imagery\nSpatial Regression\nGeospatial-learn\nA Guide: Turning OpenStreetMap Location Data into ML Features\nWorkflow Machine Learning on Satellite Image and Geospatial Data Using Pyspatialml\nAutoGluon: AutoML for Text, Image, Time Series, and Tabular Data\nEnhancement of MODIS NIDVI to 10m resolution using U-Net\nS1S2randomforest\nGeopandas Hands-on: Building Geospatial Machine Learning Pipeline\nApplying Deep Learning on Satellite Imagery Classification\nMachine Learning of Spatial Data — A Critical Review\nAI Geospatial Wildfire Risk Prediction\nMachine learning in earth sciences\nHow We Mapped the World’s Solar Power Plants\nHandling Missing Values with Random Forest\nForecasting Atmospheric CO2 with Python\nTime Series Causal Impact Analysis in Python\nHow to build a machine learning model\nforestfire impact prediction\nHow to Load Kaggle Datasets Directly into Google Colab?\nExplaining the Math of how Neural Networks Learn with Implementation from Scratch"
  },
  {
    "objectID": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#cheat-sheets",
    "href": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#cheat-sheets",
    "title": "GepSpatial ML-DL Sources",
    "section": "Cheat Sheets",
    "text": "Cheat Sheets\n\n101 machine learning algorithms for data science with cheat sheets"
  },
  {
    "objectID": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#papers",
    "href": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#papers",
    "title": "GepSpatial ML-DL Sources",
    "section": "Papers",
    "text": "Papers\n\nA novel framework for spatio-temporal prediction of environmental data using deep learning\nSpatial machine learning: new opportunities for regional science\nExploring Google Earth Engine Platform for Big Data Processing: Classification of Multi-Temporal Satellite Imagery for Crop Mapping\nGeographic Data Science with Python\nExploratory Spatial Data Analysis with Python\nHow to use GIS for Machine Learning\nDynamic population mapping with AutoGluon\nMachine learning in Earth and environmental science requires education and research policy reforms\nTen Ways to Apply Machine Learning in Earth and Space Sciences\nArtificial Intelligence and Machine Learning in Earth Science"
  },
  {
    "objectID": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#exploratory-spatial-data-analysis-esda",
    "href": "posts/data_science/2023-02-08-Geospatial_ML_DL.html#exploratory-spatial-data-analysis-esda",
    "title": "GepSpatial ML-DL Sources",
    "section": "Exploratory Spatial Data Analysis (ESDA)",
    "text": "Exploratory Spatial Data Analysis (ESDA)\n\nExploratory Data Analysis (EDA) on Satellite Imagery Using EarthPy\nWhat is Exploratory Spatial Data Analysis (ESDA)?\nESDA: Exploratory Spatial Data Analysis\nExploratory Spatial Data Analysis\nExploratory Spatial Data Analysis (ESDA) – Spatial Autocorrelation\nexploratory data analysis\nExploratory Spatial Data Analysis (ESDA)\nGeoDa: An introduction to spatial data analysis\nESDA with pygeoda and geopandas\nExploratory Spatial Data Analysis (ESDA)"
  },
  {
    "objectID": "posts/data_science/2023-03-23-TopoPyScale.html",
    "href": "posts/data_science/2023-03-23-TopoPyScale.html",
    "title": "TopoPyScale",
    "section": "",
    "text": "TopoPyScale is a downscaling toolbox for globmal and regional climate model datasets, particularly relevant to mountain ranges, and hillslopes.\nsrc: https://topopyscale.readthedocs.io/en/latest/m\nTopoPyScale uses both climate model data and Digital Elevation Models (DEM) for correcting atmospheric state variables (e.g. temperature, pressure, humidity, etc). TopoPyScale provides tools to interpolate and correct such variables to be relevant locally given a topographical context.\nThe most basic requirements of TopoPyScale is a DEM used to defined the spatial domain of interest as well as compute a number of morphometrics, and configuration file defining the temporal period, the downscaling methods and other parameters. In its current version, TopoPyScale includes the topoclass class that wraps all functionalities for ease of use. It automatically fetches data from the ERA5 repositories (Pressure and Surface levels). Other climate data sources can be added. Based on the high resolution (30-100m) DEM and the climate data, methods in the topoclass will compute, correct and interpolate variables need to force specialized land-surface models.\nIndeed, with this library you can download hourly ERA5 climate data and the downscale them in desires spatial resolution.\nDownscaled variable includes:\n2m air temperature\n2m air humidity\n2m air pressure\n10m wind speed and direction\nSurface incoming shortwave radiation\nSurface incoming longwave radiation\nPrecipitation (possibility to partition snow and rain)\nIn the following, you can done downscaling climate data step-by-step."
  },
  {
    "objectID": "posts/data_science/2023-03-23-TopoPyScale.html#very-important-note",
    "href": "posts/data_science/2023-03-23-TopoPyScale.html#very-important-note",
    "title": "TopoPyScale",
    "section": "Very Important Note:",
    "text": "Very Important Note:\n\nTo run TopoPyScale you must in python 3.9 environment.\n\n\n1- Python Environment Preparation\nTopoPyScale is tested for Python 3.9. You may create a new virtual environment using conda prior to installation.\nconda create -n downscaling python=3.9 ipython\nconda activate downscaling\nFor install dependencies use mamba instead conda:\nmamba install xarray matplotlib scikit-learn pandas numpy netcdf4 h5netcdf rasterio pyproj dask\n\n\n2- Release Installation\npip install topopyscale\n\n\n3- Setting up cdsapi\nThen you need to setup your cdsapi with the Copernicus API key system. After after creating an account with Copernicus, use following step for setting up to download EAR5 data.\n\n3-1- Config File\nOn Linux, create a file gedit ~/.cdsapirc or $HOME/.cdsapirc with inside:\nurl: https://cds.climate.copernicus.eu/api/v2\nkey: 2609:a8a3aba4-af46-4f7a-a79b-5799b58def50\n\n\n3-2- Install cdsapi\npip install cdsapi\n\n\n3-3- Testing download climate data\nIn jupyterlab cell:\nimport cdsapi\nc = cdsapi.Client()\nc.retrieve(\"reanalysis-era5-pressure-levels\",\n{\n\"variable\": \"temperature\",\n\"pressure_level\": \"1000\",\n\"product_type\": \"reanalysis\",\n\"year\": \"2008\",\n\"month\": \"01\",\n\"day\": \"01\",\n\"time\": \"12:00\",\n\"format\": \"grib\"\n}, \"download.grib\")\n\n\n\n4- Create your project directory\nFolders and files in your project directory must be as follow: Note: inputs, dem amd config.yml files are mandatory\nmy_project/\n    ├── inputs/\n        ├── dem/ \n            ├── my_dem.tif\n            └── pts_list.csv  (OPTIONAL: to downscale to specific points)\n        └── climate/\n            ├── PLEV*.nc\n            └── SURF*.nc\n    ├── outputs/\n            ├── tmp/\n    ├── pipeline.py (OPTIONAL: script for the downscaling instructions)\n    └── config.yml\n\n\n5- Create Config file\nCreate config.yml file in project directory with inside for example:\nproject:\n    name: Khaeiz\n    description: Downscaling for the Khaeiz mountains\n    authors:\n        - Madadi H.\n    date: March 2023\n    directory: /mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/\n    start: 2020-01-01\n    end: 2020-12-31\n    split:\n        IO: False\n        time: 1  # run indivudal batch in time\n        space: None  # to be implemented\n    extent: \n    CPU_cores: 4\n    climate: era5\n\nclimate:\n    era5:\n        path: inputs/climate/\n        product: reanalysis\n        timestep: 1H\n        plevels: [700,750,800,850,900,950,1000]\n        download_threads: 12\n\ndem:\n    file: srtm_47_06_z39_khaeiz.tif\n    epsg: 32639\n    horizon_increments: 10\n\nsampling:\n    method: toposub\n\n\n    toposub:\n        clustering_method: minibatchkmean\n        n_clusters: 50\n        random_seed: 2\n        clustering_features: {'x':1, 'y':1, 'elevation':4, 'slope':1, 'aspect_cos':1, 'aspect_sin':1, 'svf':1}\n\n\ntoposcale:\n    interpolation_method: idw\n    pt_sampling_method: nearest\n    LW_terrain_contribution: True\n\noutputs:\n    variables: all  # list or combination name\n    file:\n        clean_outputs: True\n        clean_FSM: True\n        df_centroids: df_centroids.pck\n        ds_param: ds_param.nc\n        ds_solar: ds_solar.nc\n        da_horizon: da_horizon.nc\n        landform: landform.tif\n        downscaled_pt: down_pt_*.nc\n\n\n6- Run codes\n\n# Optimize Number of Clusters\nfrom TopoPyScale import topoclass as tc\nimport numpy as np\n\nconfig_file = '/mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/config.yml'\nmp = tc.Topoclass(config_file)\nmp.compute_dem_param()\ndf = mp.search_optimum_number_of_clusters(cluster_range=np.arange(100,1000,50),plot=False)\n\n\nimport pandas as pd\nfrom TopoPyScale import topoclass as tc\nfrom matplotlib import pyplot as plt\n\n\n# ========= STEP 1 ==========\n# Load Configuration\nconfig_file = '/mnt/864424144424098F/Anaconda_Projects/Climate_DownScale/Caracal/Topopyscale_prj_khaeiz/config.yml'\nmp = tc.Topoclass(config_file)\n\n\n# ======== STEP 2 ===========\n# Compute parameters of the DEM (slope, aspect, sky view factor)\nmp.compute_dem_param()\nmp.extract_topo_param()\n\n\n# ----- Option 1:\n# Compute clustering of the input DEM and extract cluster centroids\n#mp.extract_dem_cluster_param()\nmp.extract_topo_cluster_param()\n# plot clusters\nmp.toposub.plot_clusters_map()\n# plot sky view factor\nmp.toposub.plot_clusters_map(var='svf', cmap=plt.cm.viridis)\n\n\n# ========= STEP 3 ==========\n# compute solar geometry and horizon angles\nmp.compute_solar_geometry()\nmp.compute_horizon()"
  },
  {
    "objectID": "posts/data_science/2023-03-23-TopoPyScale.html#note",
    "href": "posts/data_science/2023-03-23-TopoPyScale.html#note",
    "title": "TopoPyScale",
    "section": "Note:",
    "text": "Note:\nIt is possible to improve accuracy by determining"
  },
  {
    "objectID": "posts/explanations/2021-03-01_create_blog.html",
    "href": "posts/explanations/2021-03-01_create_blog.html",
    "title": "Blogging with Quarto",
    "section": "",
    "text": "Building Own weblog with Quarto + Github\n1- Install Quarto in Ubuntu: * Download quarto-1.2.247-linux-amd64.deb fil * run sudo dpkg -i quarto-1.2.247-linux-amd64.deb to instal quarto\n2- Create myblog quarto project in a temperory directory such as temp: -quarto create-project myblog --type website:blog\n3- Created files include: * Created _quarto.yml * Created .gitignore * Created index.qmd * Created posts/welcome/index.qmd * Created posts/post-with-code/index.qmd * Created about.qmd * Created styles.css * Created posts/_metadata.yml\n4- Install Git\nsudo apt-get update & sudo apt-get install git\n5- Install Github Desktop\nwget -qO - https://mirror.mwt.me/ghd/gpgkey | sudo tee /etc/apt/trusted.gpg.d/shiftkey-desktop.asc > /dev/null\nsudo sh -c 'echo \"deb [arch=amd64] https://packagecloud.io/shiftkey/desktop/any/ any main\" > /etc/apt/sources.list.d/packagecloud-shiftkey-desktop.list'\nsudo apt update && sudo apt install github-desktop\nCreate a Github repository Craete a repo with a specific name, such as example name.\n6- Open Github Desktop:\nclone the new github repository created in 5\n4- Change _quarto.yml (in temp directory) and add docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n4-1- Copy jupyter note book in a folder inside posts directory For each doc you must have a folder with name of your document. first cell in each jupyter note book as below:\n---\ntitle: \"Geospatial Data Address\"\nauthor: \"Hossein Madadi\"\ndate: \"2021-03-25\"\ncategories: [Geospatial Data]\n---\n5- Go to inside the myblog in temp directory and open terminal 6- Render the site in terminal\n`quarto render`\n7- Copy all files from temp directory to local github repository directory. 8- Commit and push in github desktop 9- In github repository web change settings: in pages from left panel change Branch to main and select docs then save 10- Open weblog with below url:\n`https://username.github.io/reponame`\n11- Quarto render 12- Committing and pushing will make the changes you see locally live on your website \nExample for use .ipynb in Quarto.\nhttps://github.com/fastai/nbdev"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html",
    "title": "Jupyterlab",
    "section": "",
    "text": "Install: curl micro.mamba.pm/install.sh | bash\n-Check: micromamba –version\n-Help: micromamba –help\n-Create “.bashrc” file in home directory and paste below text:\n\n\n\n\n\nexport MAMBA_EXE=“/home/nilik/.local/bin/micromamba”; export MAMBA_ROOT_PREFIX=“/home/nilik/micromamba”; __mamba_setup=“\\((\"\\)MAMBA_EXE” shell hook –shell bash –prefix “$MAMBA_ROOT_PREFIX” 2> /dev/null)” if [ \\(? -eq 0 ]; then  eval \"\\)__mamba_setup” else if [ -f “/home/nilik/micromamba/etc/profile.d/micromamba.sh” ]; then . “/home/nilik/micromamba/etc/profile.d/micromamba.sh” else export PATH=“/home/nilik/micromamba/bin:$PATH” # extra space after export prevents interference from conda init fi fi unset __mamba_setup\n\n\n\n-For first time After installation and creating “.bashrc” file: ‘micromamba’ is running as a subprocess and can’t modify the parent shell. Thus you must initialize your shell before using activate and deactivate.\nTo initialize the current bash shell, run:\n    $ eval \"$(micromamba shell hook --shell=bash)\"\nand then activate or deactivate with:\n    $ micromamba activate\nTo automatically initialize all future (bash) shells, run: $ micromamba shell init –shell=bash –prefix=~/micromamba\nSupported shells are {bash, zsh, csh, xonsh, cmd.exe, powershell, fish}.\n-Automatically initialize all future (bash) shells micromamba shell init –shell=bash –prefix=~/micromamba\n-Restart Terminal, close and open again.\n-Create Virtual Environmnet: micromamba create -n \n-Check all available environmnets by running: micromamba env list\n-Activate environment: micromamba activate # this activates the base environment micromamba activate  # this activates the  environment\n-Install packages: micromamba install geemap -c conda-forge"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-jupyterlab-app-desktop-updated-06152022",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-jupyterlab-app-desktop-updated-06152022",
    "title": "Jupyterlab",
    "section": "Install Jupyterlab App Desktop (Updated 06/15/2022)",
    "text": "Install Jupyterlab App Desktop (Updated 06/15/2022)\n\n1- Install Miniconda\n\nBest resources for install Anaconda or Miniconda with spatial packages*\n\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html\nDownload miniconda https://docs.conda.io/en/latest/miniconda.html#linux-installers\nMiniconda3-latest-Linux-x86_64.sh\nTo install Miniconda on Ubuntu 20.04 from command line, it only takes 3 steps excluding creating and activating a conda environment.\n(conda) Constructor to bundle JupyterLab Desktop Server into the stand-alone application. You can install Constructor using: conda install -c conda-forge constructor\nA- Download the latest shell script wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\nB- Make the miniconda installation script executable: bash Miniconda3-latest-Linux-x86_64.sh\nC- Run miniconda installation script: ./Miniconda3-py37_4.8.3-Linux-x86_64.sh\n\n\n2- Create and activate the conda environment\nTo create a conda environment, run:\nconda create -n newenv\n\n\n3- Install Jupyterlab\nconda activate newenv  conda install -c conda-forge jupyterlab\n\n\n4- Install Jupyterlab Desktop\nDownload and Install app:\nhttps://github.com/jupyterlab/jupyterlab-desktop/releases/latest/download/JupyterLab-Setup-Debian.deb\nUse Use the boundled Python environment\n\n\n5- Install Geemap Notes\nThe geemap package has some optional dependencies, such as “GeoPandas” and “localtileserver”. It is important to install all packages from conda and not using pip for install packages.\nconda create -n gee python  conda install geemap -c conda-forge  conda install -c conda-forge geopandas  conda install -c conda-forge rasterio  … … …\nInstall all packages with conda.\n\n\n5-1- Scikit-learn and Geemap\nfrom sklearn.ensemble import RandomForestClassifier\nError\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [6], line 17\n     15 from sklearn.model_selection import train_test_split\n     16 from sklearn.ensemble import RandomForestClassifier\n---> 17 from treeinterpreter import treeinterpreter as ti\n\nFile ~/Miniconda3/envs/gee/lib/python3.10/site-packages/treeinterpreter/treeinterpreter.py:5\n      2 import numpy as np\n      3 import sklearn\n----> 5 from sklearn.ensemble.forest import ForestClassifier, ForestRegressor\n      6 from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, _tree\n      7 from distutils.version import LooseVersion\n\nModuleNotFoundError: No module named 'sklearn.ensemble.forest'\nSolution\nopen file: File ~/Miniconda3/envs/gee/lib/python3.10/site-packages/treeinterpreter/treeinterpreter.py\nChange from:\nfrom sklearn.ensemble.forest import ForestClassifier, ForestRegressor\nto:\nfrom sklearn.ensemble._forest import ForestClassifier, ForestRegressor\n\n\n6- Configure File\n\n\n\n\n\n\n## NEW EDIT: (2022-09-06)\n\n\nFOR Jupyterlab Desktop\n\n\n-For recent nbclassic and JupyterLab >= 3 use c.ServerApp.root_dir instead of c.NotebookApp.notebook_dir (and jupyter-lab –generate-config instead of jupyter notebook –generate-config).\n\n\n-“jupyter_lab_config.py” file must be created in home/…/.jupyter directory.\n\n\n-Remove the # at the beginning of the line to allow the line to execute\n\n\n-example:\n\n\nc.ServerApp.root_dir = ‘/mnt/…/Anaconda_Projects’\n\n\n-In final: created and modified “jupyter_lab_config.py” file should be moved to /home/…/.config/jupyterlab-desktop\n\n\nFOR Jupyterlab in Browser\n\n\ncreated and modified “jupyter_lab_config.py” file should be moved to /home/…/.jupyter.\n\n\n\nDefault directory for jlab desktop is home, so for change default directory first create configuration file by (Note:Done in newenv environemnt):\njupyter notebook –generate-config\ncreated file: $ /home/…/.jupyter/jupyter_notebook_config.py\nBased on: https://github.com/jupyterlab/jupyterlab-desktop#configuration-files\nBy default JupyterLab Desktop will only load and write to Jupyter configuration located in:\n$XDG_CONFIG_HOME/jupyterlab-desktop or ~/.config/jupyterlab-desktop on Linux\nignoring any other Jupyter configuration which may be present in standard Jupyter paths as defined by jupyter –paths. This includes jupyter-server settings, jupyterlab settings and workspaces, and any other configuration which would normally be shared between Jupyter installations.\nSo, jupyter_notebook_config.py create file should be moved to /home/…/.config/jupyterlab-desktop. Then change from:\n# c.NotebookApp.notebook_dir \nto:\nc.NotebookApp.notebook_dir = 'user/directory/path'"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#some-problems",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#some-problems",
    "title": "Jupyterlab",
    "section": "7- Some Problems",
    "text": "7- Some Problems\nSometimes, we may have a problem such as below:\nCollecting package metadata (current_repodata.json): failed\nUnavailableInvalidChannel: The channel is not accessible or is invalid.\n  channel name: pkgs/main\n  channel url: https://repo.anaconda.com/pkgs/main\n  error code: 403\n\nYou will need to adjust your conda configuration to proceed.\nUse `conda config --show channels` to view your configuration's current state,\nand use `conda config --show-sources` to view config file locations.\nFor solve this problem in terminal of jupyterlab app run:\nconda config --remove-key channels\nThen we can create a virtual environment (venv) by:\nconda create -n <your envn name>\nWith conda info and conda config –show-sources, it can be get some information about jupyterlab app and its venv and etc."
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#jupyterlab-spellchecker",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#jupyterlab-spellchecker",
    "title": "Jupyterlab",
    "section": "Jupyterlab spellchecker",
    "text": "Jupyterlab spellchecker\ninstall:\njupyter labextension install @ijmbarr/jupyterlab_spellchecker"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#uninstall-full-program",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#uninstall-full-program",
    "title": "Jupyterlab",
    "section": "Uninstall full program",
    "text": "Uninstall full program\nsudo apt-get purge package-name\nsudo apt-get autoremove"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-fonts",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-fonts",
    "title": "Jupyterlab",
    "section": "Install fonts",
    "text": "Install fonts\nsudo apt install git\ngit clone https://github.com/fzerorubigd/persian-fonts-linux.git\ncd persian-fonts-linux\n./farsifonts.sh"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#show-disk-space",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#show-disk-space",
    "title": "Jupyterlab",
    "section": "Show disk space",
    "text": "Show disk space\ndf -h"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#clean-ubuntu",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#clean-ubuntu",
    "title": "Jupyterlab",
    "section": "Clean Ubuntu",
    "text": "Clean Ubuntu\nsudo du -sh /var/cache/apt/archives\nsudo apt-get clean\ndf -h"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-packages",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-packages",
    "title": "Jupyterlab",
    "section": "Install Packages",
    "text": "Install Packages\n\nInstall miniconda\nAnaconda Prompt (miniconda3)[Open miniconda]\nconda update -n base -c defaults conda [Update Conda installed]\nconda install -c anaconda anaconda-navigator [Install Navigator]\nOpen Anaconda Navigator\nOpen terminal\nconda install -c conda-forge gdal [Install GDAL]\nconda install -c anaconda numpy\nconda install -c anaconda pandas\nconda install -c conda-forge geopandas\nconda install -c anaconda xarray\nconda install -c conda-forge matplotlib\nconda install -c conda-forge cartopy\nconda install -c conda-forge descartes [for countries plot in cartopy]\nconda install -c conda-forge shapely\nconda install -c conda-forge fiona\nconda install -c conda-forge pyproj\nconda install -c conda-forge bqplot\nconda install -c conda-forge ipyleaflet\nconda install -c conda-forge nodejs > npm install npm@latest -g [https://nodejs.org/en/]\nconda list"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-packages-in-jupyterlab",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-packages-in-jupyterlab",
    "title": "Jupyterlab",
    "section": "Install packages in Jupyterlab",
    "text": "Install packages in Jupyterlab\n\njupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-leaflet\n\n(https://ipyleaflet.readthedocs.io/en/latest/api_reference/basemaps.html)\n-jupyter labextension install @jupyter-widgets/jupyterlab-manager\n-jupyter lab build\n-jupyter nbextension enable –py widgetsnbextension –sys-prefix\n-pip install sidecar\n-jupyter labextension install @jupyter-widgets/jupyterlab-manager\n-jupyter labextension install @jupyter-widgets/jupyterlab-sidecar\n-conda install -c pyviz holoviews bokeh\n\njupyter labextension install @pyviz/jupyterlab_pyviz"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-rasterio",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#install-rasterio",
    "title": "Jupyterlab",
    "section": "Install Rasterio",
    "text": "Install Rasterio\nrasterio package must be install with gdal, from ” https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal ” yuou can find last gdal and rasterio wheel files, then run something like this from the downloads folder:\npip install GDAL-3.1.2-cp39-cp39-win_amd64.whl rasterio-1.1.5-cp39-cp39-win_amd64.whl\nconda install -c conda-forge rasterio > [with python 3.7 (rasterio works with this version) Rasterio 1.0.x works with Python versions 2.7.x and 3.5.0 through 3.7.x, and GDAL versions 1.11.x through 2.4.x. Rasterio 1.0.x is not compatible with GDAL versions 3.0.0 or greater.]\n-with the below command and after failed and failed, installed finally. conda install -c https://conda.anaconda.org/ioos rasterio"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#cartopy-or-basemap",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#cartopy-or-basemap",
    "title": "Jupyterlab",
    "section": "Cartopy or Basemap",
    "text": "Cartopy or Basemap\nBasemap is going away and being replaced with Cartopy in the near future. For this reason, new python learners are recommended to learn Cartopy. So, install cartopy.\nNOTE: DO NOT INSTALL cartopy with basemap, they are conflict."
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#for-update",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#for-update",
    "title": "Jupyterlab",
    "section": "For UPDATE",
    "text": "For UPDATE\n\npip uninstall -y setuptools\npip install setuptools\nconda update conda\nconda update -all\nconda update -n base -c defaults conda"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#run-plotly-in-jupyterlab",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#run-plotly-in-jupyterlab",
    "title": "Jupyterlab",
    "section": "Run plotly in jupyterlab",
    "text": "Run plotly in jupyterlab\nFor use in Jupyter lab, you will have to install the plotly jupyterlab extension:\njupyter labextension install jupyterlab-plotly\nOR\njupyter labextension install @jupyterlab/plotly-extension\njupyter labextension list\njupyter lab build\nThen reopen anaconda jupyterlab"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#persian-fornt",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#persian-fornt",
    "title": "Jupyterlab",
    "section": "Persian fornt",
    "text": "Persian fornt\npip install python-bidi\n-lpympl"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#markdown-formatting",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#markdown-formatting",
    "title": "Jupyterlab",
    "section": "Markdown Formatting",
    "text": "Markdown Formatting\nThe five most important concepts to format your code appropriately when using markdown are:\n\nItalics: Surround your text with ‘_’ or ‘*’\nBold: Surround your text with ’__’ or ’**’\ninline: Surround your text with ‘`’\n\nblockquote: Place ‘>’ before your text.\n\nLinks: Surround the text you want to link with ‘[]’ and place the link adjacent to the text, surrounded with ‘()’\n\n\nHeadings\nNotice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header (‘#’ is level one, ‘##’ is level two, ‘###’ is level three and ‘####’ is level four).\n# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n\nAlternatively, for H1 and H2, an underline-ish style:\n\nAlt-H1\n======\n\nAlt-H2\n------"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#h2",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#h2",
    "title": "Jupyterlab",
    "section": "H2",
    "text": "H2\n\nH3\n\nH4\n\nH5\n\nH6\nAlternatively, for H1 and H2, an underline-ish style:"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#alt-h2",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#alt-h2",
    "title": "Jupyterlab",
    "section": "Alt-H2",
    "text": "Alt-H2\n\n\nEmphasis\nEmphasis, aka italics, with *asterisks* or _underscores_.\n\nStrong emphasis, aka bold, with **asterisks** or __underscores__.\n\nCombined emphasis with **asterisks and _underscores_**.\n\nStrikethrough uses two tildes. ~~Scratch this.~~\nEmphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\n\n\nLists\nThere are three types of lists in markdown.\nOrdered list:\n\nStep 1\n\nStep 1B\n\nStep 3\n\nUnordered list\n\nCESM-POP\nCESM-MOM\nCESM-CAM\n\nTask list\n\nLearn Jupyter Notebooks\nWriting\nModes\nOther Considerations\n\nSubmit Paper\n\n\nNOTE:\nDouble click on each to see how they are built!\n\n\\(-b \\pm \\sqrt{b^2 - 4ac} \\over 2a\\) \\(x = a_0 + \\frac{1}{a_1 + \\frac{1}{a_2 + \\frac{1}{a_3 + a_4}}}\\) \\(\\forall x \\in X, \\quad \\exists y \\leq \\epsilon\\)"
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#shortcuts-and-tricks",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#shortcuts-and-tricks",
    "title": "Jupyterlab",
    "section": "Shortcuts and tricks",
    "text": "Shortcuts and tricks\n\nCommand Mode Shortcuts\nThere are a couple of useful keyboard shortcuts in Command Mode that you can leverage to make Jupyter Notebook faster to use. Remember that to switch back and forth between Command Mode and Edit Mode with Esc and Enter.\nm: Convert cell to Markdown\ny: Convert cell to Code\nD+D: Delete cell\no: Toggle between hide or show output\nShift+Arrow up/Arrow down: Selects multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc).\nShift+M: Merge selected cells.\nShift+Tab: [press once] Tells you which parameters to pass on a function\nShift+Tab: [press three times] Gives additional information on the method"
  },
  {
    "objectID": "posts/explanations/2021-07-02-ML-Resources.html",
    "href": "posts/explanations/2021-07-02-ML-Resources.html",
    "title": "ML-DL Resources",
    "section": "",
    "text": "Machine Learning Algorithms with Python, All Machine Learning Algorithms Explained with Python\nFree courses from Universities\nDust in the Machine\nData Preparation for Machine Learning (7-Day Mini-Course)\nLinear Algebra for Machine Learning (7-Day Mini-Course)\nHow to Calculate Correlation Between Variables in Python\nNeed Help Getting Started with Applied Machine Learning?\nML YouTube Courses\nStatistics and probability\nMachine Learning for Beginners\n\n11- Data Science: Machine Learning\n12- My Advice To Machine Learning Newbies After 3 Years In The Game\n13- So You Want to Do Machine Learning But Don’t Know Where to Start\n\nCourse-Zoomcamp"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html",
    "title": "Ubuntu",
    "section": "",
    "text": "Problem: In terminal I’am seeing “bash: /etc/profile.d/vte.sh: No such file or directory”\nSolution:\nhttps://askubuntu.com/questions/1283991/doubt-in-terminal\nJust look in your /etc/profile.d folder: ls -l /etc/profile.d/vte* to see if there are any files starting with vte. In my case I found two files:\n-rw-r--r-- 1 root root 1368 Jun 11  2020 /etc/profile.d/vte-2.91.sh -rw-r--r-- 1 root root  966 Jun 11  2020 /etc/profile.d/vte.csh\nIf you find the above, then you can just create a link to one of the files:\ncd /etc/profile.d sudo ln -s vte-2.91.sh ./vte.sh"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#system-program-problem-detected",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#system-program-problem-detected",
    "title": "Ubuntu",
    "section": "2- System program problem detected?",
    "text": "2- System program problem detected?\nProblem:\nI keep getting, since several days, “System program problem detected” error message\nSolution:\nhttps://askubuntu.com/questions/1160113/system-program-problem-detected\nsudo rm /var/crash/*"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#wifi-randomly-disconnected-on-ubuntu",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#wifi-randomly-disconnected-on-ubuntu",
    "title": "Ubuntu",
    "section": "3- WiFi randomly disconnected on Ubuntu",
    "text": "3- WiFi randomly disconnected on Ubuntu\nProblem:\nThe WiFi connection auto disconnected every 5 - 10 minutes but wifi signal is still fine.\nSolution:\nhttps://askubuntu.com/questions/1030653/wifi-randomly-disconnected-on-ubuntu-18-04-lts\nTry disabling wifi power management by opening /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf and changing\nwifi.powersave = 3\nto\nwifi.powersave = 2"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#install-vpn-connection-for-university",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#install-vpn-connection-for-university",
    "title": "Ubuntu",
    "section": "4- Install vpn connection for university",
    "text": "4- Install vpn connection for university\nsudo apt-get install network-manager-openvpn-gnome"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#wifi-problem-after-install-ubuntu",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#wifi-problem-after-install-ubuntu",
    "title": "Ubuntu",
    "section": "5- Wifi Problem After Install Ubuntu",
    "text": "5- Wifi Problem After Install Ubuntu\n1- After install ubuntu when wifi adaptor not working first with “USB tethering” phone and usb cable connect to internet\n2- https://askubuntu.com/questions/1306507/wifi-stopped-working-rtl8821ce-ubuntu-20-04/1306560#1306560\nthis answer is best solution."
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#battery-problem-after-install-ubuntu",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#battery-problem-after-install-ubuntu",
    "title": "Ubuntu",
    "section": "6- Battery Problem After Install Ubuntu",
    "text": "6- Battery Problem After Install Ubuntu\n1- After install new version of Ubuntu, Battery percentage shows constant number:\nsolution: Update kernel, for example, after install Ubuntu Budgie 22.10 you must update kernel from 5.9 to 6.0."
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#jupyter-home",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#jupyter-home",
    "title": "Ubuntu",
    "section": "7- Jupyter home",
    "text": "7- Jupyter home\nconda activate /home/nilik/MYPROGRAMS/miniconda3/envs/envinfo\nBudgie setting reset: nohup budgie-panel –reset –replace &\n\n\n\n\n\n\nReset Budgie from Terminal\n\n\n\n\nnvidia-smi\n\n\nhttps://www.linuxcapable.com/how-to-install-or-upgrade-nvidia-drivers-on-ubuntu-21-10-impish-indri/\n\n\n\nsuspend & wake up problem:\nGRUB_CMDLINE_LINUX_DEFAULT=“quiet scsi_mod.scan=sync”\n\n\n\n\n\n\nChange Keyboard Language\n\n\n\n\nInstall Okular from not flathub beacuse of “open new files in new tabs” works correctly.\n\n\nFor Dark theme in okular: sudo gedit /etc/environmen\n\n\nadd: QT_QPA_PLATFORMTHEME=gtk2\n\n\nthen reboot.\n\n\n\nKeyboard Layout working correctly in persian:\nIBUs Preferences –> Advances –> Keyboard Layout –> use system keyboard layout"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#install-java",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#install-java",
    "title": "Ubuntu",
    "section": "8- Install Java",
    "text": "8- Install Java"
  },
  {
    "objectID": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#libreoffice",
    "href": "posts/explanations/2022-09-30-My-Notes-Ubuntu.html#libreoffice",
    "title": "Ubuntu",
    "section": "9- Libreoffice",
    "text": "9- Libreoffice\nInstall JAVA for using zotero extention in Libreoffice\nsudo apt-get install libreoffice-java-common\nInstall Times New Roman font on Ubuntu:\nsudo apt-get --reinstall install ttf-mscorefonts-installer"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html",
    "title": "Geospatial Data Address",
    "section": "",
    "text": "NDVI SPOT\nOdiac Dataset\nOdiac - the most beautiful global map of human-made CO2 emissions\nSoilgrids\nHarmonized World Soils Database 1KM\nopengeohub\nEarth Observing Dashboard\nData Collections\nForest Fires Data Set"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#climate-data",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#climate-data",
    "title": "Geospatial Data Address",
    "section": "Climate data",
    "text": "Climate data\n\nWorldclim\nAnnual average climate data\nClimate charts\nGlobal Historical Climatology Network Monthly\nClimate Data Online Search ***\nClimate Scenario Data from the Rossby Centre\nUpperair Air Data\nMerra 2\nCopernicus\nEumetsat\nGlobal Surface Summary of the Day - GSOD ***\nIntegrated Surface Dataset (Global) ***\nNOAA Climate Data\nSurface Data Hourly Global\nNOAA Dataset Search\nIndex of 1 2\nFree access to NCEI’s archive of global coastal, oceanographic, geophysical, climate, and historical weather data ***\nRenewable point data\nESA climate office or\nTemperature and Precipitation data 1, 2, 3\nClimate Data Online: Dataset Discovery"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#downscaling-merra2-and-other-global-datasets",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#downscaling-merra2-and-other-global-datasets",
    "title": "Geospatial Data Address",
    "section": "Downscaling MERRA2 and other Global Datasets",
    "text": "Downscaling MERRA2 and other Global Datasets\n\nThe Land surface Data Toolkit (LDT v7.2) – a data fusion environment for land data assimilation systems 1 2 3\nGlobSim: downscaling global reanalysis /// GlobSim (v1.0): deriving meteorological time series for point locations from multiple global reanalyses 1 2"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#satellite-data",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#satellite-data",
    "title": "Geospatial Data Address",
    "section": "Satellite data",
    "text": "Satellite data\n\nAppEEARS\nLPDAAC Dataset\nDatasets for deep learning with satellite & aerial imagery\nVIIRS Plus DMSP Change in Lights (VIIRS+DMSP dLIGHT), v1 (1992, 2002, 2013)\nEarthexplorer\nSearch Earthdata\nLANCE: NASA Near Real-Time Data and Imagery\nHazards and Disasters\nActive Fire Data\nFire Information for Resource Management System (FIRMS)\nWorldview Earthdata\nWorldview Snapshots\nGlovis\nNasa Earth Observations\nEarthNow! Landsat Image Viewer 1, 2\nSAR Data\n9 Best Free Land Cover/Land Use Data\nTerraScope - Free resource for sentinel satellites data\nAssessment of a Future Copernicus Earth Observation Service Component to Support Sustainable Forest Monitoring\nCopernicus Biomass\nGlobal Forest Watch map 1 2 3\nSPOT satellites (1, 2, 3, 4 and 5 archive)"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#sentinel-data",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#sentinel-data",
    "title": "Geospatial Data Address",
    "section": "Sentinel Data",
    "text": "Sentinel Data\n\nDashboard\nopenEO develops an open API to connect R, Python, JavaScript and other clients to big Earth observation cloud back-ends in a simple and unified way\nSentinell Hub\nSentinel5P\nVito Dataset\nTerraScope - Free resource for sentinel satellites data\nAssessment of a Future Copernicus Earth Observation Service Component to Support Sustainable Forest Monitoring\nCopernicus Biomass\nSea Surface Temperature\nSoil Moisture\nWater Vapour\nAerosol\nFire\nGreenhouse Gases (GHGs)\nHigh Resolution Land Cover\nLand Cover\nLakes\nLand Surface Temperature\nOcean Colour\nOzone\nRECCAP-2 supports and accelerates the analysis of regional carbon budgets\nSea State\nSea Surface Salinity\nGlobal Forest Watch map 1 2 3"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#terrian",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#terrian",
    "title": "Geospatial Data Address",
    "section": "Terrian",
    "text": "Terrian\n\nDem 1km\nlpdaac.usgs.gov\nThe Global Multi-Resolution Topography (GMRT) Synthesis, Bathymetry and Dem\nOpenTopography High-Resolution Topography Data and Tools\nSRTM data\nACE2 (Altimeter Corrected Elevations) DEM is a global DEM at 3” (approx 90m at the equator), 9”, 30’ (approx 1km at the equator) and 5’ resolution\nACE - Altimeter Corrected Elevations\nDSM ALOS Global Digital Surface Model “ALOS World 3D - 30m\nDTM FABDEM\nDSM and DTM can be used in 3D building in QGIS with Mapflow plugin"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#hydrology",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#hydrology",
    "title": "Geospatial Data Address",
    "section": "Hydrology",
    "text": "Hydrology\n\nBasins, Rivers, Drainages and etc."
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#human",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#human",
    "title": "Geospatial Data Address",
    "section": "Human",
    "text": "Human\n\nAnthropogenic Biomes 2000\nGlobal 1-km Downscaled Population Base Year and Projection Grids Based on the SSPs, v1.01 (2000 – 2100)\nGlobal Rural-Urban Mapping Project (GRUMP)\nSocioeconomic Data and Applications Center (sedac)"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#sea",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#sea",
    "title": "Geospatial Data Address",
    "section": "Sea",
    "text": "Sea\n\nOpenTopography High-Resolution Topography Data and Tools\nThe PO.DAAC: An Open Ocean of Remote Sensing and In Situ Data for Science in the Cloud\nOcean Dataset\nERDDAP oceanographic data\nFind and download data for your coastal management needs\nDigital Coast to Get the Data, Tools\nNOAA Class\nBathymetry data sets for the world’s oceans"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#github",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#github",
    "title": "Geospatial Data Address",
    "section": "Github",
    "text": "Github\n\nGlobal Map data archives\nDownload MERRA 2 Data 1 2"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#international-organisations",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#international-organisations",
    "title": "Geospatial Data Address",
    "section": "International Organisations",
    "text": "International Organisations\n\nFao Geonetwork\nUNEP Environmental Data Explorer\nIntegrated Population and Environmental Data\nNatural Earthdata, Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales\nGeoportal\nGoogle Dataset Search\nASDC Tools and Services"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#atmosphere",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#atmosphere",
    "title": "Geospatial Data Address",
    "section": "Atmosphere",
    "text": "Atmosphere\n\nAtmospheric Science Data Center\nCopernicus"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#nasa-data",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#nasa-data",
    "title": "Geospatial Data Address",
    "section": "NASA Data",
    "text": "NASA Data\n\nGiovanni\nGES DISC"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#soil",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#soil",
    "title": "Geospatial Data Address",
    "section": "Soil",
    "text": "Soil\n\nGlobal maps for Soil Hydraulic Properties"
  },
  {
    "objectID": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#articles",
    "href": "posts/geospatial/2021-03-25-address_urls_geodatabase.html#articles",
    "title": "Geospatial Data Address",
    "section": "Articles",
    "text": "Articles\n\nAccessing and Downloading Training Data on the Radiant MLHub API\nHow to get started with GRIB2 Weather Data and Python"
  },
  {
    "objectID": "posts/geospatial/2021-07-03-Learning-GIS-RS.html",
    "href": "posts/geospatial/2021-07-03-Learning-GIS-RS.html",
    "title": "GIS & RS learning and notes",
    "section": "",
    "text": "25 Map Types: Brilliant Ideas to Build Unbeatable Maps\nGeojson\nIntroduction to QGIS (Full Course Material)\nGeographic Information Systems Data Pathfinder"
  },
  {
    "objectID": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#rs",
    "href": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#rs",
    "title": "GIS & RS learning and notes",
    "section": "RS",
    "text": "RS\n\nEMIT-Data-Resources\nSpectral Indices\nObserving Earth From Space\nORNL DAAC\nEducational resource for exploring satellite images\nE-Learning LP DAAC\nMethane Emissions from Dairy Sources\nMonitor Vegetation with Google Earth Engine\nMonitor Regional Climate with Google Earth Engine\nFree online courses, tutorials and tools\nSummary statistics of geospatial raster datasets based on vector geometries\nEnhancement of MODIS NIDVI to 10m resolution using U-Net\nBeyond the Visible – Introduction to Hyperspectral Remote Sensing\nORNL DAAC Releases GEDI Level 4B Dataset Offering Gridded Estimates of Aboveground Biomass Density"
  },
  {
    "objectID": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#gee",
    "href": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#gee",
    "title": "GIS & RS learning and notes",
    "section": "GEE",
    "text": "GEE\n\nqgis-earthengine-examples\nRemote Sensing\nLand Cover Classification\nWXEE\nTraining Data Collection Using Google Earth Engine"
  },
  {
    "objectID": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#python",
    "href": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#python",
    "title": "GIS & RS learning and notes",
    "section": "Python",
    "text": "Python\n\nA Python package for geospatial analysis and interactive mapping in a Jupyter environment\nAwesome-EarthObservation-Code 1 2\nUsing Pandas and Python to Explore Your Dataset\nDownloading YouTube Videos\nCreate Beautiful Tkinter GUIs by Drag and Drop\nA simple way of creating movies from xarray objects"
  },
  {
    "objectID": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#articles",
    "href": "posts/geospatial/2021-07-03-Learning-GIS-RS.html#articles",
    "title": "GIS & RS learning and notes",
    "section": "Articles",
    "text": "Articles\n\nWhat Is NetCDF Data and Why Is It Interesting?\nVisualization in Python —Visualizing Geospatial Data\nDetecting deforestation from satellite images -How to get started with GRIB2 Weather Data and Python\nSpatial Data Science: Spatial Data Storage\nAll open source Python GIS and Earth Observation libraries\nGeoWombat: Utilities for geospatial data\nExploring Google Earth Engine Platform for Big Data Processing: Classification of Multi-Temporal Satellite Imagery for Crop Mapping\nMapping Fires using Google Earth Engine and Python"
  },
  {
    "objectID": "posts/scientific_research/Untitled.html",
    "href": "posts/scientific_research/Untitled.html",
    "title": "Envinformatics",
    "section": "",
    "text": "VOSviewer"
  },
  {
    "objectID": "posts/data_science/2023-03-23-TopoPyScale.html#important-note",
    "href": "posts/data_science/2023-03-23-TopoPyScale.html#important-note",
    "title": "TopoPyScale",
    "section": "Important Note:",
    "text": "Important Note:\nIn input folder only surf and PLEV files must be coresponding to start date and end date in config file, otherwise below code encounter with memory error.\n\n# ========= STEP 4 ==========\n# Perform the downscaling\nmp.downscale_climate()\n\n\n# ========= STEP 5 ==========\n# Export output to desired format\nmp.to_netcdf()\n\n\n7- Convert Hourly to Yearly data\n\nimport xarray as xr\n\nds_down_dsk = xr.open_dataset(\n    \"./Caracal/Topopyscale_prj_khaeiz/outputs/output.nc\",\n    chunks={\"point_id\": 10}\n)\nds_down_dsk\n\n\nds_year = ds_down_dsk.groupby('time.year').mean('time')\n\n\n\n8- Adding Coordinate dimention to results\n\nds_param = xr.open_dataset(\"./Caracal/Topopyscale_prj_khaeiz/outputs/ds_param.nc\")\nds_param\n\n\nds_khaeiz = ds_year.sel(point_id=ds_param.cluster_labels)\n\n\n\n9- Saving reult to netcdf file\n\ndef compute_scaling_and_offset(da, n=16):\n    \"\"\"\n    Compute offset and scale factor for int conversion\n\n    Args:\n        da (dataarray): of a given variable\n        n (int): number of digits to account for\n    \"\"\"\n    vmin = float(da.min().values)\n    vmax = float(da.max().values)\n\n    # stretch/compress data to the available packed range\n    scale_factor = (vmax - vmin) / (2 ** n - 1)\n    # translate the range to be symmetric about zero\n    add_offset = vmin + 2 ** (n - 1) * scale_factor\n\n    return scale_factor, add_offset\n\ndef to_netcdf(ds, fname='output.nc', variables=None):\n        \"\"\"\n        Generic function to save a datatset to one single compressed netcdf file\n\n        Args:\n            fname (str): name of export file\n            variables (list str): list of variable to export. Default exports all variables\n        \"\"\"\n\n        encod_dict = {}\n        if variables is None:\n            variables = list(ds.keys())\n\n        for var in variables:\n            scale_factor, add_offset = compute_scaling_and_offset(ds[var], n=10)\n            if str(ds[var].dtype)[:3] == 'int':\n                encod_dict.update({var:{\n                                   'dtype':ds[var].dtype}})\n            else:\n                encod_dict.update({var:{\"zlib\": True,\n                                       \"complevel\": 9,\n                                       'dtype':'int16',\n                                       'scale_factor':scale_factor,\n                                       'add_offset':add_offset}})\n        ds[variables].to_netcdf(fname, encoding=encod_dict, engine='h5netcdf')\n\n        print(f'---> File {fname} saved')\n\n\nto_netcdf(ds_khaeiz, fname='downscaled_nc.nc', variables=None)\n\n\n\n10- Extract Temperature from dataframe\n\nds_t = ds_khaeiz.t\nds_t.plot()\n\n\n\n11- Save a variable to geotif file\n\nimport xarray\nimport rioxarray\nfrom pyproj import CRS\n\nxds = xarray.open_dataset(\"downscaled_nc.nc\")\n\nbT = xds['t']\nbT = bT.rio.set_spatial_dims(x_dim='x', y_dim='y')\nbT.rio.crs\n\n# Define the CRS projection\nbT.rio.write_crs(\"epsg:32639\", inplace=True)\n\n\n# save the GeoTIFF file: \nbT.rio.to_raster(r\"temp_raster.tiff\")"
  },
  {
    "objectID": "posts/chatgpt/2023-05-23-ChatGPT.html",
    "href": "posts/chatgpt/2023-05-23-ChatGPT.html",
    "title": "ChatGPT + Python",
    "section": "",
    "text": "Ask questions about your documents without an internet connection, using the power of LLMs. 100% private, no data leaves your execution environment at any point. You can ingest documents and ask questions without an internet connection(Link)."
  },
  {
    "objectID": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#githubs",
    "href": "posts/explanations/2021-06-07-My-Notes-Jupyterlab.html#githubs",
    "title": "Jupyterlab",
    "section": "Githubs",
    "text": "Githubs\n\nMath support in Markdown\nNotebooker"
  },
  {
    "objectID": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-hugging-face",
    "href": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-hugging-face",
    "title": "ChatGPT + Python",
    "section": "Use ChatGPT-4 for free on Hugging Face",
    "text": "Use ChatGPT-4 for free on Hugging Face\nGitHub’s AI community ‘Hugging Face’ has introduced a free Chat GPT 4 chatbot for free. It will let you have the benefit of getting your queries answered without using an API key. However, owing to excess traffic on the site, you might have to wait in the queue or even wait for minutes to get the response. Follow the steps below to access Chat GPT 4 for free through Hugging Face.\nStep 1: Visit the site using https://huggingface.co/spaces/ysharma/ChatGPT4"
  },
  {
    "objectID": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-ora.sh",
    "href": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-ora.sh",
    "title": "ChatGPT + Python",
    "section": "Use ChatGPT-4 for free on Ora.sh",
    "text": "Use ChatGPT-4 for free on Ora.sh\nYou thought there were only 2 hacks? Well, there’s more to come. The next hack is through the web platform called Ora..sh which is used to quickly build LLM apps in a shareable chat interface. Through this web platform, you can use ChatGPT-4 for free and there’s no message limit here. Unlike Hugging Face, there’s no queue or waiting time, so you can use this without any problem.\nOpen the web platform Ora.sh here: https://ora.sh/openai/gpt4"
  },
  {
    "objectID": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-nat.dev",
    "href": "posts/chatgpt/2023-05-23-ChatGPT.html#use-chatgpt-4-for-free-on-nat.dev",
    "title": "ChatGPT + Python",
    "section": "Use ChatGPT 4 for Free on Nat.dev",
    "text": "Use ChatGPT 4 for Free on Nat.dev"
  },
  {
    "objectID": "posts/chatgpt/2023-06-01-PrivateGPT_Streamlit.html",
    "href": "posts/chatgpt/2023-06-01-PrivateGPT_Streamlit.html",
    "title": "PrivateGPT + Streamlit",
    "section": "",
    "text": "This article contains step by step building a FastAPI backend and Streamlit app for PrivateGPT. The PrivateGPT App provides an interface to privateGPT, with options to embed and retrieve documents using a language model and an embeddings-based retrieval system. All data remains local(src).\n1- Make a new env in the Linux terminal:\nmicromamba create -n privategpt_app\n2- Active new env and install jupyetrlab:\nmicromamba activate privategpt_app\npip install jupyterlab\n3- Open jupyterlab\njupyter lab\n4- In jupyterlab create a new folder privateGPT and open the terminal of jupyterlab from this folder.\n5- Clone privategpt from GitHub:\ngit clone https://github.com/menloparklab/privateGPT-app\n6- In the terminal get privateGPT-app:\ncd privateGPT-app\n7- Install the required dependencies:\npip install -r requirements.txt\n8- Open the privateGPT-app folder directly from My Computer and rename example.env to .env\n9- Create a new models folder inside the privateGPT-app folder.\n10- Download the LLM model and place it in the models directory:\nLLM: default to ggml-gpt4all-j-v1.3-groovy.bin. If you prefer a different GPT4All-J compatible model, just download it and reference it in your .env file.\n11- Because we added the model file in the previous step, so for preventing the download while running the code, we must change some text in the app.py.\nChange from:\n# Create the folder if it doesn't exist\nos.makedirs(folder, exist_ok=True)\n# Run wget command to download the file\nos.system(f\"wget {url} -O {filename}\")\nto:\n# Create the folder if it doesn't exist\n#os.makedirs(folder, exist_ok=True)\n# Run wget command to download the file\n#os.system(f\"wget {url} -O {filename}\")\n12- Install python3 as python in jupyterlab terminal using (important):\nsudo apt install python-is-python3\n13- To run the FastAPI backend, execute the following command in jupyterlab terminal:\ngunicorn app:app -k uvicorn.workers.UvicornWorker --timeout 1500\n14- Open new terminal in privategpt_app env and then run the Streamlit app, and use the following command:\nstreamlit run streamlit_app.py --server.address localhost\n15- For new document analysis it is better to shutdown server by Ctrl + C and repeat 13 and 14 steps, respectively.\n\nImportant Note:\nName of PDF file(s) must be without any spaces, so before uploading PDF file you must rename PDF file by removing spaces in file name.\nSources:\n1- https://github.com/menloparklab/privateGPT-app\n2- video: Easiest way to deploy privateGPT app Ideal for Businesses Organizations.mp4"
  },
  {
    "objectID": "posts/scientific_research/2023-07-09-Zettelkasten.html",
    "href": "posts/scientific_research/2023-07-09-Zettelkasten.html",
    "title": "Second Brain",
    "section": "",
    "text": "Implementing Zettelkasten workflow in reading, writing, note-taking, research method, creativity, brainstorming, idea generator, etc., using Zotero as a reference manager, and Obsidian as a note-taking app for references downloaded in Zotero. There is a whole religion around note-taking with concepts such as Second Brain, Personal Knowledge Management (PKM), and Zettelkasten.\nI find that a bit too much but I enjoy the software, it saves everything in clean .md files in your file system (makes it future-proof, you can change software later on and still have all your notes) and supports wiki-style linking between notes. I use Zotero simply because it is the best reference manager I have found, it is also free and open source if you are into that.\nMy workflow is such that I start by reading an article in my Zotero library, and make highlights and notes directly on the PDF in the app. I do this on the official app, phone, or computer. I then import these highlights and notes into obsidian for storage and further processing (src).\nThere are four parts to the zettelkasten method, and I summarize them here.\n1- Capture\n2- Organize\n3- Connect\n4- Emergence\nCapture\nCapturing is simple. There’s only one step\n- Jot it down\nUnformed ideas and potentially useful knowledge and information float around us all day long. So by capturing the idea, you save it for a later time when you can sit down, think it through, and see how it fits with the rest of your life.\nOrganize\nOrganizing is the second step of the zettelkasten method. It is when you take the time to analyze the things you captured. Many of those captures will need to be deleted, they seemed cool at the time, but now you see why they don’t work or why you don’t need them. By deleting these unneeded or useless work notes, notes about your favorite hobbies, including notes in class that you don’t need, you’ll keep your zettelkasten digital note-taking application lean. When you find things you want to keep, expand them, make them more concise, and rewrite the ideas until you’re happy with them. If a note contains many ideas, split them into individual notes. These are called atomic notes, where each note contains just one idea.\nConnect\nConnecting your notes is a vital part of the zettelkasten system. It’s where you take all those atoms and start to build larger theories from them. Luhmann did this by writing ID codes on his index cards, but using Obsidian, it will be much quicker and easier.\nEvery note you make should be linked to another note. Obsidian plugins can help you. For example, by finding orphaned notes with no links, you can add links to them and bring them back into your web of ideas.\nEmergence\nEmergence is the final stage of the zettelkasten system. It’s where you start to see new ideas emerge from the connections you have made.\nUnrelated ideas that you would have never put together may only have one note between them, so when you browse your notes and link on the links, you suddenly see the connection, and a new idea for a book or article starts to emerge.\n1- Install Zotero. 2- Install [Better BibTeX for Zotero] plugin (https://github.com/retorquere/zotero-better-bibtex) 3- Make sure you have selected a quick copy style. In Zotero open preferences > export and under “quick copy,” select a suitable citation style. 4- Test quick copy style by opening an article in zotero, choose edit -> Copy Bibliography, and try to paste it somewhere. 5- Read and highlight the text, add notes, or select an area on desired paper in Zotero. 6-0- In Obsidian for creating a new Vault in the desired path, in the bottom left side of Obsidian using “open another vault” you can create a new vault with your new path.\nVault is the top-level folder; everything you do in Obsidian will be inside this folder on your hard drive. Of course, you can create folders, save files and create notes within your vault. I’d recommend you open your file explorer and decide where your vault will be, then create a folder called “Vault” there.\nCreate These Folders\n- Literature\n- Fleeting\n- Zettelkasten (optional)\n- Templates\nLiterature: If you intend to take notes while reading, you will want to keep these separate from your zettels. Within this folder, you can arrange your literature notes any way you like. For example, you can create a subfolder for each book and keep notes within it or create one note for each book titled with the book name and author.\nFleeting Notes for Unformed Ideas: Another folder you should create is a fleeting-note or unformed ideas folder. Again, this is for exactly what you think; you can write down ideas that you haven’t yet thought through and save them in this folder for revision and review later.\nZettelkasten Folder: You don’t need to keep your zettels in a folder. They can just live in the vault’s main folder. This is where they will appear by default. But if you want to keep everything a little neater and under control, this folder is a good idea.\n-Templates Folder؛ Templates are an easy addition to make good notes in Obsidian. By creating a template with the structure or format you want to use and saving it in this folder, you can ensure they will be correct every time you make a note. To use templates, you must enable them in the Settings menu. Ctrl + T will then give you access.\n6- In Obsidian open preferences (bottom left in the main Obsidian window), under Options select Community plugins, and click browse (if this is the first community plugin you install, you will have to turn off “restricted mode.”). 7- Install Install Zotero Integration in Obsidian and then enable it. 8- Download PDF Utility of Zotero Integration in Obsidian, under Database, choose Zotero, then under Import formats, choose add import format.\n9-0- Create Your Templates Templates will save you time and reduce errors while maintaining the structure of your notes system. 9-1- How to Set Up Templates:\n\n\n\nalternative text\n\n\n9-2- Create a Templates Folder As templates are not notes in themselves but only the skeleton of a note. Create a new folder and call it “Templates.” 9-3- Turn Templates On\n- Click the Settings cog\n- Select Core Plugins\n- Scroll Down or Search for Templates\n- Enable Templates if not already Enabled\n- Click on the Template Options cog\n- Enter your Templates folder\n- Setup your Date and Time formats\n9-4- Create a template to test:\n1- Create a new note for your template items, title it “Test” and add the following text\nTitle: {{title}}\nDate: {{date}}\nTime: {{time}}\nHello World! This is my first template.\n2- Move the template note to your templates folder\n3- Now create a new note and title it anything you like\n4- With the new note created in stage 3 open, using Ctrl+P (open command pallete) type \"Templates: insert template\" snd select \"Test\" template.\n9-5- Applying Templates steps:\nTo use any of the templates below, follow these instructions.\n- Create a new note (Ctrl N) and call it anything you like\n- Paste the template text as shown below\n- Customize the template any way you wish\n- Move it to your template folder (Ctrl M)\n- Create a note\n- Hit Ctrl P & Select the template you just made\n9-6- Zotero Template Download “ZoteroNote” Template for Obsidian from this site.  Note: You have to tell Obsidian what to import from Zotero and how to structure the new note that it will create, by default obsidian will import nothing. To do this, you provide Obsidian with a template. You also have to provide where your Obsidian vault files will be located.\nThe template file has to be written in Nunjucks templating language. If you are familiar with this you can write your own template file. Otherwise, you can search the web for other users templates, but please be aware that the Zotero Integration plugin was previously named something else and used a different template system.\nYou can also download template from this site and use that. It is pretty basic, but I am sure if you give it some thought, you can adapt it to your liking. I personally find it cumbersome to edit templates in Obsidian and use another text editor (BBEdit) for this purpose, but that is up to you. Save the template file as a .md file somewhere in your vault.\n10- In “import Formats” section of Zotero Integraion in Obsidian choose a name such as “Zotero Note” 11- Choose an output path for your future notes. This is how your new notes will be named and where in your vault they will be saved.\nIn my study it must be “/Literature/{{citekey}}.md”\nThe citekey variable i suitable since it will create an unique and readable name for each article/note. I have chosen to put my new notes in a folder named Zotero but if you prefer them at top level, just write {{citekey}}.md. 12- Choose where to save imported images and how to name them. such as “/Literature/images/{{citekey}}/”  13- Add the path to your template file, mine is called ZoteroNote.md and is located in folder called Templates which is located in Templates folder It means that in “Template File” section type “ZoteroNote.md” and then select “Templates/ZoteroNote.md”. 14- Choose a bibliography style.\n\n\n\nalternative text\n\n\n15- For importing a note from Zotero, make sure Zotero is open and that you have an annotated article in your library. 16- In Obsidian use the keyboard shortcut Ctrl+P to open the command palette. Search for the name of your import format (“Zotero note” in my case). Press enter. This will open a search bar in Zotero. Search for your article in your library and press enter."
  },
  {
    "objectID": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html",
    "href": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html",
    "title": "Envinformatics",
    "section": "",
    "text": "---\ntitle: \"Using Obsidian + Zotero for Scientific Research \"\ndescription: \"Tools and workflows for managing your zettelkasten, projects, reading lists, notes, and inspiration during your research.\"\nauthor: \"Me\"\ndate: \"2023-07-12\"\ncategories: [Zettelkasten, Zotero, Obsidian]\nimage: images/zotero_obsidian.png\nskip_exec: true\nskip_showdoc: true\nformat:\n  html:\n    code-fold: true\n    self-contained: true\n---\nThis toturial isbased on below sources: 1- How to Boost Your Productivity for Scientific Research Using Obsidian 2- Obsidian Tutorial for Academic Writing\nKnowledge is the most powerful tool you have as a researcher. Knowledge, however, is worthless if it cannot be accessed quickly and effectively. The key is in a consistent and easy-to-use method of archiving information so that taking notes becomes an effortless and pleasing experience.\nThe article is divided into three parts:\nPhilosophy:\nTools:\nWorkflows:\nThroughout the article I will use the word library to mean your entire digital knowledge (library), which is used interchangeably with “Obsidian Vault” or “Zettelkasten”."
  },
  {
    "objectID": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#philosophy-how-to-take-and-organize-notes-effectively",
    "href": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#philosophy-how-to-take-and-organize-notes-effectively",
    "title": "Envinformatics",
    "section": "1. Philosophy: How to take and organize notes effectively",
    "text": "1. Philosophy: How to take and organize notes effectively\nBefore we dive into Obsidian, it’d be good to first lay out the foundations of a Personal Knowledge Management System and Zettelkasten. The inspiration for this section and a lot of the work on Zettelkasten is from Niklas Luhmann, an outstandingly productive sociologist who wrote 400 papers and 70 books. Zettelkasten means (literally) “slip box” (or library in this article). In his case, his Zettlekasten had around 90000 physical notes, which have been digitized. Nowadays, there are loads of tools available to make this process easier and more intuitive. Obsidian, specifically has a good introduction section on their website: https://publish.obsidian.md/hub/\nNotes\nWe will start by considering “What is a note?”. Although it seems like a trivial question, the answer to this may vary depending on the topic or your style of notes. The idea, however, is that a note is as “atomic” (ie. self-contained) as possible. You should be reading the note and understand the idea immediately.\nThe resolution of your notes depends on how much detail you have for that note. For example, a note about “Deep Learning” could be just a general description of what Neural Networks are, and maybe a few notes on the different types of architectures (eg. Recurrent Neural Networks, Convolutional Neural Networks, etc..).\nA good rule of thumb to have is to limit length and detail. If you require more detail, in a specific section of this note, it would make sense to break it up into several smaller notes. So, from our original note “Deep Learning” we now have three notes:\nDeep Learning\nRecurrent Neural Networks\nConvolutional Neural Networks\n#Tags and [[Links]] over /Folders/\nFor example, Deep Learning has been used for Protein Structure prediction (AlphaFold) and image classification (ImageNet). Now, if you had a folder structure like this:\n- /Proteins/\n   - Protein Folding\n- /Deep Learning/\n   - Convolutional Neural Networks\nYour notes about Protein Folding and Convolutional Neural Networks will be independent and when you are in the “Protein” folder, you won’t be able to find notes about Neural Networks.\nThere are several ways to solve this problem. The most common one is to use tags rather than folders. This way, one note can be grouped with more than just one topic. Tags can also be nested (ie. have subtags) in Obsidian.\nAlso, you can link two notes together with links. Obsidian and some other note-taking apps let you connect one note to another so that you can then jump to that note and build your “Knowledge Graph” as shown below:\n### When to use Folders\nFolders are however useful to organize your vault, especially as it grows. The main advice here is to have very few folders, as they should “weakly” collect groups of notes or better collect different types or sources of notes.\nThey generally collect different sources of information. For example, these are the folders I use in my Zettelkasten:\n\n\n\nalternative text\n\n\nMOC: Contains all the Maps of Contents to navigate the Zettelkasten. Projects: Contains one note for each side-project of my PhD where I log my progress and ideas. These are also linked to notes. Bio and ML: These two are essentially the main content of my Zettelkasten and they could in theory be fused into one folder. Papers: Here I place all the notes I take from scientific papers. The notes are synced using a bibliography .bib file from Zotero. Books: I write a note for each book I read and generally split them into multiple notes after I go through them.\nHaving a separate folder for images can also be a good idea, to avoid cluttering your main folders with image files.\nI will discuss these more in detail in the Workflow Section.\nMy general suggestion for folders is to minimize them as much as possible and to use tags and links instead.\nMaps of Content (MOC)\nAs you start growing your Zettelkasten, you might find it hard to find notes, especially when taking notes of different topics. A good solution to this is to create notes called Maps of Contents (MOCs).\nThese are notes that “signposts” your Zettelkasten library, directing you to the right type of notes. Inside of it you can link to other notes based on tags of a common topic. Usually this is done with a title, followed by your notes that relate to that title. This is an example:\n\n\n\nalternative text\n\n\nAs shown above, my Machine Learning MOC starts with the basics, all in one section. It then moves to Variational Auto-Encoders and Transformers. This allows you to group and quickly find all notes related to a tag without having to scroll through the tag search section.\nThis is why I keep MOCs at the top of my library, as I can quickly find the information I need and get a quick look at my library. These MOCs are automatically generated using an Obsidian Plugin called Dataview https://github.com/blacksmithgu/obsidian-dataview which works much like SQL queries.\nIdeally, MOCs can be expanded and should have a bit more explanations about the notes, their status, and what you still need to do. In the absence of this, Dataview does a fantastic job at creating a good structure for your notes.\nEDIT: This is the template I use for the screenshot above:\nDataview query for MOC (code)\n### Basics\n```dataview\ntable tags as \"Tags\"\nfrom \"2. Notes\"\nWHERE contains(tags, \"optimisation\")\nSORT file.name ASC\n```\n\n### VAE\n```dataview\ntable tags as \"Tags\"\nfrom \"2. Notes\"\nWHERE contains(tags, \"vae\")\nSORT file.name ASC\n```\n\n### Transformers\n```dataview\ntable tags as \"Tags\"\nfrom \"2. Notes\"\nWHERE contains(tags, \"attention\")\nSORT file.name ASC\n```\nAlternatively, this is what a book tracker looks like:\nDataview query for books in folder “4. Books”\n\n\n\nalternative text\n\n\n# Books MOC\n```dataview\ntable author as Author, date as \"Date Finished\", tags as \"Tags\", grade as \"Grade\"\nfrom \"4. Books\"\nSORT grade DESCENDING\n```\nWhere each book note looks like this:\nBook note with fields.\n\n\n\nalternative text\n\n\n### [Books MOC](../0.%20MOC/Books%20MOC.md)\n# Social Engineering : The Art of Human Hacking\n**Author**:: Chris Hadnagy\n**Date**:: 2022-01-01\n**Tags**:: #psychology/socialengineering #psychology\n**Grade**::  9/10\n**Links**::  \n\n## Gathering Information\n\n- **Example**: \"Can you tell me one person who you think could benefit from this product as much as you will?\"\n- As a salesperson you can now call people use that information to be like \"Your friend bought XYZ premium / upfront\""
  },
  {
    "objectID": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#tools-getting-to-know-obsidian",
    "href": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#tools-getting-to-know-obsidian",
    "title": "Envinformatics",
    "section": "2. Tools: Getting to know Obsidian",
    "text": "2. Tools: Getting to know Obsidian\nObsidian is my tool of choice as it is free, all the notes are stored in Markdown format, it can be customized/themed, and each panel can be moved around in drag and drop fashion. You can download it here: https://obsidian.md/.\nInterface\nAs I mentioned, Obsidian is very customizable, so I found this to be my optimal interface:\n\n\n\nalternative text\n\n\nMy interface in Obsidian. The theme is customized from https://github.com/colineckert/obsidian-things\nIf you want something simpler, each panel can be collapsed, moved, or removed in whatever way you wish. If you need to find a panel later on, you can click on the vertical “…” (bottom left of the note panel), and open the relevant panel.\nGenerally my interface is organized as such:\n\n\n\nText\n\n\nHow you can organize your Obsidian Interface.\nFolders / Search: Here I have all the relevant folders. I usually use the MOC note to get to wherever I want, otherwise, I use the search button to look for a note. Tags: I use nested tags and usually look into each of them if I am looking for specific notes to link. cMenu: Nice plugin to place useful functionality in a handy menu (https://github.com/chetachiezikeuzor/cMenu-Plugin) Global Graph: The graph shows all your notes (linked and unlinked). Linked notes will appear closer together. You can zoom in to read the title of each note. This can be a bit overwhelming at first, however, as your library grows, you get used to the positions and start thinking of possible connections between notes that you may not have thought about. Local Graph: This will show your current note, in relation to other linked notes in your library. It is useful to quickly jump to another link when you need to, and go back to the current note. Links: Finally, here I keep all the linked mentions of the notes, as well as an outline panel and the plugin Power Search (https://github.com/aviral-batra/obsidian-power-search), which allows me to search my vault by highlighting some text.\nPlugins\nAnother major advantage of using Obsidian is the vast choice of plugins. I use many but here are a few of the ones I use the most (Calendar, Citations, Dataview, Templater, Admonition):\nCalendar: https://github.com/liamcain/obsidian-calendar-plugin Gives you a calendar to organize your notes. This is optimal for taking notes from meetings or keeping a journal.\nCitations: https://github.com/hans/obsidian-citation-plugin Allows you to cite papers from a .bib file to include in your notes. You can also customize how your notes will be produced (eg. Title, Authors, Abstract, etc..)\nDataview: https://github.com/blacksmithgu/obsidian-dataview Probably among the most powerful plugins as it allows you to query your library as a database and automatically generate content. You can see an example in the MOC section.\nTemplater: https://github.com/SilentVoid13/Templater Allows you to create notes with specific templates like dates, tags, and headings.\nAdmonition: https://github.com/valentine195/obsidian-admonition Allows you to structure your notes with blocks.\nTheme (new addition)\nMany have asked about my theme settings and CSS.\nMy CSS:\n- Adds white background to all the images (which allows me to add transparent images and see them properly in dark mode)\n- Leave a 40px of space between the title bar and the content\n- Increase the font for LaTeX formulae\n\n.markdown-source-view.mod-cm6 .cm-scroller, .markdown-source-view, .cm-s-obsidian, .cm-s-obsidian span.cm-formatting-task{\nmargin-top: 40px !important;\n}\n\n.workspace-leaf.mod-active .view-header {\n    background: linear-gradient(\n120deg\n, var(--color-view-header-gradient-2) 0.5%, var(--color-view-header-gradient-2) 0.5%);\n}\n\nimg{ background: white;}\n\nmjx-math {\n  font-size: 150% !important;\n}\nview raw\nsrc\nMy settings for the Things theme add additional colors to the headings:\n{\n  \"blue-topaz-theme@@color-scheme-options\": \"color-scheme-options-default\",\n  \"blue-topaz-theme@@layout-style-options\": \"layout-style-options-default\",\n  \"blue-topaz-theme@@font-size-folder-and-file\": \"0.90em\",\n  \"blue-topaz-theme@@h1-color@@dark\": \"#C86E78\",\n  \"blue-topaz-theme@@h2-color@@dark\": \"#C88974\",\n  \"blue-topaz-theme@@font-family-vault\": \"monaco\",\n  \"blue-topaz-theme@@translucent-setting-panel\": false,\n  \"blue-topaz-theme@@blur-depth\": 26,\n  \"blue-topaz-theme@@background-settings-workplace-background-image\": false,\n  \"blue-topaz-theme@@clutter-free-headings\": true,\n  \"blue-topaz-theme@@background-image-settings-switch\": true,\n  \"blue-topaz-theme@@bt-status-off\": false,\n  \"blue-topaz-theme@@non-border-bottom-header\": false,\n  \"blue-topaz-theme@@circular-checkbox\": false,\n  \"blue-topaz-theme@@muted-activeline-bg\": false,\n  \"blue-topaz-theme@@rainbow-lines\": false,\n  \"blue-topaz-theme@@light-background-color-files\": false,\n  \"blue-topaz-theme@@folder-style-change-options-colorful\": false,\n  \"blue-topaz-theme@@folder-style-change-options-colorful-subfolder\": \"folder-colorful-one\",\n  \"blue-topaz-theme@@font-family-change-to-default\": true,\n  \"things-style@@minimal-icons-off\": true,\n  \"things-style@@full-file-names\": false,\n  \"things-style@@font-small\": 15,\n  \"things-style@@cursor\": \"default\",\n  \"things-style@@macOS-translucent\": false,\n  \"things-style@@h3-color\": \"#B87CDF\",\n  \"things-style@@h2-color\": \"#4B9DC5\",\n  \"things-style@@em-color\": \"#E5B567\",\n  \"things-style@@h1\": \"1.6em\"\n}\nsrc\nTo import my theme settings you will need the plugin Style Settings: https://github.com/mgmeyers/obsidian-style-settings"
  },
  {
    "objectID": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#workflows-doing-cool-things",
    "href": "posts/scientific_research/2023-07-11_Zotero_Obsidian.html#workflows-doing-cool-things",
    "title": "Envinformatics",
    "section": "3. Workflows: Doing cool things",
    "text": "3. Workflows: Doing cool things\nHere I outline a few of my workflows on how I use obsidian to take notes for scientific research. Several of these are very specific to my use-cases but I believe they could be useful. I will first outline and describe each of them briefly to allow you to skim through them quickly.\n3.1 Structuring Notes Effectively with Templates\n3.2 Syncing Your Notes for Free (Laptop, Phone, Tablet)\n3.3 Zotero/Mendeley/JabRef -> Obsidian — Taking Notes and Managing Reading Lists of Scientific Papers\n3.4 Managing Projects and Lab Books\n3.5 Encrypted and Private Diary\n\n3.1 Structuring Notes Effectively with Templates\nPlugins Used: Templater (optional) and Dataview (optional).\nTo take notes effectively, you must first make the process of adding new notes as simple as possible. Templates can save you a significant amount of time while also giving you a consistent structure to your notes. Below is an example:\n\n\n\nText\n\n\nAn example of a note made with a consistent template.\n### [[YOUR MOC]]\n# The Title of Your Note\n**Tags**:: \n**Links**::\nThe topmost line is a link to your Map of Content (MOC), your signpost to explore your knowledge base (see previous sections). After the title, I add tags, that relate the note to topics (and also add a link between the note and the tag) and links to other related notes.\nOccasionally, a note might still require work to do, so I add a tag “#todo”, to quickly identify all the notes that need to be expanded. In the “TODO:” section I add the work I need to do within the note.\nThe rest is followed by notes on the actual topic.\nTemplater can help you build these templates more easily. For example, I have the following template for new books:\n### [[Books MOC]]\n# Title\n**Author**:: \n**Date**:: \n**Tags**:: \n**Links**\n\n\n\nText\n\n\nWhich I can then hook with Dataview with a simple query like such:\n```dataview\ntable author as Author, date as \"Date Finished\", tags as \"Tags\", grade as \"Grade\"\nfrom \"4. Books\"\nSORT grade DESCENDING\n```\nUsing Dataview to query notes with specific templates. \n\n\n3.2 Syncing Your Notes for Free (Laptop, Phone, Tablet)\nPlugins used: None.\nOne of the things I love the most about Obsidian is that the format of the library is self-contained and portable. Everything (including plugins) is contained in your folder. Your folders and notes are available as ordinary folders and documents. You will also notice a hidden folder called ‍‍“.obsidian”. This contains all your plugins and settings, so as long as you have this, you will be able to carry your settings to other devices.\nYou can therefore use any cloud platform you like, like Google Drive, iCloud, or DropBox for free as long as you sync your folder (note: your folder should be in your Cloud Folder).\n\n\n\nText\n\n\n\n\n3.3 Zotero/Mendeley/JabRef -> Obsidian — Taking Notes and Managing Reading Lists of Scientific Papers\nPlugins used: Citations (required).\nZotero is my reference manager of choice although this workflow should work for any reference manager that produces a .bib (bibliography) file. I export this file and keep it in my cloud folder so that I can access it from Obsidian on any platform.\nI organize my Zotero library with the following tags:\n\n\n\nText\n\n\nWhen I need to do readings, I will usually filter for the tags “!!!” and “To-Read” and choose a paper. I will then annotate the paper (either on PDF using GoodNotes or on physical paper).\nThen, I create a page for the paper using a template that you can manage in the Citations plugin settings:\n\n\n\nText\n\n\nCreate a new note and then use CMD / CTRL + P to open up the commands list and find the Citations “Insert literature note content in the current pane” and you will be greeted by this beautiful view.\n\n\n\nText\n\n\nYou can then transcribe your notes into digital format. I found that the process of transcribing helped me retain information better, so I would recommend it.\n\n\n3.4 Managing Projects and Lab Books\nPhD students at the thesis writing stage are usually full of advice (read as regret). I made it a habit of asking them things they would have done earlier or differently. One of the responses stuck with me:\nDeep stuff Leo. So my big problem is basic organisation, losing track of the tasks I ve got to do and the reasons that motivate those tasks in the first place. sometimes I d just forget about the experiments that motivated a particular experiment so I d end up going other experiments that didn t make complete sense and having to reverse engineer my logic for thesis writing A wise PhD student, now Postdoc\nOrganization is key to avoid wasting time. Especially during a PhD, organizing several projects and keeping a lab book for each of them is hard. The way I deal with it is:\n- One folder for all my projects\n- One file for each project\nI then create the header of each project with a template.\n### [[Projects MOC]]\n# <% tp.file.title %>\n**Tags**::\n**Links**::\n**URL**::\n**Project Description**::## Notes:\n### <% tp.file.last_modified_date(\"dddd Do MMMM YYYY\") %>\n#### Done:\n#### TODO:\n#### Notes\nYou can insert a template into a new note with CMD + P and looking for the Templater option.\nI then keep adding new days with another template:\n### <% tp.file.last_modified_date(\"dddd Do MMMM YYYY\") %>\n#### Done:\n#### TODO:\n#### Notes:\nThis way you can keep adding days to your project and update with reasonings and things you still have to do and have done. An example below:\n\n\n\nText\n\n\nExample of project note with timestamped notes.\n\n\n3.5 Encrypted and Private Diary\nPerhaps this is one of my favorite applications of Obsidian.\nI have been frustrated with Mini Diary and its interface for a while. After the project got archived by the author I decided to find an alternative. I had two requirements:\n- It had to be private, and I wanted absolutely nobody to be able to read the content of the entries.\n- It had to sync with the cloud so I could edit it in different devices.\nI then found out about encrypting the Obsidian folder on disk. You can then decrypt the folder and open it with Obsidian. The folder can be synced as per usual.\nTo do this I use CryptoMator (https://cryptomator.org/). You can add a your Obsidian vault as a folder to encrypt on Cryptomator, set up a password and it will take care of the rest.\nYou can watch this video if you would like a step-by-step video guide"
  },
  {
    "objectID": "posts/scientific_research/2023-07-13_Research_Workflow_Obsidian.html",
    "href": "posts/scientific_research/2023-07-13_Research_Workflow_Obsidian.html",
    "title": "Envinformatics",
    "section": "",
    "text": "---\ntitle: \"Using Obsidian in Mind Mapping\"\ndescription: \"Tools and workflows for Mind Mapping projects during research.\"\nauthor: \"Me\"\ndate: \"2023-07-13\"\ncategories: [Zettelkasten, Zotero, Obsidian, Mind Mapping]\nimage: images/zotero_obsidian.png\nskip_exec: true\nskip_showdoc: true\nformat:\n  html:\n    code-fold: true\n    self-contained: true\n---\n\n\nWhat is a Mind Map?\nA mind map is a visual representation of an idea that uses nodes/points and lines to connect ideas together. The mind map primarily uses text inside of a node/point connected by lines. You can use color, images, line style, line weight, symbols, and other things to visualize your idea.\n\n\nWhy should I use a Mind Map?\n\n\n\nText\n\n\nWhen you have many lists, a mind map adds an extra dimension to lists. For example, I started this article as that mind map pictured above. Sometimes I start articles as outlines. Obviously, writing an article on mind maps must start with a mind map. mind maps work best for me in two broad use cases:\n- Generating new ideas\n- Organizing information\n\nGenerating Ideas\nAs I’ve already shown, you can develop the structure of a writing or other creative project with a mind map. I love mind maps for any sort of project work. When I am going to start a new project I almost always start with a mind map of ideas I have about what I know about the project and what I need to find out about the project.\nI also think mind maps are perfect for planning activities when they aren’t “linear”. For example, my annual planning I do as a mind map first where I write out what I know will happen in that year and then brainstorm things I want to do in the year. I then take what I generate in the mind map and can place it on the calendar.\n\n\nOrganizing Information\nWhen I am learning something new — particularly through reading — I very often start with a mind map. I love the Blank Sheet Method from Farnam Street. If I am starting a book on a topic I’m actively trying to learn, I’ll use a version of this method for my learning sessions. Each starts with a mind map about what I know about the topic and then as I’m reading, watching videos, or consuming other educational content, I’ll add to my mind map in a different color.\n\n\n\nUse Mind Maps in Obsidian\nThere are really three ways to use mind maps in Obsidian.\n1- Create a mind map directly in your vault with a dedicated Mind Map plugin. We’ll look at my two favorites below.\n2- Create a mind map directly in your vault with the Excalidraw plugin.\n3- Create a mind map with an external tool. Link to it and/or import it into your vault.\nMind Mapping in Obsidian Canvas"
  }
]